<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《SQL进阶教程》]]></title>
    <url>%2F2019%2F08%2F19%2FSQL_read3%2F</url>
    <content type="text"><![CDATA[第一章 神奇的SQL之HAVING 子句的力量 HAVING 子句的力量概述 HAVING 子句的力量]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《SQL进阶教程》]]></title>
    <url>%2F2019%2F08%2F19%2FSQL_read2%2F</url>
    <content type="text"><![CDATA[第一章 神奇的SQL之三值逻辑和NULL 三值逻辑和NULL概述 两种NULL、三值逻辑还是四值逻辑NULL是产生三值逻辑的“元凶” 两种NULL分为两种：1.未知 2.不正确不适用未知：表示有前提假设，有处理过程，但是结果不知道是怎么样的不适用：前提就不对，那基于前提的处理结果，就本就不合理。NULL并不是值。 三值：unknown，但是不同于NULL的一种。unknown 是一种明确的布尔型的真值，而NULL中的UNKNOWN不是值也不是变量。 三值真值表(NOT)：|x |NOT x|—– |—-||t | f||u | u||f | t| AND t u f t t u f u u u f f f f f OR t u f t t t t u t u u f t u f 真值的优先级：and: false &gt; unknown &gt;trueor : true &gt; unknown &gt;false 而在某列未设置非空的情况下，实际上是有可能不能直接比较的 IN 和EXISTS 的等价的但是 NOT IN 和NOT EXISTS并不是等价的。 EXISTS不会返回unknown结果，而in 可以返回 本节要点： NULL 不是值。 因为 NULL 不是值，所以不能对其使用谓词。 对 NULL 使用谓词后的结果是 unknown 。 unknown 参与到逻辑运算时，SQL 的运行会和预想的不一样。 按步骤追踪 SQL 的执行过程能有效应对 4 中的情况。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《SQL进阶教程》]]></title>
    <url>%2F2019%2F08%2F19%2FSQL_read1%2F</url>
    <content type="text"><![CDATA[第一章 神奇的SQL之自连接自连接概述 一般来说，这些连接大都是以不同的表或视图 为对象进行的，但针对相同的表或相同的视图的连接也并没有被禁止。针对相同的表进行的连接被称为“自连接”（self join）。 可重排列、排列、组合利用自联结删除重复行排序窗口函数： SELECT name, price,RANK() OVER (ORDER BY price DESC) AS rank_1, DENSE_RANK() OVER (ORDER BY price DESC) AS rank_2FROM Products; 本节要点： 自连接经常和非等值连接结合起来使用。 自连接和 GROUP BY 结合使用可以生成递归集合。 将自连接看作不同表之间的连接更容易理解。 应把表看作行的集合，用面向集合的方法来思考。 自连接的性能开销更大，应尽量给用于连接的列建立索引 练习题：]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《SQL进阶教程》]]></title>
    <url>%2F2019%2F08%2F19%2FSQL_read0%2F</url>
    <content type="text"><![CDATA[第一章 神奇的SQL之CASECASE 表达式概述利用CASE 表达式来进行分支判断，有两种CASE表达式：简单表达式(simple case expresssion )和搜索表达式（searched case expression）。 简单表达式 vs 搜索表达式 123456789101112-- 简单CASE 表达式CASE sex WHEN &apos;1&apos; THEN &apos;男&apos; WHEN &apos;2&apos; THEN &apos;女&apos; ELSE &apos;其他&apos; END-- 搜索CASE 表达式 CASE WHEN sex = &apos;1&apos; THEN &apos;男&apos; WHEN sex = &apos;2&apos; THEN &apos;女&apos; ELSE &apos;其他&apos; END 使用CASE 的注意点： 1.统一各分支返回的数据类型，CASE 各个分支的返回值的类型是需要一直的 2.不要忘了写END，很有可能出现问题 3.养成写ELSE 的习惯 使用CHECK约束12345CONSTRAINT check_salary CHECK ( CASE WHEN sex = &apos;2&apos; THEN CASE WHEN salary &lt;= 200000 THEN 1 ELSE 0 END ELSE 1 END = 1 ) 在UPDATE语句中进行条件分支12345678-- 用CASE 表达式写正确的更新操作 UPDATE Salaries SET salary = CASE WHEN salary &gt;= 300000 THEN salary * 0.9WHEN salary &gt;= 250000 AND salary &lt; 280000 THEN salary * 1.2 ELSE salary END; --这里的Else 很重要 ,否则，不在CASE范围内的数据，就被置为空 表之间的数据匹配和DECODE 函数相比，CASE表达式中一大优势是能够判断表达式，可以在CASE 中使用谓词组合。 12345678SELECT CM.course_name, CASE WHEN EXISTS (SELECT course_id FROM OpenCourses OC WHERE month = 200706 AND OC.course_id = CM.course_id) THEN &apos;○&apos; ELSE &apos;×&apos; END AS &quot;6 月&quot;, CASE WHEN EXISTS(SELECT course_id FROM OpenCourses OC WHERE month = 200707 AND OC.course_id = CM.course_id) THEN &apos;○&apos; ELSE &apos;×&apos; END AS &quot;7 月&quot;, CASE WHEN EXISTS (SELECT course_id FROM OpenCourses OC WHERE month = 200708 AND OC.course_id = CM.course_id) THEN &apos;○&apos; ELSE &apos;×&apos; END AS &quot;8 月&quot; FROM CourseMaster CM; CASE 也可以使用聚合函数123456789SELECT std_id, CASE WHEN COUNT(*) = 1 -- 只加入了一个社团的学生 THEN MAX(club_id) ELSE MAX(CASE WHEN main_club_flg = &apos;Y&apos; THEN club_id ELSE NULL END) END AS main_club FROM StudentClub GROUP BY std_id; 本节要点： 在 GROUP BY 子句里使用 CASE 表达式，可以灵活地选择作为聚 合的单位的编号或等级。这一点在进行非定制化统计时能发挥巨 大的威力。 在聚合函数中使用 CASE 表达式，可以轻松地将行结构的数据转 换成列结构的数据。 相反，聚合函数也可以嵌套进 CASE 表达式里使用。 相比依赖于具体数据库的函数，CASE 表达式有更强大的表达能 力和更好的可移植性。 正因为 CASE 表达式是一种表达式而不是语句，才有了这诸多优 点 练习题：]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo中的邮件发送]]></title>
    <url>%2F2018%2F12%2F11%2Fodoo_mail_config%2F</url>
    <content type="text"><![CDATA[近半周，主要在研究odoo中的邮件发送的内容，修改重置密码和修改密码的邮件发送的相关功能。主要发现了： odoo 简单配置邮件，利用bournce 模式 odoo catchall 模式配置邮件 【待研究】 odoo bournce模式邮件配置： odoo发送邮件的过程中碰到 no service or xxx 使用ping 来测试是否可以访问外网。在docker 里无法ping同域名，提示类似于 ping bad address 的问题，但是可以ping 通IP，这个需要后期研究一下为什么会这样。 【待研究】 是由可能是无法访问到外部网络，是由于DNS解析失败造成的，可以暂时先不管，重启一次docker就好了。 简单的方案：直接重启docker，再次ping 网址，发现可以。 继续配置odoo邮件： 配置好出向邮件，即为发件人，要求发件人开同通IMAP服务或POP服务。 配置入向服务器，收件箱，在这种模式下可以不用配置。 配置好公司邮件，在公司的参数里，设置好，这里设置为出向邮箱。 删掉bounce 设置。在设置-技术-系统参数中。虽然这里面也没有不删掉，会报错，提示发件人和认证用户不是同一个人。 自己配置的公司的发件人A，而这里却不是A，是以bounce开头的用户，是为啥呢？ 提示的错误：12345邮件投递失败通过SMTP发送邮件失败 &apos;smtp.qq.com&apos;.SMTPSenderRefused: 501mail from address must be same as authorization userbounce+12-sale.order-6@odooqs.com 阅读源码：12345678910111213141516171819@api.modeldef _get_default_bounce_address(self): &apos;&apos;&apos;Compute the default bounce address. The default bounce address is used to set the envelop address if no envelop address is provided in the message. It is formed by properly joining the parameters &quot;mail.bounce.alias&quot; and &quot;mail.catchall.domain&quot;. If &quot;mail.bounce.alias&quot; is not set it defaults to &quot;postmaster-odoo&quot;. If &quot;mail.catchall.domain&quot; is not set, return None. &apos;&apos;&apos; get_param = self.env[&apos;ir.config_parameter&apos;].sudo().get_param postmaster = get_param(&apos;mail.bounce.alias&apos;, default=&apos;postmaster-odoo&apos;) domain = get_param(&apos;mail.catchall.domain&apos;) if postmaster and domain: return &apos;%s@%s&apos; % (postmaster, domain) 从以上可以得知 _get_default_bounce_address 发送邮件名来源于 mail.bounce.alias ,默认值是postmaster-odoo.根据以上代码的值，不删除mail.bounce.alias的值，会导致发送地址错误，删除mail.bounce.alias的值，导致默认发送邮箱名是postmaster-odoo。也就是 catchall模式配置参数：还没仔细研究 catchall 是可以看作一个中介的功能。当一个联系人回复Odoo发送的邮件时，reply-to 地址是一个catchall地址，可将回复中继到Odoo（商机、订单、任务等）中正确的讨论话题，以及所有关注者的邮箱中。地址默认为”catchall@”，也可以更改。 在你的邮件服务器设置中创建一个catchall地址。我们建议您使用”catchall@”地址，可直接运行。如想使用其他别名，需要在Odoo中多设置一些步骤。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>邮件发送</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo中的JS]]></title>
    <url>%2F2018%2F12%2F06%2Fodoo_js%2F</url>
    <content type="text"><![CDATA[odoo 作为一个系统，就避免不了前后端交互的过程。 本部分，就简要讲解一下odoo中的JS。 odoo 中的JS ,主要都是继承写的。实际上，在odoo 中，前后端封装的十分优美，后端人员基本无需手动去改动JS代码，如果需要，可能只是简单的修改，例如报表查看的功能？ odoo 中的JS：关键词： Classextend()include()_super()Require.js JS 中没有Class 的，odoo中使用封装手段，将数据结构，封装成类。 extend : 扩充原类的方法，不改变原类的方法，仅仅新增 include ： 引入原类的方法，并对方法进行修改 _super() ： 类似于面向对象中的调用父类的方法 require 123456789odoo.define(&apos;tree_budget_sheet_line_quick_query&apos;, function (require) &#123; &quot;use strict&quot;; var core = require(&apos;web.core&apos;); var data = require(&apos;web.data&apos;); var ListView = require(&apos;web.ListView&apos;); var Model = require(&apos;web.DataModel&apos;); var QWeb = core.qweb; 从这段代码可以看出，odoo 中需要调用require进行引入数据，引入其他类]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo源码model]]></title>
    <url>%2F2018%2F12%2F05%2Fodoo_source_model%2F</url>
    <content type="text"><![CDATA[这部分，主要用来分析观察odoo源码中的models文件，它主要介绍源码中的文件，作用。主要构成： 注意： 函数后 第一个参数，表示哪个函数(A)和它(B)有关，第二个参数表示关联，form表示A调用函数，to表示B调用A 代码和解析部分 主要的作用 联系部分 1.代码部分 class BaseModel 包含的方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215view_init default_get from _field_create _auto_init from_add_field 多函数调用，from 按照命名，应该是用来新增字段_pop_field _drop_column _prepare_update from _add_magic_fields _setup_base fromcompute_concurrency_field _add_magic_fields from compute_concurrency_field_with_access _add_magic_fields from_build_model setup_models from _build_model_check_base_build_model_check_parent_build_model_attributes_add_manual_fields_init_constraints_onchanges_constraint_methodsis_constraint_onchange_methodsis_onchange__new____init___is_an_ordinary_table__export_xml_id_export_rowsexport_dataload_add_fake_fields_extract_recordsonly_o2m_values_convert_records_log_validate_fieldsdefault_getfields_get_keys_rec_name_fallbackview_header_getuser_has_groups_get_default_form_view_get_default_search_view_get_default_tree_view_get_default_pivot_view_get_default_kanban_view_get_default_graph_view_get_default_calendar_viewset_first_ofload_viewsfields_view_getcleanget_formview_idget_formview_actionget_access_actionsearch_countsearch_compute_display_namename_getname_createname_search_name_search_add_missing_default_valuesclear_caches_read_group_fill_results_read_group_prepare_read_group_process_groupby_read_group_prepare_data_read_group_format_resultread_group_read_group_raw_inherits_join_add_inherits_join_calc_parent_store_computeprocess_check_selection_field_value_check_removed_columns_save_constraint_drop_constraint_save_relation_table_m2o_add_foreign_key_checked_m2o_add_foreign_key_unchecked_m2o_fix_foreign_key_init_column_auto_initfunc_auto_endinit_table_exist_create_table_parent_columns_exist_create_parent_columns_select_column_data_add_sql_constraintsunify_cons_textdropadd_execute_sql_add_inherited_fields_inherits_check_prepare_setup_setup_base_setup_fields_setup_completefields_getget_empty_list_helpcheck_field_access_rightsvalidread_prefetch_field_read_from_databasequalifyget_metadata_check_concurrency_check_record_rules_result_countcheck_access_rightscheck_access_rulecreate_workflowdelete_workflowstep_workflowsignal_workflowredirect_workflowunlinkwrite_writecreate_create_where_calc_check_qorder_apply_ir_rulesapply_rule_generate_translated_field_generate_m2o_order_by_generate_order_by_inner_generate_order_by_search_uniquify_listcopy_datablacklist_given_fieldscopy_translationsget_transcopyexists_check_recursion_check_m2m_recursion_get_external_idsget_external_idprint_reportis_transient_transient_clean_rows_older_than_transient_clean_old_rows_transient_vacuumresolve_2many_commandssearch_readtoggle_active_register_hook_patch_methoddo_write_revert_method_browsebrowseidsensure_onewith_envsudowith_contextwith_prefetch_convert_to_cache_convert_to_record_convert_to_write_mapped_funcmapped_mapped_cachefilteredsortedupdatenew_is_dirty_get_dirty_set_dirty__nonzero____len____iter____contains____add__concat__sub____and____or__union__eq____ne____lt____le____gt____ge____int____str____unicode____hash____getitem____setitem___cache_in_cache_withoutrefreshinvalidate_cachemodified_recompute_check_recompute_todo_recompute_donerecompute_has_onchange_onchange_specprocess_onchange_evalprocess__init____getitem____getattr__onchange]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo与RPC]]></title>
    <url>%2F2018%2F12%2F05%2Fodoo_rpc%2F</url>
    <content type="text"></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>odoo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo中的XML与xpath]]></title>
    <url>%2F2018%2F11%2F28%2Fodoo_xpath%2F</url>
    <content type="text"><![CDATA[在之前，做爬虫的相关事情的时候，就已经接触到xpath的使用，只不过没有大规模的使用，只是浅尝辄止。 关于xpath，了解背景知识，请看这里。 关于xml，其实和HTML结构相似，类似于DOM，关于DOM和XML的相关知识，请看这里。 在odoo开发中，经常会用到odoo中元素的选择。 xpath，本质是一种对xml的选择，具体来说就是需要确定xml的节点，即XML的节点位置。 xml和HTML结构很相似，都有元素节点，节点都会有标签元素(tag)，标签又会有一些属性。节点之间的关系有： 1. 父子 2. 祖先和后代 3. 兄弟姐妹节点 接下来的部分，将讲述xml的语法。 选取节点利用路径表达式，获取节点，主要的表达式有： nodename 选取nodename的所有子节点/ 从根节点取// 从匹配选择的当前节点选文档的节点. 选取当前节点.. 选区当前节点的父节点@ 选取属性 谓语谓语用来查找某个特定的节点或者包含某个指定的值的节点。主要的谓语有：[1][last()][last()-1][position()&lt;3][@attr=value] 选取属性为value的元素[@attr] 选取含attr的元素 通配符 选择任意元素的节点@* 选择任意属性的节点node() 匹配任意类型的节点 多路径多路径中间用 | 分隔符来隔开。 参考文献w3scholl]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>XML，xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo中的XML（续）]]></title>
    <url>%2F2018%2F11%2F28%2Fodoo_lxml%2F</url>
    <content type="text"><![CDATA[odoo中，使用的lxml的库来控制xml的加载的。lxml 是一款定位XML元素位置的工具，并可以利用它对XML进行修改 关于lxml，简单了解一下，什么是[Python中的lxml]以及它的用途(https://lxml.de/)。 本部分主要简单介绍一下lxml的用法。 lxml主要几部分： 位置定位主要通过xpath ,对一段XML进行定位元素，获取元素的位置。通过标签，属性，内容等来对XML进行限制。 操作谓语 参考文献lxml官方文档]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>XML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo中的XML]]></title>
    <url>%2F2018%2F11%2F27%2Fodoo_xml%2F</url>
    <content type="text"><![CDATA[odoo中，xml可以通过context来写入吗？ 可以 odoo中LXML的用法 lxml.builder可以建立XML odoo 中的xml除开手写，继承的方式展现。另外，还可以通过context的方式来实现获取，将数据写在context中，可以直接获取到context的数据 案例： 针对某一个视图修改： 功能点权限页面视图的显示123456789101112131415161718192021222324252627282930313233343536view_function = self.env.ref(&apos;base.user_groups_view_function&apos;, raise_if_not_found=False) if view_function and view_function.exists() and view_function._name == &apos;ir.ui.view&apos;: group_no_one = view_function.env.ref(&apos;base.group_no_one&apos;) xml1, xml2 = [], [] xml1.append(E.separator(string=_(&apos;Application&apos;), colspan=&quot;2&quot;)) for app, kind, gs in self.get_groups_by_application(): # hide groups in categories &apos;Hidden&apos; and &apos;Extra&apos; (except for group_no_one) attrs = &#123;&#125; if app.xml_id in ( &apos;base.module_category_hidden&apos;, &apos;base.module_category_extra&apos;, &apos;base.module_category_usability&apos;): attrs[&apos;groups&apos;] = &apos;base.group_no_one&apos; if kind == &apos;selection&apos;: # application name with a selection field field_name = name_selection_groups(gs.ids) xml1.append(E.field(name=field_name, **attrs)) xml1.append(E.newline()) else: # application separator with boolean fields app_name = app.name or _(&apos;Other&apos;) xml2.append(E.separator(string=app_name, colspan=&quot;4&quot;, **attrs)) for g in gs: field_name = name_boolean_group(g.id) if g == group_no_one: # make the group_no_one invisible in the form view xml2.append(E.field(name=field_name, invisible=&quot;1&quot;, **attrs)) elif g.classification in (&apos;function_access&apos;, &apos;data_access&apos;): xml2.append(E.field(name=field_name, **attrs)) else: xml2.append(E.field(name=field_name, invisible=&quot;1&quot;, **attrs)) xml2.append(&#123;&apos;class&apos;: &quot;o_label_nowrap&quot;&#125;) xml = E.field(E.group(*(xml1), col=&quot;2&quot;), E.group(*(xml2), col=&quot;4&quot;), name=&quot;groups_id&quot;, position=&quot;replace&quot;) xml.addprevious(etree.Comment(&quot;GENERATED AUTOMATICALLY BY GROUPS&quot;)) xml_content = etree.tostring(xml, pretty_print=True, xml_declaration=True, encoding=&quot;utf-8&quot;) view_function.with_context(lang=None).write(&#123;&apos;arch&apos;: xml_content, &apos;arch_fs&apos;: False&#125;)]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>XML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo工作流]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_workflow%2F</url>
    <content type="text"><![CDATA[本章节主要介绍odoo中的工作流的知识。 odoo中需要用到工作流的相关知识，关于工作流，请参考这篇文章。 odoo中，关于工作流，需要有一定的配置。 工作流，在odoo中，主要用在审批流中。 审批流的]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>Workflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo视图]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_views%2F</url>
    <content type="text"><![CDATA[这部分主要介绍的是odoo中视图的总结 odoo视图中的 在常见的开发过程中，主要用到的视图有：tree,search ,form,kanban,日历等。 在这个过程中，一般tree会自动去寻找search视图，如何去找，自己还没研究透，在fileds_view_get中，可以对视图进行操作。 tree视图，有时又叫树视图或者列表视图，是展示一条数据的最简单的方法。tree视图也可以在设置属性的时候，调整增加这部分的数据 form视图，表单视图。通常在创建的时候，可以创建form视图，]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>views</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo表与字段]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_tables_fields%2F</url>
    <content type="text"><![CDATA[本部分主要讲述odoo中的表和字段的关系。 odoo中的普通模型，model.Models模型中，创建的字段，都会直接通过odoo的ORM转成后台的数据库的表，如何实现的呢，可以查看ORM部分的代码。 什么是ORM，参考这篇文章。 模型中的_name会转化成表名，而pg库的表的长度只有65个字符的长度，所以，模型的name的长度不能太长。 而odoo中字段的表的字段的类型，可以通过字段类型来完成设置，最后通过ORM的映射，转换成SQL，进而直接创建相应的字段。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>odoo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo小问题]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_small_problem%2F</url>
    <content type="text"><![CDATA[在开发中碰到了一些问题，这个就作为一个简单的问题搜索的索引吧。 视图错误模型错误数据错误一些暂时无法找到的问题odoo 中的JSodoo中的函数]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>bugs</tag>
        <tag>problems</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo报表]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_report%2F</url>
    <content type="text"><![CDATA[本章主要介绍odoo中的报表的相关知识。 在前人的指导下， odoo中报表的开发的知识已经写好了。 同时，官方提供的文档中，也有这部分的内容，参见这部分。 odoo中的报表主要有几种，Odoo中的excel，pdf和利用前端显示的报表(HTML)。 odoo报表开发的步骤： 获取数据1.1 数据获取主要通过函数来实现，注意数据获取的权限和数据是否为空，以及如何高效获取数据，以及识别数据的类型，如何处理这些数据。 渲染数据 数据的渲染的写法也是固定的，只不过是在填充不同的样式和不同的数据的展现方式而已。 数据的渲染，主要是格式的渲染，参照CSS的渲染方式来添加，主要的参数： 1234567891011align:对其方式valign:垂直对齐方式border:框线边框的宽度border_color：框线颜色num_format：数据的格式化&apos;bold&apos;: False, 加粗，可以调节数字来改变框线的宽度&apos;border_color&apos;: &apos;#000000&apos;, # black 框线的颜色&apos;fg_color&apos;: &apos;#CCCCCC&apos;,单元格的背景色 浅灰色&apos;font_name&apos;: &apos;Arial&apos;,字体&apos;font_size&apos;: 10 字体大小&apos;pattern&apos;: 1, # solid 实线 odoo中excel报表的编写，调用的是xlwt的写法，参照xlwt的写法。 odoo中PDF的报表的编写类似，不过自己还没写过这种。 odoo中HTML报表的编写，是同样的写法，不过需要组织js。 组织结构（始终围绕菜单–动作–视图来写） 3.1 编写数据选择的视图的编写，如下拉框，多选框等 3.2 动作的编写，注意权限 3.3 菜单的编写，注意权限 具体来看某个例子： 需求：要查看系统中业务伙伴中供应商的报表，需要查询供应商的详细信息。 选择的筛选逻辑：选择公司和归口管理分类，有一定的限定条件 代码组织： 选择的字段的逻辑 查询数据 编写打印的动作 编写渲染的代码 渲染的代码主要分为两部分： 解析类（解析类的代码固定，几乎不需要改动） 主体类（就是如何把数据进行组织） 通过定义 generate_xlsx_report 方法，来获取数据，数据的组织，样式的编写。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo权限]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_power%2F</url>
    <content type="text"><![CDATA[odoo中关于权限，有几个部分： 记录规则 记录规则，模型:ir.rule 菜单级别 菜单访问权限，通过分配用户组来实现，但是只是隐藏菜单可视的入口，如果知道菜单的xml_id，仍然可以通过进入指定的URL来访问。 如何实现，暂时还未研究，待研究 模型访问权限模型的访问权先，主要是通过CSV文件来实现的，分别表示对某个模型的增删改查的权限。 CSV文件最后存储在ir_model_data数据表中， 具体部分，如：access_res_users,res.users,model_res_users,base.group_user,1,1,1,1 第一个是参数是xml_id，用于存储在数据库的ir_model_data中，作为标识，也可以直接设置-技术-外部标识中（debug模式下）搜索到， 第二个参数res.users 表示描述性标题，它只是内容展示，在模块中最好也保持其唯一性。官方模块通常使用模型名称和组的圆点分隔的字符串。model_res_users 表示的是model_id，是由模型自动生成的xmlid，一般是model+模型名base.group_user表示的是用户组或群组。 最后的数字表示的是：1，表示拥有这部分权限，0表示没有这部分权限,表示的顺序是：读写创删权限。 字段权限 odoo中，利用字段的权限，可以控制不同群组访问不同的字段。 参考文献：《odoo中文教程》 《OpenERP应用和开发基础》 《Odoo 10 Development Essentials》 参考网站和论坛：odoov 网站 odoo 官网 odoo 中文论坛 odoo hk odoo 论坛1 odoo v odoo文档 翻译网站 odoo 手册 odoo官方文档 odoo 其它企业 一些blog和博主：toby2chen Jeff odoo OCA]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo函数]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_function%2F</url>
    <content type="text"><![CDATA[odoo中的一些函数的介绍 搜索与查询类search search_read search_count name_search 创建类create 更新类write 删除类unlink]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>odoo</tag>
        <tag>function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odoo中的API]]></title>
    <url>%2F2018%2F11%2F23%2Fodoo_api%2F</url>
    <content type="text"><![CDATA[odoo中，最重要的就是API的使用，多种不同的API方法。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>odoo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下git问题处理]]></title>
    <url>%2F2018%2F04%2F16%2Fgit_problem%2F</url>
    <content type="text"><![CDATA[碰到git登录信息有误的情况： 如何在电脑上消除已经登录的github等信息 在控制面板\用户帐户\凭据管理器目录下删除你的登录网址即可。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jack的keras自学历程3——函数式模型]]></title>
    <url>%2F2018%2F03%2F29%2Fkeras_3%2F</url>
    <content type="text"><![CDATA[函数式模型可以从函数式编程的知识中了解。（函数式编程）keras函数式提供的编程模型接口是让用户有定义多输出模型，有向无环图（DAG）或者具有共享层的模型的途径。换句话说，只要你的模型不是类似VCG（是什么）一样一条路走到黑的模式，或者你的模型需要多个输出的模型，你应该选择函数式模型。它是最广泛的一类模型，序列（序贯）模型知识它的一种特殊情况。 本节假设你对Sequential模型已经比较熟悉。 第一个模型：全连接网络（什么是全连接网络）Sequential是实现全连接网络的最好方式，我们要从最简单的全连接网络开始.。开始前有几个概念需要弄清：层对象接受张量为参数，返回一个张量输入是张量，输出也是张量的框架就是一个模型，该模型通过Model定义这样的模型可以被训练，就像Sequential一样 from keras.layers import Input, Dense from keras.models import Model # This returns a tensor inputs = Input(shape=(784,)) # a layer instance is callable on a tensor, and returns a tensor x = Dense(64, activation=&apos;relu&apos;)(inputs) x = Dense(64, activation=&apos;relu&apos;)(x) predictions = Dense(10, activation=&apos;softmax&apos;)(x) # This creates a model that includes # the Input layer and three Dense layers model = Model(inputs=inputs, outputs=predictions) model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;]) model.fit(data, labels) # starts training 利用函数式模型的接口，我们可以很容易重用已经训练好的模型：可以把模型当作一个层一样，通过提供一个tensorflow来调用它，但是要注意，当你调用一个模型时，你不仅重用了它的结构，页重用了它的权重。 多输入和多输出的模型函数式模型的一个典型场景是搭建一个多输入、多输出的模型。以twitter分析为例。 from keras.layers import Input, Embedding, LSTM, Dense from keras.models import Model # Headline input: meant to receive sequences of 100 integers, between 1 and 10000. # Note that we can name any layer by passing it a &quot;name&quot; argument. main_input = Input(shape=(100,), dtype=&apos;int32&apos;, name=&apos;main_input&apos;) # This embedding layer will encode the input sequence # into a sequence of dense 512-dimensional vectors. x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input) # A LSTM will transform the vector sequence into a single vector, # containing information about the entire sequence lstm_out = LSTM(32)(x) auxiliary_output = Dense(1, activation=&apos;sigmoid&apos;, name=&apos;aux_output&apos;)(lstm_out) auxiliary_input = Input(shape=(5,), name=&apos;aux_input&apos;) x = keras.layers.concatenate([lstm_out, auxiliary_input]) # We stack a deep densely-connected network on top x = Dense(64, activation=&apos;relu&apos;)(x) x = Dense(64, activation=&apos;relu&apos;)(x) x = Dense(64, activation=&apos;relu&apos;)(x) # And finally we add the main logistic regression layer main_output = Dense(1, activation=&apos;sigmoid&apos;, name=&apos;main_output&apos;)(x) model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output]) model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;binary_crossentropy&apos;, loss_weights=[1., 0.2]) model.fit([headline_data, additional_data], [labels, labels], epochs=50, batch_size=32) model.compile(optimizer=&apos;rmsprop&apos;, loss={&apos;main_output&apos;: &apos;binary_crossentropy&apos;, &apos;aux_output&apos;: &apos;binary_crossentropy&apos;}, loss_weights={&apos;main_output&apos;: 1., &apos;aux_output&apos;: 0.2}) # And trained it via: model.fit({&apos;main_input&apos;: headline_data, &apos;aux_input&apos;: additional_data}, {&apos;main_output&apos;: labels, &apos;aux_output&apos;: labels}, epochs=50, batch_size=32) 共享层还有使用函数式模型的场合是使用共享层的时候。以微博的数据为例。 import keras from keras.layers import Input, LSTM, Dense from keras.models import Model tweet_a = Input(shape=(140, 256)) tweet_b = Input(shape=(140, 256)) # This layer can take as input a matrix # and will return a vector of size 64 shared_lstm = LSTM(64) # When we reuse the same layer instance # multiple times, the weights of the layer # are also being reused # (it is effectively *the same* layer) encoded_a = shared_lstm(tweet_a) encoded_b = shared_lstm(tweet_b) # We can then concatenate the two vectors: merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1) # And add a logistic regression on top predictions = Dense(1, activation=&apos;sigmoid&apos;)(merged_vector) # We define a trainable model linking the # tweet inputs to the predictions model = Model(inputs=[tweet_a, tweet_b], outputs=predictions) model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;binary_crossentropy&apos;, metrics=[&apos;accuracy&apos;]) model.fit([data_a, data_b], labels, epochs=10) 层“节点”的概念]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>keras系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jack的keras自学历程2——序列模型（序贯模型）]]></title>
    <url>%2F2018%2F03%2F29%2Fkeras_2%2F</url>
    <content type="text"><![CDATA[序贯模型是多个神经网络的线性叠加，也就是“一条路走到黑”。可以通过向Sequential模型传递一个list来构造： from keras.models import Sequential from keras.layers import Dense,Activation model = Sequential([Dense(32,units=784),Activation(&apos;relu&apos;),Dense(10),Activation(&apos;softmax&apos;)]) # 模型是一个输入为784的模型，通过Dense将其压缩到32，然后调用relu激活函数对其处理，第二层是对第一层输出的32再次使用Dense将其压缩，压缩到10，而后，调用softmax激活函数对第二层网络的输出进行处理 也可以通过.add()方法来将一个网络层加入模型之中： model.Sequential() model.add(Dense(32,input_shape=(784,)) model.add(Activation(&apos;relu&apos;)) 注意：这里input_shape和units都表示输入的参数，神经网络需要指定一个输入参数，而后的神经网络的输入是上一层的输出，且可以自动推到出来，所以就不需要指定这个参数，具体如何计算出这个参数，需要深入理解深度学习网络的知识（这部分内容有待扩充）。 model=Sequential() model.add(Dense(32,input_shape=(784) input_shape()是一个tuple类型的数据，可以填入None，如果是None，表示此位置可能是任何一个正整数。数据中的batch大小不应该包含在其中。（这句话不懂）有些2D层，如Dense,支持通过指定输入维度input_dim来隐含指定的输入数据shape，是一个int类型数据。有些3D层支持通过输入input_dim和input_length来指定输入的shape。如果需要为输入指定一个固定大小的batch_size，可以传递batch_size参数到一个层中（这段话需要接触更多的深度学习的知识） 在训练模型前，需要调用compile()来对学习过程进行配置。compile()接受三个参数：优化器optimizer：该参数可以指定为已预定义的优化器名，如rmsprop,adagrad，或者一个Optimizer类的对象，详情看：optimier(http://keras-cn.readthedocs.io/en/latest/other/optimizers/)损失函数loss，该参数为模型试图将损失达到最小的目标函数，可以定义为一个预定义的损失函数名，如categorical_crossentropy,mse，也可以定义为一个损失函数。详情参见loss(http://keras-cn.readthedocs.io/en/latest/other/objectives/)评估指标函数metrics：对训练的结果进行评估的指标。可以是一个预定义的指标的名字，也可以是一个用户自定义的函数，指标函数应该返回一个张两或者一个字典，一个metrics_name-&gt;metrics_value的字典。指标函数不一定需要指定。 对编译好的模型进行训练：keas以numpy数组作为输入数据和标签的数据类型。一般使用fit()，详细使用请参考这里（http://keras-cn.readthedocs.io/en/latest/models/sequential/） 文档中的例子：CIFAR10 小图片分类：使用CNN和实时数据提升IMDB 电影评论观点分类：使用LSTM处理成序列的词语Reuters（路透社）新闻主题分类：使用多层感知器（MLP）MNIST手写数字识别：使用多层感知器和CNN字符级文本生成：使用LSTM …]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>keras系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jack的keras自学历程1——尝试与概览]]></title>
    <url>%2F2018%2F03%2F29%2Fkeras_1%2F</url>
    <content type="text"><![CDATA[说明学习过程期间参考的资料有： [keras官方文档](https://keras.io/zh/) http://keras-cn.readthedocs.io/ 自己自学keras的前提是接触了部分机器学习的算法，而后接触到了深度学习中的神经网络部分，可能本教程需要更多的知识，如果遇到不懂的知识点，请自行bing或者Google。 keras是基于Python的深度学习库，兼容2和3，底层的依赖神经网络开源库可以是tensorflow，theano和CNTK。它与tensorflow和theano一样，是一种基于符号运算的库。所谓符号运算，自己的理解，就是在程序开始跑之前，需要对程序进行搭建，类似于工程中的蓝图，先要定义好各种变量，建立一个计算图，计算图规定了变量之间的计算关系，当等计算图搭建好之后，才可以输入数据，才会有输出值。官方网站的比喻是，用管道搭建供水系统，在拼水管的过程，就是在搭建图的过程。安装keras前需要安装依赖：numpy,scipy,pyyaml 快速上手：快速建立深度学习的神经网络模型：搭建序列（序贯）模型： from keras.models import Sequential #导入keras中的Sequential包 model=Sequential() #定义模型 from keras.layers import Dense,Activation #导入全连接层和激活函数 model.add(Dense(units=64,input_dim=100)) #构建输入层 model.add(Activation(&apos;relu&apos;)) #使用激活函数 model.add(Dense(units=10)) #对第一层的输出调用全连接，输出10类 model.add(Activation(&apos;softmax&apos;)) #调用激活函数来对数据进行分类 #在模型搭建好后，要对模型进行编译才能调用 model.compile(loss=&apos;categorical_crossentropy&apos;, optimizer=&apos;sgd&apos;, metrics=[&apos;accuracy&apos;]) #对模型进行编译，loss是模型训练时的损失函数，optimizer是模型使用的优化器，metrics是模型训练时的评估标准 编译完成后，要在训练的数据集上进行一定次数的迭代，为什么要多次迭代实验呢？通过按照batch进行一定次数的迭代来训练网络 model.fit(x_train,y_train,epochs=5,batch_size=32) #这段代码中的epochs表示迭代次数，batch_size表示每一次迭代每一次输入层所抽取的数据量（不知道这样表达对不对？） 在完成模型的训练之后，需要来看我们的模型是否达到我们的要求： loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) 或者说，通过我们训练出来的偶像，对新的数据进行预测： classes = model.predict(x_test, batch_size=128) batch的定义自己还不是很懂，查看官方文档。多查阅两次。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>keras系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centsos 下 python3 安装的坑]]></title>
    <url>%2F2018%2F03%2F21%2Fpython_install%2F</url>
    <content type="text"><![CDATA[参考资料： 作者：风Boy链接：https://www.jianshu.com/p/e2fc97b452de 今天在centos 7 下准备安装keras，准备安装python3 ，而后发生了一些问题，编译安装好后，需要对软连接进行处理。删除旧连接，增加新链接。还有改变yum的配置文件，默认的是python2. 准备使用easy_install 安装命令的时候出现了错误， pkg_resources.VersionConflict: (setuptools 33.1.1 (/usr/local/python3/lib/python3.6/site-packages/setuptools-33.1.1-py3.6.egg), Requirement.parse(&apos;setuptools==0.9.8&apos;)) During handling of the above exception, another exception occurred: ... ... pkg_resources.DistributionNotFound: The &apos;setuptools==0.9.8&apos; distribution was not found and is required by the application 安装好setuptool之后，还是报错 按照上面的作者那个方法，重新编译python，发现还是不行。 换用其他方法，自己觉得应该是pip的连接没有创建，于是后面又将软连接创建了。 但是创建连接之后，pip 可以使用，但是又出现了新的问题： 缺失ssh pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available 于是又用yum安装open-ssh,openssl-devel。 安装好后，又来重新编译python。（应该是必须重新编译） 编译好后，pip install 可以使用 ，安装第三方库。 但是easy_install 还是用不了。 解决方法：网上下一个setuptools工具，解压后进入解压目录输入命令： python setup.py install 之后创建软连接，保证可以在多个地方进行运行 easy_install 命令。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>centsos</tag>
        <tag>python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Spark快速大数据分析读书笔记》]]></title>
    <url>%2F2018%2F03%2F20%2Fspark_readnotes%2F</url>
    <content type="text"></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大学期间接触到的概念和知识点]]></title>
    <url>%2F2018%2F03%2F17%2Fknowledge_on_college%2F</url>
    <content type="text"><![CDATA[在大学快要毕业时，决定要写一下，自己大学都接触了哪些东西，或者说见识了哪些世面。 大学期间接触到的概念和知识点： 程序开发： c,c++,java,c#,html,css,js,python,python,golang,react,vue.js,node.js,R,haskell 概念： 前端 后端 数据库 爬虫 计算机网络 管理信息系统 信息安全 大数据 数据分析与挖掘 操作系统 数据结构和算法 编译原理 技术： 函数式编程 面向对象编程 数据库（SQL Server,DB2,Oracle,MySQL,Mariadb,PostgreSQL,HBase,Bigtable,elastic） 数据库结构化查询语言（SQL） 非结构数据库（NO SQL） 大数据（HDFS,MapReduce,hive,Pig,sqoop,spark，分布式，ETL，数据仓库） 信息安全（加解密，路由，缓存，计算机网络协议，渗透，测试，入侵检测，防火墙，木马，病毒，代理，黑产,DDOS,中间人攻击，SEO，作弊与反作弊） 系统（Linux,系统组成原理，shell，命令，内存管理，编译原理，） 数据分析与挖掘（描述性统计，探索性数据分析，ETL，EDA，统计学，SPSS，SAS，机器学习，商务智能，傅里叶变换，平滑与变换，numpy,scipy,pandas,scikit-learn,埋点，神经网络，分类与聚类，决策树，PCA,k-means,关联规则，kaggle,tf,keras） 语言的虚拟机技术 自然语言处理 其他（docker） 爬虫（抓包重放，requests，bs4，xpath，css_selector，scrapy,正则表达式,orm） web开发（django,flask,web2py） SQL调优 常见的数据结构和算法(leetcode)]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>大学，生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HSQL学习]]></title>
    <url>%2F2018%2F03%2F15%2Fhive_HQL%2F</url>
    <content type="text"><![CDATA[HQL文档 HQL包含DML、DLL操作,是否不支持DCL？ RMDB包含DML,DLL和DCL操作 DCL（data control language）:数据库控制功能：用来设置或更改数据库用户或角色的语句，并控制数据库事务发生的时间及效果，对数据库进行监视（commit,savepoint,rollback,set transaction） DDL (data definition language):主要用在定义或改变表的结构，数据类型，表与表之间的关系的工作上，主要用在建表时使用（create,alter,drop,truncat,comment,grant,revoke） DML (data mainpulation language):主要是增删改查，对数据表种的数据进行操作(select,insert,update,delete,call,explain plan,lock table) HQL_DDL:建表删除表改结构创建/修改视图创建数据库显示命令 HQL_DML向数据表种加载文件将查询结果插入到Hive表中但不支持增删改操作，只支持查操作 HQL_查询操作SELECT [ALL | DISTINCT] select_expr, select_expr, …FROM table_reference[WHERE where_condition][GROUP BY col_list [HAVING condition]][ CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]][LIMIT number]]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>HSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive内建函数]]></title>
    <url>%2F2018%2F03%2F15%2Fhive_function%2F</url>
    <content type="text"><![CDATA[(问题:hQL语句是否对大小写敏感，对语句关键字不敏感，但对表名、数据库名可能敏感) 关系运算: 常见的关系运算: 等值比较: = 不等值比较: &lt;&gt; 小于比较: &lt; 小于等于: &lt;= 大于比较: &gt; 大于等于: &gt;= 空值判断: IS NULL 非空值判断: IS NOT NULL LIKE比较: LIKE JAVAlike: RLIKE 正则操作: REGEXP 逻辑运算与数学运算:（精度不同能否参与运算） HIVE数学运算: 加法: + 减法: - 乘法: * 除法: / 取余: % 位与: &amp;&amp; AND 位或: || OR 位异或 : ^ XOR 位取反: ! NOT 数值运算:(此处有问题) 取整: round 执行精度取整: round 向下取整: floor 向上取整: ceil 向上取整: ceiling 随机取整: rand 自然指数函数: exp 10为底的对数函数: log10 2为底的对数函数: log2 对数函数: log 幂运算函数: pow 幂运算函数: power 开方函数: sqrt 2进制函数: bin 16进制函数: hex 反转16进制函数: unhex 进制转换函数: conv 绝对值函数: abs 取余函数: pmod 正弦函数: sin 余弦函数: cos 反余弦函数: acos positive函数: positive negative函数: negaitve 日期函数: unix时间戳转日期: from_unixtime 获取当前unix时间戳: unix_timestamp 日期转unix时间戳: unix_timestamp 日期时间转日期函数: to_date 日期转年函数: year 日期转月函数: month 日期转天函数: day 日期转小时函数: hour 日期转分钟函数: minute 日期转秒函数: second 日期转周函数: weekofyear 日期比较函数: datediff 日期增加函数: date_add 日期减少函数: date_sub 条件函数: if条件: if 非空查找: coalesce 条件判断函数: CASE 字符串函数: 字符串长度: length 字符串反转: reverse 字符串连接函数: concat 带分隔符字符串连接函数: concat_ws 字符串截取函数: substr,substring 字符串转大写函数: upper,ucase 字符串转小写函数: lower,lcase 去除空格函数: trim 左边去除空格含: ltrim 右边去除空格函数: rtrim 正则表达式替换函数: regexp_replace 正则表达式解析函数: regexp_extract url解析函数: parse_url json解析函数: get_json_object 空格字符串函数: space 重复字符串函数: repeat 首字符ascii函数: ascii 左补足函数: lpad 右补足函数: rpad 分割字符串函数: split 集合查找函数: find_in_set]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>概览</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[近期学习]]></title>
    <url>%2F2018%2F03%2F15%2Frecent_orders%2F</url>
    <content type="text"><![CDATA[近期学习： Java 学习，基础，能看得懂代码 Python Scrapy nutch Linux的命令 hadoop的体系结构 学习hive 学习SQL进阶 SQL进阶： order by group by having join union SQL内建函数 SQL函数 SQL调优(MySQL 数据查询的模型，数据存储的数据结构，查询的优化) Linux的命令（进阶）: 以下内容参考自：Linux的命令大全网 系统管理： 系统安全 进程和作业管理 用户和工作组管理 X-windows SELinux 文件系统管理 系统关机和重启 网络管理： 网络应用 高级网络 网络测试 网络安全 网络配置 网络服务器 软件|打印|开发|工具： 常用工具命令 软件包管理 编程开发 打印 文件目录管理： 文件查找和比较 文件内容查看 文件处理 文件编辑 目录基本操作 文件权限属性设置 文件过滤分割与合并 文件压缩与解压 文件备份和恢复 文件传输 硬件|监测|内核|shell Shell内建命令 性能监测与优化 硬件管理 内核与模块管理 磁盘管理 主要涉及： 归档操作 文件搜索 文本操作 SSH 登录 授权操作 用户管理 系统操作 文件下载 scrapy学习： 基础学习 进阶——中间件开发 数据分析报告： 统计局报告下载 近期空余时间研究： 1.网页文本提取研究：基于行快密度的算法 goose python的使用，python 开源库newspaper 2.自然语言处理 自己要去想一下，自己的研究方向： 自己目前接触到的内容有：]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-yum软件安装问题解决]]></title>
    <url>%2F2018%2F03%2F14%2Fyum_error%2F</url>
    <content type="text"><![CDATA[关于yum的使用碰到的一个问题今天在Linux下碰到这种问题： Another app is currently holding the yum lock; waiting for it to exit... 另一个应用程序是：PackageKit 内存：114 M RSS （451 MB VSZ） 查到的解决方案是： rm -rf /var/run/yum.pid 来强行解除锁定，然后就可以正常使用你的yum了 原因：yum被锁定，可能是centos7安装好后，yum是自动更新，然后就会自动设置更新，连接远程服务，所以就会有占用的yumid。而yum同时只能启动一个，所以需要将软件更新。 解决思路：或者将yum设置为不自动更新，或者yum开启后将所有软件更新，或者，将yumid给kill，但是由于软件兼容性问题，还是推荐将yum设置为不自动更新。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>问题解决</tag>
        <tag>Linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive导论]]></title>
    <url>%2F2018%2F03%2F13%2Fhive_index%2F</url>
    <content type="text"><![CDATA[hive 中文文档翻译：hive中文文档 hive gitbook Hive它是什么： 是一种数据仓库工具，基于Hadoop的一种数据仓库工具。 可以将结构化的数据文件映射为一种数据库表，并提供类SQL查询功能 本质是将HSQL转换成mapreduce程序 HDFS是文件一种系统管理系统 Hive对应着HDFS的文件 Hadoop 包含两个模块：MapReduce和HDFS。 HDFS是用于存储和处理数据集MapReduce是一种并行编程模型。 Hadoop包含了其他工具，现在是一系列的体系，包含但不限于： Sqoop:用于导入与导出数据 Pig:开发MapReduce操作语言的平台 Hive:一种数据仓库 HQL:数据仓库的工具--ETL工具 Hive的特点： 专为OLAP设计， 自带SQL类型语言：HQL, 可扩展。 常见的数据仓库模型：星形模型（面向主题的模型）雪花模型（基于星形模型的发展，核心的转变） 按照颗粒度的顺序，Hive数据被组织成： 数据库 表 分区：每个表可以有一个或多个用于决定数据如何存储的分区键 桶：每个分区的数据，基于表的一些列的哈希函数值，又被分割成桶 HIVE的数据类型： 数字（Tinyint,smallint,int,bigint,float,double,decimal） 日期/时间（timestamp,date,interval） 字符（string,varchar,char） 杂型（boolean,binary） 复杂类型（array,maps,struct） 安装启动： hive启动前需要启动hadoop（基于hadoop的HDFS）。 hive的配置文件要写好！！！ hive的配置文件要写好！！！ hive的配置文件要写好！！！ 首次的启动需要初始化schema, ./schematool -initSchema -dbType derby(或者其他的数据库，如MySQL) 安装远程模式和本地模式，初始化schema： 前提：需要修改配置文件中的主机，并且保证mysql数据库能被远程（Linux）访问 ./schematool -initSchema -dbType mysql() 启动hadoop: start-all.sh 启动hive: hive 退出hive: exit; 停止hadoop: stop-all.sh 已经搭建好的平台，最好不要更新，不然所配置的文件都要更新，而且可能会出错。 Hadoop书单： hadoop :the definitive guide（hadoop权威指南） hadoop in action hadoop in practice hive启动配置远程的数据库的时候，在启动hive前，要初始化hive]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>概览</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能设备（手机、平板）如何利用蓝牙给电脑提供网络服务]]></title>
    <url>%2F2018%2F03%2F11%2Fsurfingby_blueteeth%2F</url>
    <content type="text"><![CDATA[因为学校的原因，自己办理的联通的宽带，小米笔记本无法通过无线直接连上联通宽带，所以就尝试着从多方面拓展联通无线网的使用。 主要方法有： 1.通过数据线，将连接上wifi的手机，接入电脑usb插口，即可通过手机上网，还可以边充电边上网，对于没有网络接口和无法使用有线的电脑来说，是个很好的解决方法，而且网络稳定较蓝牙技术更为稳定。但是不知道这个通过手机上网，所传递的数据内容，是否会通过手机的内存处理，后面研究一下 2.通过蓝牙工具，通过蓝牙，将电脑所请求的网络数据发送到电脑上。方法： 2.1.电脑和手机均要能互相发现对方，并链接配对上； 2.2 手机开启设置中的通过蓝牙共享网络设置， 2.3 电脑右击蓝牙图标，选择加入个人局域网，在该页面，点击进入已连接的手机，选择连接时使用：接入点即可通过蓝牙共享网络服务。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>网络链接</tag>
        <tag>蓝牙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习解决问题方法]]></title>
    <url>%2F2018%2F03%2F09%2Fmlpro%2F</url>
    <content type="text"><![CDATA[机器学习的大致过程：预处理、特征提取与选择、分类器设计]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[傅里叶变换是什么]]></title>
    <url>%2F2018%2F03%2F09%2Ffft%2F</url>
    <content type="text"><![CDATA[以下资料参考自：http://blog.csdn.net/l494926429/article/details/51818012]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>傅里叶变换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析师技术要求]]></title>
    <url>%2F2018%2F03%2F07%2Fdata_ana_pro%2F</url>
    <content type="text"><![CDATA[数据分析师所需的技能包括但不限于：硬技能：数据分析的软件和统计学知识 工具： 查询语言：SQL,HIVE,Pig 脚本语言：Python,Matlab,shell 统计工具：R,SPSS,SAS,SPSS moderler 数据处理：Excel 编程语言：Java（scala） 软技能： 对数据的敏感程度，问题定义与发现，问题分析 数据量化能力 需要接触的构架与思想： 了解转化漏斗模型思维架构 市场主流量化工具Google Analytics、webtrends、Omniture、Double Click 机器学习算法有基本认识并能调用工具实现 完善用户拉新、激活、留存机制，从用户生命周期角度运营用户，维护用户活跃、激活沉默用户，针对不同用户类型设计有效激励机制，提升活跃度和留存率等； 计算广告 SQL性能调优 工作流程：完成可行性分析、进行数据采集、数据清洗、构建模型、性能调优、分析报告 数据分析流程： 1.数据导入 （导入本地或web的csv文） 2.数据变换 3.数据统计描述 4.探索性数据分析 5.验证性数据分析 数据分析步骤： 导入数据 数据准备、探索和清理 拟合统计模型-评估拟合结果-模型的交叉验证（循环） 在新数据上的评估模型预测 生成报告 利用Python进行数据分析： 需要掌握numpy,scipy,pandas等工具统计快速傅里叶变换平滑和滤波基线和峰值分析 数据分析是将收集的数据进行加工、整理和分析，使其转换为信息，通常的方法有：老七种工具，即排列图、因果图、分层法、调查表、散步图、直方图、控制图；新七种工具，即关联图、系统图、矩阵图、KJ法、计划评审技术、PDPC法、矩阵数据图；]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>职位分析</tag>
        <tag>数据分析师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[excel找到一行数据的第N大值的所在列]]></title>
    <url>%2F2018%2F03%2F06%2Fexcel_findtitle%2F</url>
    <content type="text"><![CDATA[今天碰到一个excel中的问题，要找到每行数据中的最大值，最小值，第二大值的所在列，并返回它们的列名 如： id c1 c2 c3 c4 c5 01 10 22 23 1 6 要找到id为01的最大值（23）、最小值（1）,第二大值（22）的所在列，并返回它们的列名 其中，找到最大值，最小值，开源分别用max(),min()函数来找到，而中间的函数可以通过其他方法，如large()函数来找到。 large()函数用法：large(范围，名次)如：large(c1:c5,2) 选择c2-c5的第二大值 在找到这些值之后，需要的是对这些值进行定位。主要涉及的方法有： index()和match() index(要找的首行范围，第m行，第n列) match(要找的值，可能包含该值的范围，排列参数) 如：index($AP$1:$AS$1，1，match(j，ap2:as2，0))表示在AP1-AS1的第一行范围内找到j所在位置，并返回该值的列名 本次解决方案：1INDEX($B$1:$M$1,1,MATCH($N2,$B2:$M2,))]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的更新语句]]></title>
    <url>%2F2018%2F02%2F27%2Fmysql_update%2F</url>
    <content type="text"><![CDATA[今天碰到这样的问题：A表结构：col1 col2 col3 B表结构：col1,col2,col3 两者表的结构相同，但是表的数据不同，现需要将数据进行修改，将A表的数据更新到B表。语句如下： update A,B set A.col1 = B.col1,A.col2 = B.col2 , A.col3 = B.col3 where]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP方向的技术要求]]></title>
    <url>%2F2018%2F02%2F20%2Fnlppro%2F</url>
    <content type="text"><![CDATA[NLP方向的技术要求但不限于： 数据结构与算法深度学习、机器学习keras、tensorflow自然语言处理、知识库NLP相关理论与技术方法java、数学正则，脚本语言中英文分词、词性标注、句法分析、自动文本分类、关键值提取（自然语言处理技术）、语义分析方法和技术（相似度计算、本体理论、语义推理）人机对话编码能力、数据结构和算法Linux 、python、c++常见的自然语言处理的算法和模型（语言模型、maxent,CRF,plsa,lda,w2v,cnn/rnn）中文分析，文本分类与聚类知识图谱n-gram]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>职位分析</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快捷键 —— sublime text3]]></title>
    <url>%2F2018%2F02%2F12%2Fsublime_text3_short_key%2F</url>
    <content type="text"><![CDATA[以下内容参考自：http://blog.csdn.net/cywosp/article/details/31791881选中类Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本。 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑。eg：快速选中并更改所有相同的变量名、函数名等。 Ctrl+L 选中整行，继续操作则继续选择下一行，效果和 Shift+↓ 效果一样。 Ctrl+Shift+L 先选中多行，再按下快捷键，会在每行行尾插入光标，即可同时编辑这些行。 Ctrl+Shift+M 选择括号内的内容（继续选择父括号）。eg：快速选中删除函数中的代码，重写函数体代码或重写括号内里的内容。 Ctrl+M 光标移动至括号内结束或开始的位置。 Ctrl+Enter 在下一行插入新行。举个栗子：即使光标不在行尾，也能快速向下插入一行。 Ctrl+Shift+Enter 在上一行插入新行。举个栗子：即使光标不在行首，也能快速向上插入一行。 Ctrl+Shift+] 选中代码，按下快捷键，展开代码。 Ctrl+K+0 展开所有折叠代码。 Ctrl+← 向左单位性地移动光标，快速移动光标。 Ctrl+→ 向右单位性地移动光标，快速移动光标。 shift+↑ 向上选中多行。 shift+↓ 向下选中多行。 Shift+← 向左选中文本。 Shift+→ 向右选中文本。 Ctrl+Shift+← 向左单位性地选中文本。 Ctrl+Shift+→ 向右单位性地选中文本。 Ctrl+Shift+↑ 将光标所在行和上一行代码互换（将光标所在行插入到上一行之前）。 Ctrl+Shift+↓ 将光标所在行和下一行代码互换（将光标所在行插入到下一行之后）。 Ctrl+Alt+↑ 向上添加多行光标，可同时编辑多行。 Ctrl+Alt+↓ 向下添加多行光标，可同时编辑多行。 编辑类Ctrl+J 合并选中的多行代码为一行。eg：将多行格式的CSS属性合并为一行。 Ctrl+Shift+D 复制光标所在整行，插入到下一行。 Tab 向右缩进。 Tab 向左缩进。 Ctrl+K+K 从光标处开始删除代码至行尾。 Ctrl+Shift+K 删除整行。 Ctrl+/ 注释单行。 Ctrl+Shift+/ 注释多行。 Ctrl+K+U 转换大写。 Ctrl+K+L 转换小写。 Ctrl+Z 撤销。 Ctrl+Y 恢复撤销。 Ctrl+U 软撤销，感觉和 Gtrl+Z 一样。 Ctrl+F2 设置书签 Ctrl+T 左右字母互换。 F6 单词检测拼写 搜索类Ctrl+F 打开底部搜索框，查找关键字。 Ctrl+shift+F 在文件夹内查找，与普通编辑器不同的地方是sublime允许添加多个文件夹进行查找，略高端，未研究。 Ctrl+P 打开搜索框。eg：1、输入当前项目中的文件名，快速搜索文件2、输入@和关键字，查找文件中函数名3、输入：和数字，跳转到文件中该行代码4、输入#和关键字，查找变量名 Ctrl+G 打开搜索框，自动带：，输入数字跳转到该行代码。举个栗子：在页面代码比较长的文件中快速定位。 Ctrl+R 打开搜索框，自动带@，输入关键字，查找文件中的函数名。举个栗子：在函数较多的页面快速查找某个函数。 Ctrl+： 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 Ctrl+Shift+P 打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 Esc 退出光标多行选择，退出搜索框，命令框等。 显示类Ctrl+Tab 按文件浏览过的顺序，切换当前窗口的标签页。 Ctrl+PageDown 向左切换当前窗口的标签页。 Ctrl+PageDown 向右切换当前窗口的标签页。 Alt+Shift+1 窗口分屏，恢复默认1屏（非小键盘的数字） Alt+Shift+2 左右分屏-2列 Alt+Shift+3 左右分屏-3列 Alt+Shift+4 左右分屏-4列 Alt+Shift+5 等分4屏 Alt+Shift+8 垂直分屏-2屏 Alt+Shift+9 垂直分屏-3屏 Ctrl+K+B 开启/关闭侧边栏。 F11 全屏模式 Shift+F11 免打扰模式]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
        <tag>sublime text3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surprise——推荐系统库的使用的数学知识]]></title>
    <url>%2F2018%2F02%2F12%2Fsome_math_knowledge%2F</url>
    <content type="text"><![CDATA[统计学知识参考自：http://blog.sina.com.cn/s/blog_628033fa0100kjjy.html————————————————————————————————————————————————————————————在接触surprise库的过程中，碰到这几个参数：MAE(平均绝对误差):Mean Absolute ErrorSSE(和方差、误差平方和)：The sum of squares due to errorMSE(均方差、方差)：Mean squared errorRMSE(均方根、标准差)：Root mean squared errorR-square(确定系数)：Coefficient of determinationAdjusted R-square：Degree-of-freedom adjusted coefficient of determination 要对它们有一定的理解，对后面的调用有一定的帮助。 SVD知识参考自：http://blog.csdn.net/zhongkejingwang/article/details/43053513————————————————————————————————————————————————————————————SVD：Singular Value Decomposition又称奇异矩阵分解，在数据处理上，用于数据降维压缩上。 cross_validation知识参考自：http://blog.csdn.net/chl033/article/details/4671750———————————————————————————————————————————————————————————— GridSearchCV知识参考自： SGD知识参考自：http://blog.csdn.net/chenzhi1992/article/details/52850759———————————————————————————————————————————————————————————— ALS知识参考自http://blog.csdn.net/lingerlanlan/article/details/44085913———————————————————————————————————————————————————————————— NMF知识参考自：http://blog.sina.com.cn/s/blog_9ce5a1b501018vb2.html———————————————————————————————————————————————————————————— slope知识参考自：http://blog.csdn.net/xidianliutingting/article/details/51916578———————————————————————————————————————————————————————————— k-fold知识参考自：https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation————————————————————————————————————————————————————————————]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>问题解决</tag>
        <tag>推荐系统</tag>
        <tag>surprise框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python2与Python3共存时如何安装各自的包]]></title>
    <url>%2F2018%2F02%2F08%2Fpython2andpython3%2F</url>
    <content type="text"><![CDATA[linux下： 安装python2和python3并保证同时存在下载、编译安装、创建链接 安装python-pip和python3-pip，利用它来安装其他包pip2 install pkg 安装2的包pip3 install pkg 安装3的包 windows重命名python3 目录下的python.exe文件为python3.exe在保证两个版本都可以用时： python3 -m easy_install pkg 安装3的包python3 -m pip install pkg 安装3的包python2 -m easy_install pkg 安装2的包python2 -m pip install pkg 安装2的包]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04的KDE安装与卸载]]></title>
    <url>%2F2018%2F02%2F08%2Fkdeofubuntu%2F</url>
    <content type="text"><![CDATA[安装安装KDE桌面apt-get install kubuntu-desktop```12345678910#### 重启### 卸载#### 卸载Kubuntu桌面```sudo apt-get remove kubuntu-desktop 卸载kde相关组件apt-get remove kde```123#### 卸载不必要的软件```sudo apt-get autoremove 清理sudo apt-get clean]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>KDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快捷键 —— Pycharm]]></title>
    <url>%2F2018%2F02%2F02%2Fpycharmshort_keys%2F</url>
    <content type="text"><![CDATA[本文参考自： http://blog.csdn.net/jlhx123456/article/details/47031589编辑（Editing）Ctrl + Space 基本的代码完成（类、方法、属性） Ctrl + Alt + Space 快速导入任意类 Ctrl + Shift + Enter 语句完成 Ctrl + P 参数信息（在方法中调用参数） Ctrl + Q 快速查看文档 Shift + F1 外部文档 Ctrl + 鼠标 简介 Ctrl + F1 显示错误描述或警告信息 Alt + Insert 自动生成代码 Ctrl + O 重新方法 Ctrl + Alt + T 选中 Ctrl + / 行注释 Ctrl + Shift + / 块注释 Ctrl + W 选中增加的代码块 Ctrl + Shift + W 回到之前状态 Ctrl + Shift + ]/[ 选定代码块结束、开始 Alt + Enter 快速修正 Ctrl + Alt + L 代码格式化 Ctrl + Alt + O 优化导入 Ctrl + Alt + I 自动缩进 Tab / Shift + Tab 缩进、不缩进当前行 Ctrl+X/Shift+Delete 剪切当前行或选定的代码块到剪贴板 Ctrl+C/Ctrl+Insert 复制当前行或选定的代码块到剪贴板 Ctrl+V/Shift+Insert 从剪贴板粘贴 Ctrl + Shift + V 从最近的缓冲区粘贴 Ctrl + D 复制选定的区域或行 Ctrl + Y 删除选定的行 Ctrl + Shift + J 添加智能线 Ctrl + Enter 智能线切割 Shift + Enter 另起一行 Ctrl + Shift + U 在选定的区域或代码块间切换 Ctrl + Delete 删除到字符结束 Ctrl + Backspace 删除到字符开始 Ctrl + Numpad+/- 展开折叠代码块 Ctrl + Numpad+ 全部展开 Ctrl + Numpad- 全部折叠 Ctrl + F4 关闭运行的选项卡 查找/替换(Search/Replace)F3 下一个 Shift + F3 前一个 Ctrl + R 替换 Ctrl + Shift + F 全局查找 Ctrl + Shift + R 全局替换 运行(Running)Alt + Shift + F10 运行模式配置 Alt + Shift + F9 调试模式配置 Shift + F10 运行 Shift + F9 调试 Ctrl + Shift + F10 运行编辑器配置 Ctrl + Alt + R 运行manage.py任务 调试(Debugging)F8 跳过 F7 进入 Shift + F8 退出 Alt + F9 运行游标 Alt + F8 验证表达式 Ctrl + Alt + F8 快速验证表达式 F9 恢复程序 Ctrl + F8 断点开关 Ctrl + Shift + F8 查看断点 导航(Navigation)Ctrl + N 跳转到类 Ctrl + Shift + N 跳转到符号 Alt + Right/Left 跳转到下一个、前一个编辑的选项卡 F12 回到先前的工具窗口 Esc 从工具窗口回到编辑窗口 Shift + Esc 隐藏运行的、最近运行的窗口 Ctrl + Shift + F4 关闭主动运行的选项卡 Ctrl + G 查看当前行号、字符号 Ctrl + E 当前文件弹出 Ctrl+Alt+Left/Right 后退、前进 Ctrl+Shift+Backspace 导航到最近编辑区域 Alt + F1 查找当前文件或标识 Ctrl+B / Ctrl+Click 跳转到声明 Ctrl + Alt + B 跳转到实现 Ctrl + Shift + I 查看快速定义 Ctrl + Shift + B 跳转到类型声明 Ctrl + U 跳转到父方法、父类 Alt + Up/Down 跳转到上一个、下一个方法 Ctrl + ]/[ 跳转到代码块结束、开始 Ctrl + F12 弹出文件结构 Ctrl + H 类型层次结构 Ctrl + Shift + H 方法层次结构 Ctrl + Alt + H 调用层次结构 F2 / Shift + F2 下一条、前一条高亮的错误 F4 / Ctrl + Enter 编辑资源、查看资源 Alt + Home 显示导航条F11书签开关 Ctrl + Shift + F11 书签助记开关 Ctrl + #[0-9] 跳转到标识的书签 Shift + F11 显示书签 搜索相关(Usage Search)Alt + F7/Ctrl + F7 文件中查询用法 Ctrl + Shift + F7 文件中用法高亮显示 Ctrl + Alt + F7 显示用法 重构(Refactoring)F5 复制 F6 剪切 Alt + Delete 安全删除 Shift + F6 重命名 Ctrl + F6 更改签名 Ctrl + Alt + N 内联 Ctrl + Alt + M 提取方法 Ctrl + Alt + V 提取属性 Ctrl + Alt + F 提取字段 Ctrl + Alt + C 提取常量 Ctrl + Alt + P 提取参数 控制VCS/Local HistoryCtrl + K 提交项目 Ctrl + T 更新项目 Alt + Shift + C 查看最近的变化 Alt + BackQuote(’) VCS快速弹出 模版(LiveTemplates)Ctrl + Alt + J 当前行使用模版 Ctrl +J 插入模版 基本(General)Alt + #[0-9] 打开相应的工具窗口 Ctrl + Alt + Y 同步 Ctrl + Shift + F12 最大化编辑开关 Alt + Shift + F 添加到最喜欢 Alt + Shift + I 根据配置检查当前文件 Ctrl + BackQuote(’) 快速切换当前计划 Ctrl + Alt + S 打开设置页 Ctrl + Shift + A 查找编辑器里所有的动作 Ctrl + Tab 在窗口间进行切换]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[近期任务]]></title>
    <url>%2F2018%2F02%2F02%2Ffurthertarget%2F</url>
    <content type="text"><![CDATA[这两天，把基本的云盘搜索综合做了第一步，后面还有两个没完成（主要原因是：后面的链接没有处理好），还有后期，对异常情况的判断，希望能达到插拔式的效果，自动判断是否失效，而不是现在当出现了问题，程序就停止工作，可以尝试使用多进程来实现。后面还有前端的编写，还有后面上线，以及数据展现的方式：如何对这么多数据进行展现，是一次性展现所有的，还是对数据进行切割，返回一部分数据，程序使用翻页的形式完成。 后面几天，完成推荐系统surprise的评分情况，以及使用实体标注，来对自然语言进行处理。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>任务想法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime text 3 小技巧 —— 如何进行文本查重并去重]]></title>
    <url>%2F2018%2F01%2F31%2Fsublime_text3skill%2F</url>
    <content type="text"><![CDATA[排序按住F9（edit-sort lines）,对文本进行排序 查找重复行并替换按住ctrl+h 调出替换面板查找字符串： ^(.+)$\r\n+注意：确保正则模式打开替换字符串：\1 点击替换所有，即可]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>sublime text3</tag>
        <tag>技巧与工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python设置左闭右开的原则]]></title>
    <url>%2F2018%2F01%2F20%2Fpython_problem0%2F</url>
    <content type="text"><![CDATA[以下资料参考自：http://blog.csdn.net/HDOJ_lin/article/details/78831868，https://www.zhihu.com/question/24883243和https://stackoverflow.com/questions/9963401/why-are-standard-iterator-ranges-begin-end-instead-of-begin-end———————————————————————————————————————————— 两个原因：1.2.人们日常使用，认为半开半闭的区间最好，且从左往右的顺序更容易读。]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surprise框架FAQ]]></title>
    <url>%2F2018%2F01%2F18%2Fsurprise_faq%2F</url>
    <content type="text"><![CDATA[你将在这里找到FAQ和其他没有在用户手册中展现的用例。 如何为每一个用户推荐Top-N我们将在这里使用movielens-100k的数据集来为每一个用户提供评分最高的top-10的电影。我们先在整个数据集上用SVD算法，然后我们为没有出现在训练集上的每一组（user，item）数据进行评分预测。我们将为每一个用户提供专属的top-10的电影名单。 代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from collections import defaultdictfrom surprise import SVDfrom surprise import Datasetdef get_top_n(predictions, n=10): &apos;&apos;&apos;Return the top-N recommendation for each user from a set of predictions. Args: predictions(list of Prediction objects): The list of predictions, as returned by the test method of an algorithm. n(int): The number of recommendation to output for each user. Default is 10. Returns: A dict where keys are user (raw) ids and values are lists of tuples: [(raw item id, rating estimation), ...] of size n. &apos;&apos;&apos; # First map the predictions to each user. top_n = defaultdict(list) for uid, iid, true_r, est, _ in predictions: top_n[uid].append((iid, est)) # Then sort the predictions for each user and retrieve the k highest ones. for uid, user_ratings in top_n.items(): user_ratings.sort(key=lambda x: x[1], reverse=True) top_n[uid] = user_ratings[:n] return top_n# First train an SVD algorithm on the movielens dataset.data = Dataset.load_builtin(&apos;ml-100k&apos;)trainset = data.build_full_trainset()algo = SVD()algo.fit(trainset)# Than predict ratings for all pairs (u, i) that are NOT in the training set.testset = trainset.build_anti_testset()predictions = algo.test(testset)top_n = get_top_n(predictions, n=10)# Print the recommended items for each userfor uid, user_ratings in top_n.items(): print(uid, [iid for (iid, _) in user_ratings]) 如何计算准确率和召回率我们可以使用下面的模型来为每一个用户计算准确率和召回率 准确率： 相关的推荐项目集合/推荐项目 召回率： 相关的推荐项目集合/相关的项目 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from collections import defaultdictfrom surprise import Datasetfrom surprise import SVDfrom surprise.model_selection import KFolddef precision_recall_at_k(predictions, k=10, threshold=3.5): &apos;&apos;&apos;Return precision and recall at k metrics for each user.&apos;&apos;&apos; # First map the predictions to each user. user_est_true = defaultdict(list) for uid, _, true_r, est, _ in predictions: user_est_true[uid].append((est, true_r)) precisions = dict() recalls = dict() for uid, user_ratings in user_est_true.items(): # Sort user ratings by estimated value user_ratings.sort(key=lambda x: x[0], reverse=True) # Number of relevant items n_rel = sum((true_r &gt;= threshold) for (_, true_r) in user_ratings) # Number of recommended items in top k n_rec_k = sum((est &gt;= threshold) for (est, _) in user_ratings[:k]) # Number of relevant and recommended items in top k n_rel_and_rec_k = sum(((true_r &gt;= threshold) and (est &gt;= threshold)) for (est, true_r) in user_ratings[:k]) # Precision@K: Proportion of recommended items that are relevant precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1 # Recall@K: Proportion of relevant items that are recommended recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1 return precisions, recallsdata = Dataset.load_builtin(&apos;ml-100k&apos;)kf = KFold(n_splits=5)algo = SVD()for trainset, testset in kf.split(data): algo.fit(trainset) predictions = algo.test(testset) precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4) # Precision and recall can then be averaged over all users print(sum(prec for prec in precisions.values()) / len(precisions)) print(sum(rec for rec in recalls.values()) / len(recalls)) 如何为每个用户计算K近邻你可以使用算法对象中的get_neighbors()方法，它是一个仅用相似度来衡量的相关算法，就像KNN算法。 下面的例子是我们返回的电影Toy Story的10NN结果，结果输出:1234567891011The 10 nearest neighbors of Toy Story are:Beauty and the Beast (1991)Raiders of the Lost Ark (1981)That Thing You Do! (1996)Lion King, The (1994)Craft, The (1996)Liar Liar (1997)Aladdin (1992)Cool Hand Luke (1967)Winnie the Pooh and the Blustery Day (1968)Indiana Jones and the Last Crusade (1989) 由于电影名称于其原始/内部ID之间的转换，有很多模板，但是这一切都归结于使用get_neighbors()方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import io # needed because of weird encoding of u.item filefrom surprise import KNNBaselinefrom surprise import Datasetfrom surprise import get_dataset_dirdef read_item_names(): &quot;&quot;&quot;Read the u.item file from MovieLens 100-k dataset and return two mappings to convert raw ids into movie names and movie names into raw ids. &quot;&quot;&quot; file_name = get_dataset_dir() + &apos;/ml-100k/ml-100k/u.item&apos; rid_to_name = &#123;&#125; name_to_rid = &#123;&#125; with io.open(file_name, &apos;r&apos;, encoding=&apos;ISO-8859-1&apos;) as f: for line in f: line = line.split(&apos;|&apos;) rid_to_name[line[0]] = line[1] name_to_rid[line[1]] = line[0] return rid_to_name, name_to_rid# First, train the algortihm to compute the similarities between itemsdata = Dataset.load_builtin(&apos;ml-100k&apos;)trainset = data.build_full_trainset()sim_options = &#123;&apos;name&apos;: &apos;pearson_baseline&apos;, &apos;user_based&apos;: False&#125;algo = KNNBaseline(sim_options=sim_options)algo.fit(trainset)# Read the mappings raw id &lt;-&gt; movie namerid_to_name, name_to_rid = read_item_names()# Retrieve inner id of the movie Toy Storytoy_story_raw_id = name_to_rid[&apos;Toy Story (1995)&apos;]toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)# Retrieve inner ids of the nearest neighbors of Toy Story.toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)# Convert inner ids of the neighbors into names.toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id) for inner_id in toy_story_neighbors)toy_story_neighbors = (rid_to_name[rid] for rid in toy_story_neighbors)print()print(&apos;The 10 nearest neighbors of Toy Story are:&apos;)for movie in toy_story_neighbors: print(movie) 自然而然，对于只做稍改的用户同样适用。 如何序列化一个算法预测算法可以使用dump()和load()函数进行序列化和加载，SVD算法是在数据集上进行训练并进行序列化的，然后再次导入，用于后面的计算。12345678910111213141516171819202122232425import osfrom surprise import SVDfrom surprise import Datasetfrom surprise import dumpdata = Dataset.load_builtin(&apos;ml-100k&apos;)trainset = data.build_full_trainset()algo = SVD()algo.fit(trainset)# Compute predictions of the &apos;original&apos; algorithm.predictions = algo.test(trainset.build_testset())# Dump algorithm and reload it.file_name = os.path.expanduser(&apos;~/dump_file&apos;)dump.dump(file_name, algo=algo)_, loaded_algo = dump.load(file_name)# We now ensure that the algo is still the same by checking the predictions.predictions_loaded_algo = loaded_algo.test(trainset.build_testset())assert predictions == predictions_loaded_algoprint(&apos;Predictions are the same&apos;) 可以将算法连同其一起序列化，以便可以使用pandas dataframe来进一步分析或于其他算法进行比较。下面的两个是例子：KNN算法的转储和分析两种算法的比较 如何建立自己的预测算法整个手册在这里 说明是raw 和inner idsuser和item都有原生(raw)的id和内置(inner)的id，一些方法可能会返回原生id（如predict()方法），有一些返回内置id。 原生id是从评分文件或 pandas 的dataframe中定义的。它们可以是字符串或者是数字。要注意，如果评分数据是从标准的脚本文件中读取的，那么它们就是以字符串的形式展现的。你要知道当你使用像predict()或者其他方法那样接受原生id作为参数的时候，这一点很重要。 在测试集的构建中，每个原生的id会被映射到被称作是inner id的唯一整数中——能更适用于surprise地操作。原生id 和inner id的相互转化方法可以通过调用训练集中的 to_inner_uid(), to_inner_iid(), to_raw_uid()和to_raw_iid()方法实现。 我能使用使用自己数据集吗？数据集能是pandas dataframe形式吗？可以，都可以，查看这里 如何调用算法参数你可以按照这里的所说的算法来使用GridSearchCV 类调整算法参数。 调优之后，你可能希望对算法性能进行无偏估计。 在训练集上如何获取准确度量你可以使用Trainset对象中的build_testset()方法来构建测试集，测试集可以使用test()方法来调用。 1234567891011121314151617from surprise import Datasetfrom surprise import SVDfrom surprise import accuracyfrom surprise.model_selection import KFolddata = Dataset.load_builtin(&apos;ml-100k&apos;)algo = SVD()trainset = data.build_full_trainset()algo.fit(trainset)testset = trainset.build_testset()predictions = algo.test(testset)# RMSE should be low as we are biasedaccuracy.rmse(predictions, verbose=True) # ~ 0.68 (which is low) 查看样例文件以获取更多用例。 如何保存数据以进行无偏估计如果你的目标是调用算法参数，你可能需要留出部分数据对算法性能进行无偏估计。例如：你可能需要将数据分为两组，一组用户girdsearch进行参数调整，另一组用于无偏估计。1234567891011121314151617181920212223242526272829303132333435363738394041424344import randomfrom surprise import SVDfrom surprise import Datasetfrom surprise import accuracyfrom surprise.model_selection import GridSearchCV# Load the full dataset.data = Dataset.load_builtin(&apos;ml-100k&apos;)raw_ratings = data.raw_ratings# shuffle ratings if you wantrandom.shuffle(raw_ratings)# A = 90% of the data, B = 10% of the datathreshold = int(.9 * len(raw_ratings))A_raw_ratings = raw_ratings[:threshold]B_raw_ratings = raw_ratings[threshold:]data.raw_ratings = A_raw_ratings # data is now the set A# Select your best algo with grid search.print(&apos;Grid Search...&apos;)param_grid = &#123;&apos;n_epochs&apos;: [5, 10], &apos;lr_all&apos;: [0.002, 0.005]&#125;grid_search = GridSearchCV(SVD, param_grid, measures=[&apos;rmse&apos;], cv=3)grid_search.fit(data)algo = grid_search.best_estimator[&apos;rmse&apos;]# retrain on the whole set Atrainset = data.build_full_trainset()algo.fit(trainset)# Compute biased accuracy on Apredictions = algo.test(trainset.build_testset())print(&apos;Biased accuracy on A,&apos;, end=&apos; &apos;)accuracy.rmse(predictions)# Compute unbiased accuracy on Btestset = data.construct_testset(B_raw_ratings) # testset is now the set Bpredictions = algo.test(testset)print(&apos;Unbiased accuracy on B,&apos;, end=&apos; &apos;)accuracy.rmse(predictions) 如何构建可再现的实验一些算法随机地初始化参数（有时与numpy一起），交叉验证折叠也是随机产生地，如果你需要多次再现你的实验，你只需要在你的程序开始前设置RNG的种子(seed)就行了。123456import randomimport numpy as npmy_seed = 0random.seed(my_seed)numpy.random.seed(my_seed) 数据存储在哪，如何改变它的位置默认情况下，通过surprise下载的数据集是被存储在用户目录下的/surprise_data文件夹下，也可以将其存放到其他位置，你只需要通过设置’SURPRISE_DATA_FOLDER’参数就可以改变默认目录了。]]></content>
      <categories>
        <category>翻译草稿</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>surprise框架</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime text3打开txt文件乱码问题及解决]]></title>
    <url>%2F2018%2F01%2F18%2Fproblems_codefault%2F</url>
    <content type="text"><![CDATA[参考自： segmentfault —————————————————————————————————— 自己今天用Python写TXT文件的时候，使用记事本打开没有乱码，而使用sublime text3打开时，就发生了乱码的情况，这是怎么回事呢？ 自带的记事本保存TXT的编码是GBK，而sublime text3不支持GBK编码，所以需要自行转换一下。但是python写入txt文件，为什么是GBK格式呢？]]></content>
      <categories>
        <category>草稿</category>
      </categories>
      <tags>
        <tag>sublime text3</tag>
        <tag>问题解决</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surprise框架之如何建立你自己的预测算法（翻译）]]></title>
    <url>%2F2018%2F01%2F17%2Fsurprise_predict_yourown%2F</url>
    <content type="text"><![CDATA[本节将会向你展示如何通过surprise建立一个个性化的预测算法。 基础想自己来试一下吗？ 构建你自己的预测算法是很简单的：每一个算法都只是从AlgoBase类中派生出来的，它有一个自己的被称作是predict()的评估方法。它需要传递进去 一个inner user id , 一个inner item id ，查看这里，返回的是一个评估率 r^ui:实例代码：123456789101112131415161718192021from surprise import AlgoBasefrom surprise import Datasetfrom surprise.model_selection import cross_validateclass MyOwnAlgorithm(AlgoBase): def __init__(self): # Always call base method before doing anything. AlgoBase.__init__(self) def estimate(self, u, i): return 3data = Dataset.load_builtin(&apos;ml-100k&apos;)algo = MyOwnAlgorithm()cross_validate(algo, data, verbose=True) 这个算法是我们可以想到的最糟糕的：它只是预测评分为3，而不管用户和项目。 如果你想存储关于预测的额外信息，你也可以返回一个带有给定细节的字典:12345def estimate(self, u, i): details = &#123;&apos;info1&apos; : &apos;That was&apos;, &apos;info2&apos; : &apos;easy stuff :)&apos;&#125; return 3, details 这个字典会被作为details字段存储到prediction中，以便后面分析用到。 fit 方法我们现在来试一下构建一个稍微聪明点的算法——他能预测所有训练集中的评分数据的平均值。 因为他只是一个不依赖于现有的用户或u物品的常量，所以，我们可以只计算一次。通过定义fit方法即可完成：12345678910111213141516171819202122class MyOwnAlgorithm(AlgoBase): def __init__(self): # Always call base method before doing anything. AlgoBase.__init__(self) def fit(self, trainset): # Here again: call base method before doing anything. AlgoBase.fit(self, trainset) # Compute the average rating. We might as well use the # trainset.global_mean attribute ;) self.the_mean = np.mean([r for (_, _, r) in self.trainset.all_ratings()]) return self def estimate(self, u, i): return self.the_mean fit方法被称为例如 通过在交叉验证过程的每个折叠处的cross_validate函数（但是您也可以自己调用它）。 在做任何事之前，你应该调用基类fit()方法。 说明：fit()方法返回self，所以，这也说明可以使用类似于algo.fit(trainset).test(testset) 的表达式。 训练集的属性当基类的fit()方法返回后，你需要了解的关于现有的训练集的信息都存储在self.trainset的属性中了。它是一个有很多有关预测的属性和方法的对象。 举例说明它的用法，我们会做一个预测所有评分的均值、所有用户的评分均值和物品评分的平均值的均值。（有语病）代码：12345678910111213def estimate(self, u, i): sum_means = self.trainset.global_mean div = 1 if self.trainset.knows_user(u): sum_means += np.mean([r for (_, r) in self.trainset.ur[u]]) div += 1 if self.trainset.knows_item(i): sum_means += np.mean([r for (_, r) in self.trainset.ir[i]]) div += 1 return sum_means / div 说明：注意，在fit方法中计算所有用户的均值可能是一个更好的主意，从而避免多次重复相同的计算。 何时进行预测是否能进行预测是你来决定的。如果你觉得可以进行预测了，那就抛出PredictionImpossible异常，但是你要先导入这个：1from surprise import PredictionImpossible 这个异常能被predict()方法捕获，估计量r^ui将被传递到全局变量——评分u的均值上。 使用相似度量和基准估计如果你的算法使用相似度度量或者基准估计，那么你需要将bsl_options和sim_options传递进去作为初始化方法init的参数，并沿着Base class 进行传递。在这节查看如何使用使用这些参数。 compute_baselines()方法和compute_similarities()方法能被fit方法回调（或者说其他的任何地方）：123456789101112131415161718192021222324252627282930313233class MyOwnAlgorithm(AlgoBase): def __init__(self, sim_options=&#123;&#125;, bsl_options=&#123;&#125;): AlgoBase.__init__(self, sim_options=sim_options, bsl_options=bsl_options) def fit(self, trainset): AlgoBase.fit(self, trainset) # Compute baselines and similarities self.bu, self.bi = self.compute_baselines() self.sim = self.compute_similarities() return self def estimate(self, u, i): if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)): raise PredictionImpossible(&apos;User and/or item is unkown.&apos;) # Compute similarities between u and v, where v describes all other # users that have also rated item i. neighbors = [(v, self.sim[u, v]) for (v, r) in self.trainset.ir[i]] # Sort these neighbors by similarity neighbors = sorted(neighbors, key=lambda x: x[1], reverse=True) print(&apos;The 3 nearest neighbors of user&apos;, str(u), &apos;are:&apos;) for v, sim_uv in neighbors[:3]: print(&apos;user &#123;0:&#125; with sim &#123;1:1.2f&#125;&apos;.format(v, sim_uv)) # ... Aaaaand return the baseline estimate anyway ;) 随意探索一下prediction_algorithms包源代码，以了解可以做什么。]]></content>
      <categories>
        <category>翻译草稿</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>surprise框架</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surprise框架之使用预测算法（翻译）]]></title>
    <url>%2F2018%2F01%2F16%2Fsurprise_predict%2F</url>
    <content type="text"><![CDATA[surprise框架提供了一堆内建的算法。所有算法都来自AlgoBase基类，其中实现了一些关键方法（例如预测，拟合和测试）。 可用预测算法的列表和详细信息可以在prediction_algorithms包文档中找到。 每个算法都是surprise的全局命名空间的一部分，所以你仅仅只需从surprise包中导入他们的名字即可，例如:12from surprise import KNNBasicalgo = KNNBasic() 其中一些算法可能使用基准估计，有些可能使用相似性度量。我们将在这里回顾如何配置基线和相似性的计算方法。我们来回顾一下基准估计和相似度估计. 基准估计设置说明： 本节只适用于最小化正则化平方误差（或等价）的算法（或相似性度量）： 对于在另一个目标函数（例如SVD算法）中使用基线的算法，基线配置是不同的，对每个算法都是特定的。请参考他们自己的文档。 首先，如果你不想配置通过基准估计的方法，你也就无需进行修改，默认的参数也能很好的完成。如果你想更好，那这就是为你准备的。 你可能需要阅读这里,来了解什么是基准估计。基准可以用两种方式进行估算： 使用随机梯度下降 使用交替最小二乘法 你可以配置使用bsl_options在创建算法时传递参数来计算基准的方法。这个参数是一个字典，其中’method’表示要使用的方法，接受的值是’ALS’(默认)和’SGD’，根据其值，可以设置其他选项，对于ALS: ‘reg_i’:表示项目的正规化参数，默认值是10； ‘reg_u’:表示用户的正规化参数，默认值是15； ‘n-epochs’: 表示ALS过程的迭代次数，默认是10. 对于SGD: ‘reg’:表示优化的成本函数的正规化参数，默认值是0.02 ‘learing_rate’: 表示SGD的学习率，默认值是0.05 ‘n-epochs’:表示SGD的迭代次数，默认值是20 对于两个处理方式（ALS和SGD），用户和物品的偏差都被初始化为0。 使用案例：ALS:1234567print(&apos;Using ALS&apos;)bsl_options = &#123;&apos;method&apos;: &apos;als&apos;, &apos;n_epochs&apos;: 5, &apos;reg_u&apos;: 12, &apos;reg_i&apos;: 5 &#125;algo = BaselineOnly(bsl_options=bsl_options) SGD: 12345print(&apos;Using SGD&apos;)bsl_options = &#123;&apos;method&apos;: &apos;sgd&apos;, &apos;learning_rate&apos;: .00005, &#125;algo = BaselineOnly(bsl_options=bsl_options) 说明：一些相似性度量可能会用到基准，例如pearson_baseline相似。无论在实际的预测中是否用到r^ui，配置的工作都是相同的。12345bsl_options = &#123;&apos;method&apos;: &apos;als&apos;, &apos;n_epochs&apos;: 20, &#125;sim_options = &#123;&apos;name&apos;: &apos;pearson_baseline&apos;&#125;algo = KNNBasic(bsl_options=bsl_options, sim_options=sim_options) 这将引导我们进行相似性度量配置，我们现在将对此进行回顾。 相似度估计配置许多算法使用相似度度量来评分。 ‘name’：在similarities模块中定义的相似度的名称 。默认是’MSD’。 ‘user_based’：是否在用户之间或项目之间计算相似度。这对预测算法的性能有很大的影响。默认是True。 ‘min_support’：共同项目的最小数量（当’user_based’ 是’True’）或最小数量的普通用户 ‘shrinkage’：要应用的收缩参数（仅与pearson_baseline相似性相关 ）。默认值是100。 代码：1234sim_options = &#123;&apos;name&apos;: &apos;cosine&apos;, &apos;user_based&apos;: False # compute similarities between items &#125;algo = KNNBasic(sim_options=sim_options) 12345sim_options = &#123;&apos;name&apos;: &apos;pearson_baseline&apos;, &apos;shrinkage&apos;: 0 # no shrinkage &#125;algo = KNNBasic(sim_options=sim_options)See also]]></content>
      <categories>
        <category>翻译草稿</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>surprise框架</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[surprise框架之起步（翻译）]]></title>
    <url>%2F2018%2F01%2F16%2Fsurprise_getstatted%2F</url>
    <content type="text"><![CDATA[基本使用自动进行交叉验证surprise 有一系列的内建的算法和数据集供用户来进行尝试。在最简单的形式中，它的模型仅需几行代码就可以运行一次交叉验证的流程：代码如下： 12345678910111213from surprise import SVDfrom surprise import Datasetfrom surprise.model_selection import cross_validate# Load the movielens-100k dataset (download it if needed),data = Dataset.load_builtin(&apos;ml-100k&apos;)# We&apos;ll use the famous SVD algorithm.algo = SVD()# Run 5-fold cross-validation and print resultscross_validate(algo, data, measures=[&apos;RMSE&apos;, &apos;MAE&apos;], cv=5, verbose=True) 运行之后的结果应该是如下所示(实际结果可能会因随机性而异)：1234567Evaluating RMSE, MAE of algorithm SVD on 5 split(s). Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Mean StdRMSE 0.9311 0.9370 0.9320 0.9317 0.9391 0.9342 0.0032MAE 0.7350 0.7375 0.7341 0.7342 0.7375 0.7357 0.0015Fit time 6.53 7.11 7.23 7.15 3.99 6.40 1.23Test time 0.26 0.26 0.25 0.15 0.13 0.21 0.06 如果数据集（movielens_100k dataset）还没有下载，load_builtin()方法会提供下载，并将数据集保存在用户目录下的.surprise_data文件夹下，当然，你也可以选择其他地方进行保存。 我们将使用著名的SVD模型，当然其他算法模型亦可以，可以通过查看链接获取更多细节。 cross-validation()方法通过使用cv参数来进行交叉验证的过程。通过计算准确率来衡量效果。我们这里使用了经典的五次交叉验证，但是也可以使用更精彩的交叉验证（查看资料）。 Train-test划分和fit()方法如果您不想运行完整的交叉验证程序，则可以使用train_test_split()对给定尺寸的trainset和testset进行采样，并使用您选择的精度度量。 你需要选择fit()方法，fit()方法可以在训练集上来训练算法，test()方法将会返回从测试集上生成的预测值。 实例代码如下：123456789101112131415161718192021from surprise import SVDfrom surprise import Datasetfrom surprise import accuracyfrom surprise.model_selection import train_test_split# Load the movielens-100k dataset (download it if needed),data = Dataset.load_builtin(&apos;ml-100k&apos;)# sample random trainset and testset# test set is made of 25% of the ratings.trainset, testset = train_test_split(data, test_size=.25)# We&apos;ll use the famous SVD algorithm.algo = SVD()# Train the algorithm on the trainset, and predict ratings for the testsetalgo.fit(trainset)predictions = algo.test(testset)# Then compute RMSEaccuracy.rmse(predictions) 返回的结果是0.9411 在此说明：你可以通过下面一行代码来训练并测试一个算法。1predictions = algo.fit(trainset).test(testset) 在一些例子中，你的训练集和测试集数据已经被另一些例子所定义，请参考此处来处理这些问题。 训练整个测试集数据和predict()方法毫无疑问，我们可以简单地将算法运用到我们的整个数据集，而不是使用交叉验证。这种方法可以通过使用build_full_trainset()方法来实现，该方法可以建立一个trainset对象。 实例代码如下： 123456789101112from surprise import KNNBasicfrom surprise import Dataset# Load the movielens-100k datasetdata = Dataset.load_builtin(&apos;ml-100k&apos;)# Retrieve the trainset.trainset = data.build_full_trainset()# Build an algorithm, and train it.algo = KNNBasic()algo.fit(trainset) 我们可以通过predict()方法来预测评分。我们假设您对用户196和302物品感兴趣(确保他们在训练集中)，并且你知道正确的评分是4，通过下面的代码可以实现计算: 12345uid = str(196) # raw user id (as in the ratings file). They are **strings**!iid = str(302) # raw item id (as in the ratings file). They are **strings**!# get a prediction for specific users and items.pred = algo.predict(uid, iid, r_ui=4, verbose=True) 计算结果:1user: 196 item: 302 r_ui = 4.00 est = 4.06 &#123;&apos;actual_k&apos;: 40, &apos;was_impossible&apos;: False&#125; 说明： predict()使用原始ID(请阅读关于[原始ID和内部ID](http://surprise.readthedocs.io/en/stable/FAQ.html#raw-inner-note)的信息)。 由于我们使用的数据集已经从文件中读取，原始ID是字符串（即使它们代表数字） 使用用户的自定义数据集尽管surprise有一系列的内建数据集，但是你还是能够使用用户自定义的数据集。可以通过文件导入(csv)或者一个pandas dataframe导入。此外，你需要为surprise定义一个reader对象来解析文件或者数据框。 为载入一个文件(如csv)，你可以使用load_from_file()方法来实现。代码如下：1234567891011121314151617from surprise import BaselineOnlyfrom surprise import Datasetfrom surprise import Readerfrom surprise.model_selection import cross_validate# path to dataset filefile_path = os.path.expanduser(&apos;~/.surprise_data/ml-100k/ml-100k/u.data&apos;)# As we&apos;re loading a custom dataset, we need to define a reader. In the# movielens-100k dataset, each line has the following format:# &apos;user item rating timestamp&apos;, separated by &apos;\t&apos; characters.reader = Reader(line_format=&apos;user item rating timestamp&apos;, sep=&apos;\t&apos;)data = Dataset.load_from_file(file_path, reader=reader)# We can now use this dataset as we please, e.g. calling cross_validatecross_validate(BaselineOnly(), data, verbose=True) 其他更多关于阅读器和如何使用的信息，详情查看reader class 文档。 说明： 正如您在前一节中已经知道的那样，Movielens-100k数据集是内置的，因此加载数据集的更快的方法是执行data = Dataset.load_builtin(&apos;ml-100k&apos;)。 我们当然会在这里忽略这个。 为载入一个从pandas dataframe 的数据集，你需要使用load_from_df()方法。你同样需要一个reader对象吗，但只需指定rating_scale参数即可。dataframe必须要有三列数据，包括用户id,物品id,用户对物品的评分数据。因此每一列都需要给定评分。这不是限制性的，因为您可以轻松地重新排列数据框的列。实例代码如下: 12345678910111213141516171819202122import pandas as pdfrom surprise import NormalPredictorfrom surprise import Datasetfrom surprise import Readerfrom surprise.model_selection import cross_validate# Creation of the dataframe. Column names are irrelevant.ratings_dict = &#123;&apos;itemID&apos;: [1, 1, 1, 2, 2], &apos;userID&apos;: [9, 32, 2, 45, &apos;user_foo&apos;], &apos;rating&apos;: [3, 2, 4, 3, 1]&#125;df = pd.DataFrame(ratings_dict)# A reader is still needed but only the rating_scale param is requiered.reader = Reader(rating_scale=(1, 5))# The columns must correspond to user id, item id and ratings (in that order).data = Dataset.load_from_df(df[[&apos;userID&apos;, &apos;itemID&apos;, &apos;rating&apos;]], reader)# We can now use this dataset as we please, e.g. calling cross_validatecross_validate(NormalPredictor(), data, cv=2) dataframe的初始情况可能如下: itemID rating userID 0 1 3 91 1 2 322 1 4 23 2 3 454 2 1 user_foo 使用交叉验证迭代对于交叉验证，我们可以使用cross_validate()方法，它能为我们做所有的困难的工作。但为了更好的控制，我们还可以实例化交叉验证迭代器，并使用迭代器的split()方法和算法的test()方法对每个分裂进行预测。下面地案例是我们使用经典地k次交叉验证地过程，分裂了三次。代码如下:123456789101112131415161718192021from surprise import SVDfrom surprise import Datasetfrom surprise import accuracyfrom surprise.model_selection import KFold# Load the movielens-100k datasetdata = Dataset.load_builtin(&apos;ml-100k&apos;)# define a cross-validation iteratorkf = KFold(n_splits=3)algo = SVD()for trainset, testset in kf.split(data): # train and test algorithm. algo.fit(trainset) predictions = algo.test(testset) # Compute and print Root Mean Squared Error accuracy.rmse(predictions, verbose=True) 可能的结果：RMSE: 0.9374RMSE: 0.9476RMSE: 0.9478 其他的交叉验证迭代的方法也可以使用，如留一法和ShuffleSplit。在此处查看所有的迭代器。surprise的交叉验证的涉及工具是受scikit-learnAPI工具的启发。 一个特殊的交叉验证案例是当其他文件预先定义了folds。例如:movielens-100k数据集已经提供了5个训练和测试的文件。surprise通过surprise.model_selection.split.PredefinedKFold对象来解决这个问题。实例代码: 12345678910111213141516171819202122232425262728293031from surprise import SVDfrom surprise import Datasetfrom surprise import Readerfrom surprise import accuracyfrom surprise.model_selection import PredefinedKFold# path to dataset folderfiles_dir = os.path.expanduser(&apos;~/.surprise_data/ml-100k/ml-100k/&apos;)# This time, we&apos;ll use the built-in reader.reader = Reader(&apos;ml-100k&apos;)# folds_files is a list of tuples containing file paths:# [(u1.base, u1.test), (u2.base, u2.test), ... (u5.base, u5.test)]train_file = files_dir + &apos;u%d.base&apos;test_file = files_dir + &apos;u%d.test&apos;folds_files = [(train_file % i, test_file % i) for i in (1, 2, 3, 4, 5)]data = Dataset.load_from_folds(folds_files, reader=reader)pkf = PredefinedKFold()algo = SVD()for trainset, testset in pkf.split(data): # train and test algorithm. algo.fit(trainset) predictions = algo.test(testset) # Compute and print Root Mean Squared Error accuracy.rmse(predictions, verbose=True) 当然，如果你仅通过导入单一的文件来进行训练，用单一的文件来测试，谁也阻止不了你。然而，folds_files参数仍需要是一个list。 使用GridSearchCV调整算法参数cross_validate()函数针对给定的一组参数报告交叉验证过程的准确性度量。如果你想知道哪个参数组合可以产生最好的结果，那么GridSearchCV类就可以解决问题。 给定一个参数的字典，这个类彻底地尝试所有的参数组合，并报告任何准确性度量（在不同的分割平均）的最佳参数。 它受到scikit-learn的GridSearchCV的启发。下面的按理是我们使用不同的值来设置SVD算法的n_epochs,lr_all和reg_all参数.代码如下:123456789101112131415161718from surprise import SVDfrom surprise import Datasetfrom surprise.model_selection import GridSearchCV# Use movielens-100Kdata = Dataset.load_builtin(&apos;ml-100k&apos;)param_grid = &#123;&apos;n_epochs&apos;: [5, 10], &apos;lr_all&apos;: [0.002, 0.005], &apos;reg_all&apos;: [0.4, 0.6]&#125;gs = GridSearchCV(SVD, param_grid, measures=[&apos;rmse&apos;, &apos;mae&apos;], cv=3)gs.fit(data)# best RMSE scoreprint(gs.best_score[&apos;rmse&apos;])# combination of parameters that gave the best RMSE scoreprint(gs.best_params[&apos;rmse&apos;]) 运行结果:120.961300130118&#123;&apos;n_epochs&apos;: 10, &apos;lr_all&apos;: 0.005, &apos;reg_all&apos;: 0.4&#125; 我们在这里评估3倍交叉验证过程的平均RMSE和MAE，但是可以使用任何交叉验证迭代器。一旦调用了fit()，best_estimator属性为我们提供了一个具有最佳参数集的算法实例，我们可以使用它：12algo = gs.best_estimator[&apos;rmse&apos;]algo.fit(data.build_full_trainset()) 说明：如bsl_options和sim_options等字典型参数需要特殊处理，如: 12345param_grid = &#123;&apos;k&apos;: [10, 20], &apos;sim_options&apos;: &#123;&apos;name&apos;: [&apos;msd&apos;, &apos;cosine&apos;], &apos;min_support&apos;: [1, 5], &apos;user_based&apos;: [False]&#125; &#125; 自然而然，两者可以组合使用，如KNNBaseline算法：1234567param_grid = &#123;&apos;bsl_options&apos;: &#123;&apos;method&apos;: [&apos;als&apos;, &apos;sgd&apos;], &apos;reg&apos;: [1, 2]&#125;, &apos;k&apos;: [2, 3], &apos;sim_options&apos;: &#123;&apos;name&apos;: [&apos;msd&apos;, &apos;cosine&apos;], &apos;min_support&apos;: [1, 5], &apos;user_based&apos;: [False]&#125; &#125; 为了进一步分析，cv_results属性具有所有需要的信息，并且可以在pandas dataframe中导入：1results_df = pd.DataFrame.from_dict(gs.cv_results) 在我们的案例中，cv_results属性可能是这样的（已经转成了浮点型数据）：1234567891011121314151617181920&apos;split0_test_rmse&apos;: [1.0, 1.0, 0.97, 0.98, 0.98, 0.99, 0.96, 0.97]&apos;split1_test_rmse&apos;: [1.0, 1.0, 0.97, 0.98, 0.98, 0.99, 0.96, 0.97]&apos;split2_test_rmse&apos;: [1.0, 1.0, 0.97, 0.98, 0.98, 0.99, 0.96, 0.97]&apos;mean_test_rmse&apos;: [1.0, 1.0, 0.97, 0.98, 0.98, 0.99, 0.96, 0.97]&apos;std_test_rmse&apos;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]&apos;rank_test_rmse&apos;: [7 8 3 5 4 6 1 2]&apos;split0_test_mae&apos;: [0.81, 0.82, 0.78, 0.79, 0.79, 0.8, 0.77, 0.79]&apos;split1_test_mae&apos;: [0.8, 0.81, 0.78, 0.79, 0.78, 0.79, 0.77, 0.78]&apos;split2_test_mae&apos;: [0.81, 0.81, 0.78, 0.79, 0.78, 0.8, 0.77, 0.78]&apos;mean_test_mae&apos;: [0.81, 0.81, 0.78, 0.79, 0.79, 0.8, 0.77, 0.78]&apos;std_test_mae&apos;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]&apos;rank_test_mae&apos;: [7 8 2 5 4 6 1 3]&apos;mean_fit_time&apos;: [1.53, 1.52, 1.53, 1.53, 3.04, 3.05, 3.06, 3.02]&apos;std_fit_time&apos;: [0.03, 0.04, 0.0, 0.01, 0.04, 0.01, 0.06, 0.01]&apos;mean_test_time&apos;: [0.46, 0.45, 0.44, 0.44, 0.47, 0.49, 0.46, 0.34]&apos;std_test_time&apos;: [0.0, 0.01, 0.01, 0.0, 0.03, 0.06, 0.01, 0.08]&apos;params&apos;: [&#123;&apos;n_epochs&apos;: 5, &apos;lr_all&apos;: 0.002, &apos;reg_all&apos;: 0.4&#125;, &#123;&apos;n_epochs&apos;: 5, &apos;lr_all&apos;: 0.002, &apos;reg_all&apos;: 0.6&#125;, &#123;&apos;n_epochs&apos;: 5, &apos;lr_all&apos;: 0.005, &apos;reg_all&apos;: 0.4&#125;, &#123;&apos;n_epochs&apos;: 5, &apos;lr_all&apos;: 0.005, &apos;reg_all&apos;: 0.6&#125;, &#123;&apos;n_epochs&apos;: 10, &apos;lr_all&apos;: 0.002, &apos;reg_all&apos;: 0.4&#125;, &#123;&apos;n_epochs&apos;: 10, &apos;lr_all&apos;: 0.002, &apos;reg_all&apos;: 0.6&#125;, &#123;&apos;n_epochs&apos;: 10, &apos;lr_all&apos;: 0.005, &apos;reg_all&apos;: 0.4&#125;, &#123;&apos;n_epochs&apos;: 10, &apos;lr_all&apos;: 0.005, &apos;reg_all&apos;: 0.6&#125;]&apos;param_n_epochs&apos;: [5, 5, 5, 5, 10, 10, 10, 10]&apos;param_lr_all&apos;: [0.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.01, 0.01]&apos;param_reg_all&apos;: [0.4, 0.6, 0.4, 0.6, 0.4, 0.6, 0.4, 0.6] 正如你所看到的，每个列表具有相同大小的参数组合数量。 它对应于下表：123456789split0_test_rmse split1_test_rmse split2_test_rmse mean_test_rmse std_test_rmse rank_test_rmse split0_test_mae split1_test_mae split2_test_mae mean_test_mae std_test_mae rank_test_mae mean_fit_time std_fit_time mean_test_time std_test_time params param_n_epochs param_lr_all param_reg_all0.99775 0.997744 0.996378 0.997291 0.000645508 7 0.807862 0.804626 0.805282 0.805923 0.00139657 7 1.53341 0.0305216 0.455831 0.000922113 &#123;‘n_epochs’: 5, ‘lr_all’: 0.002, ‘reg_all’: 0.4&#125; 5 0.002 0.41.00381 1.00304 1.00257 1.00314 0.000508358 8 0.816559 0.812905 0.813772 0.814412 0.00155866 8 1.5199 0.0367117 0.451068 0.00938646 &#123;‘n_epochs’: 5, ‘lr_all’: 0.002, ‘reg_all’: 0.6&#125; 5 0.002 0.60.973524 0.973595 0.972495 0.973205 0.000502609 3 0.783361 0.780242 0.78067 0.781424 0.00138049 2 1.53449 0.00496203 0.441558 0.00529696 &#123;‘n_epochs’: 5, ‘lr_all’: 0.005, ‘reg_all’: 0.4&#125; 5 0.005 0.40.98229 0.982059 0.981486 0.981945 0.000338056 5 0.794481 0.790781 0.79186 0.792374 0.00155377 5 1.52739 0.00859185 0.44463 0.000888907 &#123;‘n_epochs’: 5, ‘lr_all’: 0.005, ‘reg_all’: 0.6&#125; 5 0.005 0.60.978034 0.978407 0.976919 0.977787 0.000632049 4 0.787643 0.784723 0.784957 0.785774 0.00132486 4 3.03572 0.0431101 0.466606 0.0254965 &#123;‘n_epochs’: 10, ‘lr_all’: 0.002, ‘reg_all’: 0.4&#125; 10 0.002 0.40.986263 0.985817 0.985004 0.985695 0.000520899 6 0.798218 0.794457 0.795373 0.796016 0.00160135 6 3.0544 0.00636185 0.488357 0.0576194 &#123;‘n_epochs’: 10, ‘lr_all’: 0.002, ‘reg_all’: 0.6&#125; 10 0.002 0.60.963751 0.963463 0.962676 0.963297 0.000454661 1 0.774036 0.770548 0.771588 0.772057 0.00146201 1 3.0636 0.0597982 0.456484 0.00510321 &#123;‘n_epochs’: 10, ‘lr_all’: 0.005, ‘reg_all’: 0.4&#125; 10 0.005 0.40.973605 0.972868 0.972765 0.973079 0.000374222 2 0.78607 0.781918 0.783537 0.783842 0.00170855 3 3.01907 0.011834 0.338839 0.075346 &#123;‘n_epochs’: 10, ‘lr_all’: 0.005, ‘reg_all’: 0.6&#125; 10 0.005 0.6 命令行的使用surprise 同样支持命令行工具，例如：1surprise -algo SVD -params &quot;&#123;&apos;n_epochs&apos;: 5, &apos;verbose&apos;: True&#125;&quot; -load-builtin ml-100k -n-folds 3 查看使用细节运行：1surprise -h]]></content>
      <categories>
        <category>翻译草稿</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>surprise框架</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于农村秸秆燃烧地想法]]></title>
    <url>%2F2018%2F01%2F14%2Fmind0%2F</url>
    <content type="text"><![CDATA[昨天晚上遭遇了一场持久的农村燃烧秸秆的浓烟，昨天晚上估计燃烧了一晚上。 昨天先是把窗户打开，想着浓烟尽快散去，但是发现似乎浓烟一直都在，然后就等着，把窗户关上了。之后再把窗户打开通风，发现浓烟还在…… 直至今早，房间里似乎依然还是有浓烟的味道，感觉很是不爽。 早上起来感觉鼻腔疼痛，喉咙不舒服，有些痒，而后，就把窗户继续打开通风。 由此自己昨天晚上半夜想了一下关于农村秸秆回收利用的事情。 自己的思路大致分为两种： 第一种，是不对秸秆进行燃烧回收，直接回田处理，辅以能快速分解的菌类，使得稻草的有机物能迅速变成无机物，便于来年植物的吸收。 第二种，是由专业的商业公司对秸秆进行二次回收，利用回收来的秸秆进行二次利用。 第一种，是避免空气污染的主要方式，且环保无污染，成本低。需要政府、科研所、村民的配合。科研所在新技术的发展下，假设能研制出一种新型菌类，只能分解秸秆，且能高效快速分解。 在政府的要求下，这两年使用大型收割机进行收割的农户，大多选择了就地还田，也有部分选择将其作为燃料，供给日常燃料。在田中的稻草，哪怕经过曝晒，其中仍然含有水分，所以有农户半夜偷偷燃烧时，会使其产生大量浓烟，污染空气，这种浓烟中含有大量SO2,NO2等对人体有害的气体，而且浓烟中还有一些没有完全燃烧的炭。这些污染物都能对人体造成伤害。 在政府的要求下，收割机在收割时将秸秆处理碾碎成微小颗粒，并将菌类按照一定比例混在粉末中，在菌类的分解下，稻田中的颗粒能迅速分解，将有机物还田，实现元素的二次利用，并且政府向村民公布这种方式的好处。其实为了不让农户在夜间燃烧秸秆，现在也有部分收割机加装了相关设备，在收割时已经将秸秆截断了。 第二种，是由专业的商业公司对秸秆进行二次回收，利用回收来的秸秆进行生产。需要政府、商业公司、农户的配合。 在农户进行采收时，也有部分农户选择将秸秆作为燃料，这种情况下，可以在收割时，选用相应的工具，在收割时，将秸秆进行捆绑，便于农户进行移动； 如若农户不需要秸秆。在收割季开始之前，商业公司（可能包括新能源电厂、生产菌类的农场，生产饲料的畜牧业公司）和收割团队商议，一同去农田，收割时，商业公司选择生产条件，电厂可以利用燃烧这种电进行发电，虽说这种发电是种季节性的，但是可以作为新能源方式来处理，由国家提供成熟的生产指导技术，以地域划分，建立生个小规模的电站还是可观的，可以供给企业用电。 而这新型利用秸秆进行烧热发电的技术，就需要科研所的技术团队来进行攻关了。 也可以将秸秆处理成饲料，供给动物食用。 亦或者是对其处理，使之能生产菌类蔬菜。 现在的科技进步导致的是农村的人口越来越少，愿意种田的人也越来越少。乡下空气很好，但是每烧一次秸秆，就会导致下风方的村民十分难受。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>任务想法</tag>
        <tag>环保</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《如何有效整理信息》读书笔记]]></title>
    <url>%2F2018%2F01%2F13%2Fmanageinfo%2F</url>
    <content type="text"><![CDATA[在日常生活中，我经常发现需要手中有一个笔记本，来记录自己的感悟和发现，也需要整理自己的生活。 自己用过的的工具有印象笔记、有道云笔记、OneNote、wuderlist等工具，但是发现似乎都没有手写笔记本来得高效。 作者推荐使用笔记本时要使用三个原则： 一元化，即所有想记录地一切都记录其中 时序化，为笔记添加日期，作者建议六位日期（年后面两位+月份两位+日期两位） 索引化，在一本笔记完成之后，可以自行整理笔记，对笔记建立索引。便于后期自行搜寻 一元化书中提到一元化笔记的三个优点： 简洁而简单，不会造成混乱 可以无压力地持续使用 自由度高，能够随意调整 作者建议将自己所有想记录的东西都记录在其中，这便是一元化。 有时候，当自己拿起笔，就会发现东西会越写越多的情况。记录内容包括但不限于自己的一日三餐的感受、遇事的感受、处理问题的情景。 在方便时，可以将笔记本携带身上，随时记录，在不方便时，可以先用其他可以书写的材料代替，比如小便签，写上自己想记录的内容，待方便时，再将其粘贴到笔记本上。 时序化时序化，为笔记添加日期，作者建议六位日期，也可以用八位日期，都行，将笔记持续记录，记录笔记的日期，便于以后的索引与整理。也可以帮助自己记录在什么时候记录了，什么时候没有记录。 索引化在一本笔记本整理完后，需要对笔记本进行复盘与整理，要建立笔记的索引。也不必所有的事件都需要建立索引，可以选择性地将自己认为重要地部分，进行索引。 当笔记本比较多时，可以考虑使用电子表格进行索引。这个便于以后存档与搜索。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>读书笔记</tag>
        <tag>资源管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP算法概览]]></title>
    <url>%2F2018%2F01%2F12%2Fkmpagl%2F</url>
    <content type="text"><![CDATA[在用匹配字符串(A)对目标字符串(B)进行匹配的过程中，会出现多次遍历的情况，当A越长，要进行匹配的次数越多，且一旦匹配失效，其时间成本在急剧上升。在这里，结合其他前辈的经验，理解一下大名鼎鼎的KMP算法的思路吧。 解决思路： 在进行匹配之前，需要计算匹配字符串的部分匹配值。 1.查找目标字符串中第一个与匹配字符串中第一个字符相等的位置，将A移动到该位置，开始匹配； 2.若匹配字符串的第N（N&gt;=2）个字符能匹配目标字符串，则继续匹配，直至， 若（1）在目标字符串中找到匹配字符串，则结束此轮查找； 若（2）在目标字符串中没有查找到匹配字符串，记录当前已成功匹配的数量x和匹配字符串该位置下的部分匹配值t,计算移动位数k=x-t，移动k位，进行下一轮匹配，执行1,2 字符串A:ABCDABD字符串B：AHADABFABCDABCDABD 有两个概念：已匹配字符数和对应的部分匹配值已匹配的字符数：在字符串B中，找到已匹配字符串A的数量对应的部分匹配值：字符串A中，到达某位置时，该部分的字符串的前缀和后缀的公共字符串的长度。 前缀：某字符串除开尾字符外的顺序字符串的集合。后缀：某字符串除开首字符串的顺序字符串的集合。 以A为例：ABCDABD到达A时：前缀和后缀都为0,公共字符串长度为0 到达AB时：前缀：A后缀：B公共字符串长度为0 到达ABC时：前缀：A,AB后缀：BC,C公共字符串长度为0 到达ABCD时：前缀：A,AB,ABC后缀：D,CD,BCD公共字符串长度为0 到达ABCDA时：前缀：A,AB,ABC,ABCD后缀：A,DA,CDA,BCDA公共字符串长度为1，为A 到达ABCDAB时：前缀：A,AB,ABC,ABCD,ABCDA后缀：B,AB,DAB,CDAB,BCDAB公共字符串长度为2，为AB 到达ABCDABD时：前缀：A,AB,ABC,ABCD,ABCDA,ABCDAB后缀：D,BD,ABD,DABD,CDABD,BCDABD公共字符串长度为0 还是以开头的字符串为例：匹配字符串：ABCDABD目标字符串：AHADABFABCDABCDABD当开始匹配时：1.A匹配A，匹配数为1，部分匹配值为0，移动位数为1-0=1，B不与H匹配，往前移动1位，2.A不与H匹配，往前移动1位,3.A与A匹配，B与D不匹配，匹配数为1，部分匹配值为0，移动位数为1-0=1，往前移动1位4.D与A不匹配，往前移动1位，5.A与A匹配，B与B匹配，F与C不匹配，匹配数为2，部分匹配值为0，往前移动2-0=2位，6.A与A匹配，B与B匹配，C与C匹配，D与D匹配，A与A匹配，B与B匹配,C与D不匹配，匹配数为6，部分匹配值为2，移动位数为6-2=4位， A与A匹配,B与B匹配,C与C匹配，D与D匹配，A与A匹配，B与B匹配,D与D匹配，匹配成功。距离为1+1+1+1+2+4=10 “部分匹配”的实质是，有时候，字符串头部和尾部会有重复。 参考资料：阮一峰的博客。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《釜山行影评》]]></title>
    <url>%2F2018%2F01%2F12%2Fmoviecomments1%2F</url>
    <content type="text"><![CDATA[电影很好看，故事发生在韩国，以丧尸感染人类为背景。 故事开头是发生在白天，一司机在收费站经历消毒之后，回家路上因为要接电话分神，开车撞死了一头鹿，下车之后，但是司机就检查了一下，然后开车走了。之后，等司机离开，死去的鹿，抽搐了两下就站立起来。 男主（一基金公司的经理）的公司似乎是做了什么不可告人的项目，男主让下属（金代理）处理好，虽然说是不知道什么内容，但是写在这里，应该是有意图的。在处理好公事后，询问金代理最近孩子们都喜欢什么。最后，男主回家带回来一份儿童节就已经送过的礼物，但男主并不知道，等女儿指出来，父女俩就很尴尬了。与此同时，男主的女儿想去见远在釜山的母亲，因为明天就是她的生日，而妻子也想见见女儿。而男主为何会和妻子分开，电影中借孩子的口吻反映出来一部分原因：基金公司的经理，在长期的工作习惯中，一直是趋利避害，努力在保证着自己的利益–也就是所谓的自私，自私的性格，让两人分开。 男主的母亲告诉男主昨天发生在学校开家长会的事情，并把录好了的录影机交给男主。在看完视频之后，男主经过反复思考，决定陪女儿去釜山。半夜开车去车站，准备送到女儿后，中午回来继续工作。由此，父女俩踏上了死亡列车（或者说是希望列车）。但是，在列车开动之前，发生了意外，但也是必然吧：一被感染的人上了车。故事就由此正式展开 。 开车后，小萝莉（男主的女儿）发现车站的工作人员被扑倒，被吓了，但是，没有告诉熟睡中的父亲。转而起身去上厕所，遇到了男二号，成庆（貌似姓金）和他妻子。电影中的富贵男，发现了流浪汉长期呆在厕所里，就转告乘务人员，乘务人员就在这里处理这个事情，富贵男告诉小萝莉（秀安）如果不好好学习，以后就会成为像流浪汉一样的人（这时在输出他自己的价值观），而秀安反驳到，妈妈说这种话的人，都不是好人（事实上故事中他的行为证明了这一点）。与此同时，女乘务员发现了被感染的那个女子。在呼叫列车乘务长的过程中，女子彻底被感染，开始撕咬人类。由于都没有防备，被感染的人类快速尸变，并快速蔓延开来，很快，整个车厢里就感染了好几节。男主被电话吵醒，在接完电话后，发现女儿不见了。就起身找女儿，在寻找的过程中，发现了车厢中的动荡，在一探究竟之后，发现了丧尸，转身往回跑的过程，发现女儿，抱起女儿往回跑，但是没想到，回来的路也有了丧尸，在男二（成庆）的掩护之下，成功躲过一劫，但是男主在跑到安全车厢后（也就是没有人被感染的车厢时），立马把门关上，把成庆和他媳妇儿留在外面，幸亏成庆手快，打开了门，进去之后快速锁好了门。男主发现丧尸的智慧低下，不能开门，就提醒成庆，可以把手从门上拿下来。这时成庆开始发飙：你小子跑路的时候，我给你打掩护，结果你先到了，竟然还把门关上了，成庆的媳妇就拉住他，提醒他别说了。而他们发现，丧尸只攻击看到的人，于是，成庆媳妇儿就用报纸和水把透明的车厢门封上了，丧尸的动静马上安静下来。 第一部分的高潮到此结束。 在众人逐渐安定下来的时候，石安告诉女儿，在危险的情况下，自己永远是最重要的，不要给别人让座，要学会保护好自己。 之后，男主接到了母亲打来的电话，母亲问他们的行程情况，并让他（石安）照顾好女儿。呼吸变重，挂掉了电话（推测应该也是被感染了，但是石安也没仔细想）。 众人通过各种手段，了解到国内发生的情形，发言人声明国内只是在发生暴乱，请国民安心，不要随便外出，不要相信各种谣言。但是众人也已经知道事实，人心惶惶。 列车长告诉乘客，将在天安鞍山站停靠，所有人下车。但是，当列车逐渐开到车站的时候，发现，车站已被丧尸占领，列车只好往前开。列车长告诉乘客：下一站停靠大田站，有乘客下车，将会有军队和警车保卫他们离开。石安通过联系公司在大田的人员（闵大尉），了解到，大田将会对他们进行隔离。石安以利诱，让闵大尉安排他们让他们父女俩不被隔离。闵大尉答应了，告诉他们下车出站之后，不要走主广场，走旁边的通道。 第二波高潮来了： 当众人听从列车员的安排，都下车出站时，石安和秀安走的旁边的通道，而众人走的是主通道。秀安问为什么不和众人走在一起时，秀安指责父亲的自私。而那位流浪汉在乘乱逃出，听到石安和闵大尉的谈话之后，跟随他们走的旁边的通道。当众人坐电梯下楼时，发现了尸变了的警察和军队。可是这个时候，已经晚了，众人开始往回逃。流浪汉在看到警察之后，往警察那边跑，却发现警察尸变了，吓得腿软了。石安发现之后，往回跑，提醒女儿快跑，发现尸变了的警察已经跑到了女儿的身边，多亏了男二号成庆和他媳妇，救了孩子一命。众人就开始抵御丧尸的攻击，石安也在此时受到了攻击，多亏流浪汉用大衣遮住丧尸的脸，石安得以逃过一劫。在几个棒球队员、石安和成庆的合力辅助下，大部分没被感染的人已经安全上车了。但是富贵男提醒乘务员让列车长开车，棒球队的啦啦队队长珍熙坚决反对，因为她的朋友们还没上车。乘务员绝对让列车长发车。棒球队员等人觉得拖延时间够了，就转身去赶车，但是车已经发动了，车站上的人已经被感染，咬伤了几个人，最后，经过一番搏斗，几个人里只剩下了三个人：石安、成庆和珍熙的男友（虽说他没表白，但两人的关系在几人上车的时候，就已经表明）。 在之后的联系中，发现：秀安和成庆的媳妇儿，以及故事中的姐妹花中的姐姐，被困在十三节车厢，而他们在第九节车厢，珍熙和大部分人在第十五节车厢。几人商议决定：一起前往十三节，救出几人，然后去十五节车厢和大部分人会合。 第二波高潮结束在角色分配上：成庆自己排在第一，棒球男排在第二，石安殿后。在经历了与一节车厢丧尸的搏斗之后，体力不足。但是他们发现丧尸不能在暗的环境下看见，但能听见。于是石安心生一计:在火车过隧道的时候，用手机吸引丧尸的注意力，乘机跑过去。 于是，三人就以这种方式跑到了第十三节车厢。棒球男联系珍熙，珍熙告诉列车员情况，一群人会过去。但是此时的富贵男从中作梗，或者说说出了其他人的心声：这些人从那边跑过来，谁知道有没有被感染，于是坚决反对。并将珍熙绑住，把门用衣服领带给固定住，不让他们过来，但是此时，被困到第十四节车厢的他们，车门还没被关紧，几个人负责堵住丧尸，一个人负责砸门。为了掩护石安，成庆被丧尸咬了，门终于关上了。但是成庆也被感染了，成庆将怀孕的妻子托付给石安，请求他照顾好，并让他赶紧带着其他人跑向刚砸开的门。其他人都安全地离开了。但是此时，姐妹花中的姐姐却被困住了，在门关上时，被丧尸感染了。其他人一致要求刚进来的人走向第十六节车厢，要求把他们隔离。姐妹花中的妹妹发现因为这群人的原因，自己的姐姐本来可以进来，却被感染了，心里捣鼓了几句，乘其他人在封锁第十五、十六节车厢的车门的时候，打开了第十四、十五节车厢的车门。 第三波高潮结束 而第十五节中，只有两个人没被感染：富贵男和一乘务员，和秀安他们一样，因为躲在厕所而幸免。 列车继续前进。第十六节车厢里只剩了：珍熙（在众人要求石安和棒球男等人离开，被隔离时同棒球男一同离开）、成庆的媳妇儿、棒球男、石安、流浪汉和秀安。整趟列车中只有这些人和列车长没被感染。而此时石安接到了金代理的电话，此次事件可能是由于他们在那个项目中的打捞的油轮上带起来的病毒引起的。金代理表示很后悔，不知道这件事对不对，石安听到消息后，很震惊，之后，听金代理说釜山已经安全了。军队掌管了所有权，已经安全隔离了。金代理挂了电话之后，石安拼命清洗手上的血污。 列车在前进的过程中，到站了，是不打算停的，但是列车长发现了道路被出轨的列车挡住了。列车长决定换车。列车长在启动机车之后，下车提醒其他人（安全的）坐火车头去釜山。此时对面冲过来一辆火车头，撞翻了一辆满载着丧尸的列车，丧尸就快冲出来了。而石安、秀安、流浪汉和成庆媳妇被压在两辆车之间，天无绝人之路，石安发现了一个车轱辘之间的缝隙可以通行，于是先爬出来，看清楚周围没有丧尸之后，让其他人赶紧爬过去。而此时丧尸从被压碎的玻璃窗中跑出来了。流浪汉挡住了他们，让秀安和孕妇爬了出去。 富贵男在出卖乘务员之后，冲出了列车（没关车门），导致身后一大波丧尸跟随。首先遇到了珍熙和棒球男，打开他们关上的车门，没有关，反而把一旁在等男友砸开车窗的珍熙推给了丧尸，棒球男发现之后，就停止了砸窗，转身去救珍熙，而此时富贵男就趁机撞开车窗，跑了出去。珍熙被感染之后，棒球男就一直抱着她，直到自己被感染…… 列车长在遇到富贵男时，就起身与富贵男身后的丧尸搏斗，结果也被感染了。富贵男独自一人趁机爬上了车头。石安、秀安与成庆媳妇看到了行进中的车头，起身追赶。逐渐追上了，而身后的丧尸也追了上来。于是看到了一场类似于打群架的情形：一群人追着几个人。身后的丧尸在扒上了车头之后，由于人数过多，都拖在地上，都开始让列车减速。于是石安就开始用脚踢这些丧尸。渐渐地，丧尸的数量少了，速度提起来了，最后，丧尸就完全被驱逐干净了。 三人在走向驾驶室时，发现了已经被感染的富贵男。富贵男说明想回家，想石安带他回家，但是很快就尸变了，尸变之后，两人开始搏斗。在搏斗的过程中，石安不幸被感染。 石安为了两人的安全，将两人锁在驾驶室，期望成庆的媳妇儿照顾好女儿之后，自己跳下了车头，跳下车头之前，石安是愉悦的：回忆起女儿刚出生时的情形，自己十分高兴。 两人安全到达隔离带，士兵请求上级如何处置未知人士时，上峰命令击毙。而秀安在此时唱起了歌曲：那首她在学校没唱完的歌曲，狙击手没有开枪，故事在一群士兵包围两人的情形中结束。嗯，三千六百多字的故事梗概，内容不是特别详细。 就自己看完这部电影，觉得在故事中，涉及到了一下几部分的内容： 1. 资本的力量的强大，在爆发问题时，资本的力量，可以让有钱人权贵可以暂时免受灾难的侵扰。但是当灾难无法避免和消除时，只能保留一部分的安全，甚至这一部分人的安全也不能保证。资本可以让人享受特权对待。 2. 人性是不可考验的，在故事中，石安对成庆的所作所为，以及众人对石安、成庆等人的隔离、姐妹花妹妹为了和姐姐团聚打开车门，富贵男的自私等等，都是基于人性在面对危难时的个人反映。 人性有好有坏，在面对危难境地时，看站在人们面前的是哪种，是哪种人提出来的，在关键时刻提出来的声音，往往会成为人们共同的心声（人们在有多个选择时，更多的时候，会被周围人感染，大众心理会倾向于一种发声）。 3. 人性的自私，在危难时刻最容易体现。资本是自私的，但资本却是由人掌控的。权贵是自私的，因为他们有钱、权，在正常的时候，他们可以享受更高质量的生活。因此，他们不希望改变，人性中的自私会更充分体现。 故事中也有很多美好： 石安母亲对石安的爱，石安对秀安的爱（父女，母子的亲情）； 成庆对妻子的爱，表面上是妻管严，实际上对妻子的爱，无以言表； 棒球男对珍熙的爱，虽说没有说出口，但是却陪伴到最后； 姐妹花中妹妹对姐姐的依恋，对姐姐生活地同情与爱等等。 在这些人一同前进的时候，我也感受到了人性的美好，人性中的善良。哪怕刚开始，石安对成庆的所作所为，但是毕竟后面石安还是改过来了。当三人前往十三节车厢救人时，棒球男没有勇气对自己昔日的队友下手，哪怕其他两人已经深陷困境，这之间的犹豫，也很美好。 尽管电影中，每当他们深陷困境时，总有好运降临，助他们躲过重重劫难。艺术做到这里也就足够了，虽说有些假。 总之，这部电影真的很好看。这部电影真的很好看。这部电影真的很好看。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络初探——HTTP协议]]></title>
    <url>%2F2018%2F01%2F12%2FHTTP%2F</url>
    <content type="text"><![CDATA[HTTP协议构建于TCP/IP协议之上，默认端口号是80。 HTTP协议是无连接无状态的。 HTTP协议是以ASCII码传输，建立在TCP/IP协议上的规范。 HTTP报文（请求和响应） 请求报文 报文分为三部分：状态行、请求头、消息主体。 HTTP定义了四种客户端与服务器端交互的方法： GET/POST/PUT/DELETE,对应着四种操作：查、增、改、删。 1.GET用于信息获取，有URL长度的限制2.POST可能修改服务器上的资源请求，大小没有限制，但是出于安全考虑。服务器在实现时会做一定的限制。 响应报文 报文也由三部分组成：状态行、响应头、响应正文 常见的响应状态码： 200 客户端请求成功，服务端发送正常数据301 客户端请求失败，请求被永久重定向302 客户端请求失败，请求被暂时重定向304 文件未修改，可以直接使用缓存文件400 请求错误，服务器无法解析401 请求未经授权， 这个状态代码必须和WWW-Authenticate报头域一起使用403 服务器收到请求，但是拒绝提供服务404 服务器收到请求，但是服务器上没有资源500 服务器发生不可预知的错误，导致无法完成客户端的请求503 服务器当前不能处理客户端的请求，一段时间后，可能会恢复正常]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析之探索性分析]]></title>
    <url>%2F2018%2F01%2F12%2FEDA%2F</url>
    <content type="text"><![CDATA[探索性数据分析是指对已有数据通过作图、制表、计算特征量等手段探索数据的结构和规律的一种数据分析方法。特别是当我们队这些数据中的信息没有足够的经验，不知道该用何种传统统计方法进行分析是，探索性数据分析就会非常有效。课程内容主要包括描述性统计分析、多维数据的可视化、降维技术、数据模型的可视化。 描述统计分析：以揭示数据分布特性的方式汇总并表达定量数据的方法。 包括数据的频数分析、数据的集中趋势分析、数据的离散程度分析、数据的分布以及一些基本的统计图形。描述性统计是一类统计方法的汇总，作用是提供了一种概括和表征数据的有效且相对简便的方法。通常用图示法来表述，易于看懂，能发现质量特性值的分布状况、趋势走向的一些规律，便于采取措施。用于汇总和表征数据，通常是对数据进一步定量分析的基础，或是对推断性统计方法的有效补充。 常见的描述性统计方法分为三类： 数据的统计量描述，如均值、方差。 图示技术描述，如直方图、散布图、趋势图、排列图、条形图和饼分图 文字语言分析和描述，如统计分析表、分层、因果图、亲和图和流程图。 常见的描述性统计的应用范围：适用于能够收集到定量数据的所有领域，提供有关产品、过程或质量管理体系的信息，也用于管理。 多元数据和多维数据的区别？ 以下方法来自：http://staff.ustc.edu.cn/~zwp/teach/MVA/Lec2_slides.pdf多元数据的可视化方法：1.几何投影方法2.基于像素的可视化技术3.层次可视化技术4.基于图标的可视化结束 以下方法来自：http://www.afenxi.com/post/1643降维技术的方法：缺失值比率低方差滤波高相关滤波随机森林/组合树主成分分析反向特征消除前向特征构造 数据模型的可视化 ——————————未完待续……]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>探索性分析</tag>
        <tag>数据分析与挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动手写爬虫概览]]></title>
    <url>%2F2018%2F01%2F12%2Fcrawl0%2F</url>
    <content type="text"><![CDATA[编写的爬虫代码的主要涉及的过程： 获取网页数据–提取数据–数据的清洗和处理–数据存储。 以下是编写爬虫程序所需要安装的库。在这里，默认大家在安装Python之时已经安装好pip这个工具。 1.获取网页数据阶段，可能会用到的库：requests,urllib2 进入控制台界面： pip install urllib2 pip install requests 2.提取数据需要用到的库：bs4,re,lxml pip install bs4 3.数据清洗和处理：re,JSON 4.数据存储：mysqldb（SQL数据库）,pymongo（NOSQL）,xlwt（excel） pip install pymongo pip install xlwt mysqldb的库需要去其他地方下载：https://sourceforge.net/projects/mysql-python/]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>概览</tag>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动手写爬虫总结1]]></title>
    <url>%2F2018%2F01%2F12%2Fcrawl1%2F</url>
    <content type="text"><![CDATA[爬虫总结： 自2016年十月中旬至此，前后花费了一个半月写爬虫，也是平时零零散散写的爬虫，从最简单的页面获取，到页面数据解析，再到后面的翻页，保存到Excel中，再到获取动态网页，加载JSON数据，到最后的将数据保存到数据库，中间踩的坑又不少，最近就开始简单的总结吧。 自己当初使用的是Python 2.7的环境 1.从最简单的页面获取而言，有几个库（模块）的使用，用到了urllib2,urllib,requests等。 具体的使用，通过urllib2获取网页的HTML代码，使用的过程中，为了防止爬虫被ban，在获取某些页面的过程中，添加了浏览器的headers，将爬虫伪装成浏览器去访问页面，常见的伪装就是加上USER-AGENT（UA）,也有部分网页即使加上了headers也没用，这个时候，就要往爬虫的headers方面去下功夫，到底是什么限制了爬虫的访问。 目前，在爬取个人的关注方面出现了问题。为了更像浏览器，在获取多个页面的时候，就要提前设置访问的间隔时间，使用time()这个系统库，再利用的是random()库，获取任意的（0.0-1）时间，用获取到的时间进行处理，最后，间隔时间在5s以上为佳。 2.在页面获取之后，要对页面的数据（html代码）进行处理。 主要的方式，就是通过BeautifulSoup这个库，来解析页面，解析的依赖库，主要就是html.parser和lxml这两个解析器，其中，html.parser和lxml这两个解析器各有优劣，但是对lxml而言，更好的处理方式，是通过DOM来解析，能获得更好的效果，在写爬虫程序前，要分析爬虫的获取的数据，分析网页的结构，获取正确的位置，可以借助F12这个浏览器的工具，来快速定位所需要的信息的位置。 在页面解析的过程中，除开BS4这个库之外，还使用到了re（正则表达式）这个模块，许多要获取的信息，在通过BS4解析之后，还是要进一步的清洗，提取出关键的数据，此时需要学习正则表达式的基本知识。尽管在后面，有提到可以使用string这个库的使用，在一定程度上可以减缓正则表达式在解析页面过程中的使用。但是这都是后知后觉了。 3.获取需要的数据后，要对数据进行存储。 自己最开始学习使用的是用的文件存储，比如，使用python的文件读写，写入到txt格式，但是在此的过程中，发现任务量比较大，如果使用txt格式存储，就不太适合了，到后面所有的数据都是用的Excel格式存储的。主要使用的库是XLWT，在写入的过程中，发现某一列数据可能会写入多行，这个时候，定义下函数来存储就是比较好的了。 4.获得JSON数据，刚开始接触到这个，其实是在文章页面，获得文章信息之后，需要再获取评论的信息，在评论区，是通过动态加载而来，第一页数据使用的Ajax将评论异步加载过来的，在网络上寻求帮助之后，提供了几个建议： a.通过浏览器加载渲染，就是实际上通过浏览器来访问，并且获得评论的信息，但是发现这种方式的效率太低，而且对电脑要求的比较高，需要下载Webdriver和PhantomJS 来解析JS b.到后面，自己在反复尝试之后，发现，可以直接访问JSON数据，就是从浏览器发送请求，然后服务器端发送相关的数据页面，这样的方式既达到了自己的效果，又使得电脑的负载更低，使用的方向有8K+的基本信息的页面，以及每个人文章的评论的内容，当然，获取的JSON数据并不是我们需要的，我们需要处理，处理成干净的JSON数据，然后利用JSON这个库，来处理，将其处理成字典数据，最后，在字典的里面，通过键-值来提取我们需要的数据。 5.连接到数据库，这一部分，主要是同学完成的。其中的重难点，应该是数据库的设计，因为在爬取数据的过程中，发现了许多的问题，并不是像老师说的那么简单就行，表格之间有关系，而一张表又不可能写完所有的信息，在此，就想着使用数据库来存储数据。 6.IP代理的问题，在实验室，实际上就只有一个IP，这样限制了爬虫的速度，不可能爬取太快，一旦稍微速度快了，就会错误，网站拒绝提供服务，会提醒DDOS攻击（10054）错误。因此，就想着在此基础上，想着爬取网络上的IP代理，使用代理访问（主要是高匿代理），但是目前，高匿代理，在许多代理网站上都是没用的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析的学习概览]]></title>
    <url>%2F2018%2F01%2F12%2Fclusteragl%2F</url>
    <content type="text"><![CDATA[聚类学习是一种无监督学习，是可以根据数据集中的自相似进行自动分类。 聚类的结果希望达成类内大相似，类间不相似的分类效果。 聚类分析的方法有很多，列举下：基于划分的聚类、基于层次的聚类、基于密度的聚类、基于网格的聚类和基于模型的聚类等几种。 其中，基于层次聚类的聚类方法有：凝聚法和分裂法。 凝聚法是自底向上，逐步聚类，一开始将每个样本作为一个单独的组，然后相继地合并相近的对象或组，直到所有的组合并成一个，或者达到一个终止条件。 分裂法是自顶向下，逐步递归。一开始将所有样本置于一个簇中，在迭代的每一步中，一个簇被分裂为更小的簇，直到最终每个样本在单独的一个簇中。（这个不是分类算法） 基于密度的方法，代表算法是DBSCAN和OPTICS算法。 其中，DBSCAN是根据一个密度阈值来控制簇的增长。而 OPTICS算法是另一种基于密度的方法，它为自动的和交互的聚类分析计算一个聚类顺序。 基于划分的方法有：K-均值、K-众数、CLARA、CLARANS、FCM等。 基于网格的方法把样本空间量化为有限数目的单元，形成了一个网格结构，所有的聚类操作都在这个网格结构上进行。这种方法的主要优点是它的处理速度很快，其处理时间独立与数据样本的数目，只与量化空间中的每一位的单元数目有关。典型的方法有：STING和WaveCluster。 基于模型的方法，为每一个簇假定了一个模型，寻找数据对给定模型的最佳组合。一个基于模型的算法可能通过构建反映数据点空间分布的密度函数来定位聚类。它也基于标准的统计数字自动决定聚类的数目，考虑“噪声”数据或孤立点，从而产生健壮的聚类方法，典型的方法有：COBWEB和CLASSIT。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概览</tag>
        <tag>聚类分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析之K-means算法]]></title>
    <url>%2F2018%2F01%2F12%2Fclusteragl1%2F</url>
    <content type="text"><![CDATA[聚类分析是根据数据之间的自相似自动分类的。 分类的结果是，类内大相似，类间不相似或小相似。 聚类的分类方法有： 1.划分 2.层次 3.基于密度 4.基于网络 5.基于模型 划分聚类：K-Means算法将数据集划分为K个簇，每隔簇内部的样本都相似。主要步骤： 1.确定K个簇。 2.计算距离并进行归类 3.重新定K。 当出现单一条件不同、标准不同时，需要对数据进行归一化处理。 常见的有：(x-min)/（max-min）但是K-means由于其本身选取的是均值作为？ 有一定的局限性： 1.不适用于非数值型数据，如分类数据，序列数据，且容易受极值的影响 2.K-means算法是对初始值敏感的，如果起初随意定初始值很可能无法迭代到本该的最优点。 3.K-means算法倾向于聚类后的半径相近，倾向于密度均匀的聚类 4.无法聚类出有型的点（一般只能用于球形） 鉴于K-means算法的缺点，可以使用，K-mediods，K-modes,K-prototype，二分 K-means算法进行替代 层次聚类分为凝聚法和分裂法。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>聚类分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树算法——概述]]></title>
    <url>%2F2018%2F01%2F12%2Fdecisiontree0%2F</url>
    <content type="text"><![CDATA[决策树是一种基本的分类和回归方法。 决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。主要优点是模型具有可读性、分类速度快。 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。 决策树学习通常包括3个步骤：特征选择、决策树的生成与决策树的剪枝。 主要包括ID3算法，C4.5算法和CART算法。 决策树模型 分类决策树模型是一种描述对实力进行分类的树形结构。决策树由节点和有向边组成，节点有两种类型：内节点和叶节点，内节点表示一个特征或属性，叶节点表示一个类。 用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点；这时，mri一个子节点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶节点。最后将实力分到叶节点的类中。 特征选择特征选择在于选择对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，经验上扔掉这样的特征对决策树学习的精度影响不大，通常特征选择的准则是信息增益或信息增益比。 ——————————还未涉及决策树的剪枝部分……]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概览</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统》读书笔记——协同过滤推荐]]></title>
    <url>%2F2018%2F01%2F12%2Frecommendsys2%2F</url>
    <content type="text"><![CDATA[在应用协同过滤推荐技术时，除了用户评分数据，我们不需要额外的物品数据。 这样的好处是避免了付出很大的代价向系统提供详细而且实时更新的物品描述信息，但是这种仅基于纯粹的协同过滤的算法却无法根据物品的特性和用户的特殊偏好来客观选择推荐物品的。 本章中，将物品的特征描述为“内容”。因为后面章节部分介绍的大多技术的最初目的是推荐有意思的文本文档。此外，绝大多数的方法的假设是物品的特性能自动从文档内容本身或无结构的文本描述中提取。所以，基于内容的推荐系统的典型例子是比较候选文章的主要关键词和用户过去高度评价过的其他文章中出现的关键词来推荐新文章，而这些被推荐的物品，就常常指的是“文档”。 本章讨论的基于内容的推荐，关注的算法侧重于推荐文本描述的物品，并能自动“学习”用户记录（基于知识的推荐系统通常是显式询问用户的偏好）。 内容的表示与相似度描述物品目录最简单的方法就是维护每个物品的特征的详细列表（属性集、特征集或物品信息）。以推荐图书为例，可以把体裁、作者、出版社等信息保存在数据库中，根据不同的用户兴趣， 用这些特征来描述他们的偏好，而要做的就是将物品特征同用户偏好匹配起来。推荐系统需要收集用户的记录，直接的方法是明确询问用户ta所接受的范围和内容，间接的方式是让用户给一组物品进行评分，可以是整体评分，也可以是对不同维度进行评分。 基于内容的推荐系统的工作原理是，评估用户还没看到的物品与当前用户过去喜欢的物品的相似度。而衡量相似度的方法有两种：1.0/1衡量，就是将用户过去喜欢的物品，同其他ta没见过的物品n进行比较，如果n的类型在ta喜欢的类型中，则标为1，否则就为0；2.计算相关关键词的相似度或重叠度。典型的相似度系数会用到Dice系数，它比较适合多值特征组合。它是这样描述的：如果每本图书b由一组关键词k描述，那么Dice系数描述bi和bj为：2*（kbi∩kbj）/(|kbj|+|kbi|) 向量空间模型和TF-IDF严格来讲，图书的类别信息，并不算是图书的内容，而只能算是附加知识。基于内容的推荐方法不是去维护一组“元信息”特征，而是使用一列在文档中出现的相关关键词，它的主要思想是，能够从文档内容本身或没有限制的文字描述中自动生成列表。 文档内容可以用不同方法转换到这样的关键词列表中。首先，一种单纯的方法是将现在所有文档的词语设为一个列表，然后用布尔型向量描述每一个文档，1表示文档出现在文档中，0表示没有出现。如果用户记录用一个相似的列表描述，那么计算兴趣文档和文档的重合程度就可以匹配文档。 这个方法存在的问题很明显。首先是它假设每个词在文档中的重要性是相同的，此外，当文档比较长的时候，用户记录和文档的重叠几率自然就会大，所以，这样的推荐系统，往往更倾向于推荐长文档。 另一种方法是TF-IDF。它代表的是TF(term-frequence)-IDF(inverse documents frequence)，这种技术在信息检索领域应用广泛。 词频描述某个词在一篇文档中出现的频繁程度。为了阻止更长的文档得到更高的相关度的权值，必须对文档长度进行归一化处理，有几种方法是可行的。其中一种简单方法是将词出现的次数比上同文档中其他关键词出现的最多次数。假定：f是某词在文档中出现的频数，fj是其他关键词中最常出现的关键词的频数 TF=f/fj 反文档频率是组合了词频后的第二个衡量值，旨在降低所有文档中几乎都会出现的关键词的权重。其思想是，那些常见的词语对区分文档没有用，应该给那些仅出现在某些文档中的词更高的权值。N是所有可推荐的文档数量，ni是N中关键词i出现的数量。 IDF的计算方式：log (N/ni) 文档关键词的组合TF-IDF的权值可以计算为这两个子向量的乘积。 在TF-IDF模型中，文档不是表示为每个关键词的布尔值向量，而是算出的TF-IDF值向量。（这部分没弄懂，TF-IDF向量不是计算某一个次嘛？怎么来计算整篇文档的？所有关键词的TF-IDF再次相乘？） 其实，文档在处理之前，实际上还要去掉停用词，停用词的去除，对模型的效果可能更为明显。 TF-IDF的改进及局限TF-IDF向量一般很大并且很稀疏。可以使用其他技术来让它们更紧凑，并从向量中删除不相关的信息。 1.停用词和词干还原。一种直接的方法是删除所谓的停用词。在英语中，这类词一般是介词和冠词，这类词在文档中几乎都会出现，可以将他们从文档向量中删除。此外，另一种技术是词干还原或合并，目的是将相同的词语的不同变形替换成他们共同的词干（词根）。 这些技术进一步缩小了向量规模，与此同时，在用户记录中也用到词干来改进匹配过程。词干还原的实现方法包括：词形分析（后缀拆分法）和查字典法（wordnet）。尽管这些方法很强大，但是还是可能存在陷阱，如缩略语和双关语，会导致匹配不当的问题，词干还原法只用句法后缀拆分可能增加匹配不相关文档的风险。 2.精简规模。仅用n个信息量最大的词语来减少文档描述的规模，期望删除数据中的“噪声”。实验结果表明，如果挑选的关键词太少，一些重要文档特征就可能无法覆盖到；如果挑选的关键词太多，文档模型中用到关键词的重要性就有限，而且这些噪声反而使得推荐效果变差。原则上，特征选择技术也可以用于觉得信息量的关键词，然而这种基于学习的方法，不仅会增加模型复杂度，也有可能会倾向于过拟合描述训练数据的样本。也有人提出用外部词典来删除该领域不相关的词语，实验表明，这种方法能持续提高精度，尤其是只能得到很少的训练样本时。 3.短语。短语比单词更能描述文本，用它来替换词，可能进一步提高描述的准确性。局限性：从文本中抽取个别关键词并赋权的方法有另外的局限，没有考虑到关键词的上下文，在某些情况下没有正确体现描述的“含义”。一般来说，我们假设出现在一篇文档中的词语通常是适合刻画文档，而不是在文档中出现“反语境”。 基于内容的相似度检索协同过滤可以被成为“推荐相似用户喜欢的物品”，基于内容的推荐是称为“推荐与用户过去喜欢的物品相似的物品”。从这里来看，推荐系统的任务还是（基于用户记录）来预测用户是否喜欢自己没有见过的物品。本节会介绍依赖向量——空间文档表示模型的最常见技术。 最近邻（还没写完）想要估计用户对某篇文档感兴趣的程度，最初的方法是查询用户过去是否喜欢相似的文档。这需要两类信息：1.用户对以前商品的“喜欢/不喜欢”的评价记录，其次，需要一个标准来衡量文档的相似度。一般地，使用余弦相似度方法来评价文档是否相似。 相关性反馈——R0cchio方法它也是一种基于向量模型的方法，曾用于信息检索系统SMART，它的特定是，用户不能只提交给系统基于关键词的查询词，还要反馈检索结果是否相关。 其他文本分类方法将这类问题看作是分类问题，应用机器学习中的有监督学习技术，就会自动决定用户是否对某篇文档感兴趣，有监督学习意味着算法依赖现成的训练数据。文中还介绍了基于概率模型的方法、其他线性分类器和机器学习、显示决策模型和特征选择。 讨论对比评估局限性]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统》读书笔记——协同过滤推荐]]></title>
    <url>%2F2018%2F01%2F12%2Frecommendsys1%2F</url>
    <content type="text"><![CDATA[协同过滤推荐的主要思想：利用已有用户群过去的行为或意见预测当前用户最可能喜欢哪些东西或对哪些东西感兴趣。 此类的系统在在线零售系统中成为对用户需求个性化定制内容的工具。 纯粹的协同过滤方法的输入数据只有给定的用户—物品评分矩阵，输出数据一般有以下几种类型： 1. 表示当前用户对物品喜欢或者不喜欢程度的预测数值， 2. n项推荐物品的列表。当然，这个TOP-N列表不会包含用户已经买过的物品。 推荐方法： 基于用户的最近邻推荐这是一种早期的推荐系统，它的主要思想：首先，给定一个评分数据集和当前用户的id作为输入，找出与当前用户过去有相似偏好的其他用户，这些用户有时也被称为对等用户或最近邻；然后对当前用户没有见过的每个物品p，利用其近邻对p的评分计算预测值。这种方法的假设是： 1. 如果用户过去有相同的偏好，则他们未来也有相似的偏好 2. 用户偏好不会随着时间变化而变化。 书中以Alice购物为例，利用Pearson系数来评估用户的相似性（如何计算，我会在单独一篇博文中讲述）。 在实际应用中，由于评分数据集非常大，我们需要考虑计算的复杂度。 此外，评分矩阵通常十分稀疏，每个用户只对所有有效的物品的小部分进行评分。 最后，我们给新用户推荐什么，如何处理没有评分的新物品。这些问题都要深入讨论。 其他用于评估用户相似度的方法：改进余弦相似度，spearman相关系数和均方差。 然而实验分析显示，对于基于用户的推荐系统，pearson的相关系数比其他的方法更为优秀。不过，后来发现的基于物品的推荐技术，利用余弦相似度方法比pearson系数推荐效果更好。 事实上，基于近邻的预测方法在遇到当前用户只有非常少的共同的物品评分时会出错，导致不准的预测。 于是，有专家学者就提出了重要性赋权的方法——基于线性化简相似度权值的方法。尽管简单，但是在评分物品少于50个的时候，准确预测的效率提升非常显著。但是还有问题需要解决：赋权方案和启发式决定阈值在现实中是否有效；评分数据集更小时，我们不能期望找到很多有50个共同评分物品的用户。 能通过精细调整预测权重来提高准确度的方法是样本拓展。它指的是强调那些接近1和-1的值，对原始数值乘以一个常量p来调整近邻的权值。 选择近邻为了减小计算量并且保证计算预测值，我们只包括了那些与当前用户有正向关联的用户。降低近邻集合规模的方法通常是为用户相似度定义一个具体的最小阈值，或者将规模大小限制为一个固定值，而且只考虑k个近邻。 但是这两种方法有潜在的问题：如果阈值过高，那么规模就会小很多，意味着很多商品无法预测。相反，阈值过低，那么规模就不会显著降低。 k取多少，是个问题，MOVIE LENS上的测试显示，在大多数情况下，20到50个近邻似乎比较合理。 基于物品的最近邻推荐在电子商务网站，大多采用的是基于物品的推荐。 这种推荐，比较适合做线下的预处理。因此，即使评分数据矩阵非常大，也能做到实时计算的推荐。 基于物品的推荐算法的思想是：利用物品之间的相似度来计算预测值。 还是以Alice购物为例，书中介绍的是利用余弦相似度来进行物品的相似度的计算。（如何利用余弦相似度来计算，我也会另开一篇来进行介绍说明） 不管是基于物品还是基于用户，当数量达到一定程度的时候，计算的实时性是达不到的。需要对数据进行预处理。 为了保证性能与精度，通常采用离线预计算数据。 思想：实现构造一个物品相似度矩阵，描述物品两两之间的相似度。运行时，通过确定与物品p最相似的物品并计算用户u对这些邻近物品的加权总和来得到用户u对物品的评分。近邻数量受限于当前用户评过分的物品个数。 除开这种模型上的预处理之外，还可以仅利用评分矩阵中的某一部分数据来降低复杂度。一种基本技术是二次抽样，这种技术可以随机选取数据的子集，或者忽略那些仅有少量评分或非常热门物品的用户记录。 一般地，可以利用这些技术来加速计算，但是由于用到的信息少了，系统做出的精准预判的能力可能会下降。 基于协同过滤的评分方法显式收集用户的观点：通常是5分或者7分制。通过记录用户的打分，来收集用户的评分数据集。 隐式收集用户的观点：跟踪用户的浏览记录，收集用户的浏览日志，对用户的浏览日志进行分析，推荐可能效果更为显著。 数据稀疏与冷启动的问题实际应用中，可能获取的用户评分数据十分少，因此。评分矩阵就是一个稀疏矩阵。这种情况下的挑战就是利用较少的数据来获得精准的预测。直接做法，利用用户的附加信息（年龄，性别，教育程度兴趣等）来辅助分类用户，这是借助外界的信息。 解决数据稀疏与冷启动的问题的一种方法——基于图的方法（后面单独拿出来讲解）主要思想：利用假定用户品味的传递性，并且是不变的（书中没有提到），并由此增强额外的信息矩阵。 矩阵因子分解Netflix的竞赛表明高等矩阵因子分解方法对提高推荐系统的预测准确率有很大的帮助。简单来说，推荐系统使用矩阵因子分解方法来从评分模式抽取一组潜在的因子，并通过这些因子向量来描述用户和物品。在电影领域，这些自动识别的因子，很有可能对应一部电影的常见标签，也有可能是无法解释的。当当前用户和物品在这些因子相似时，就会推荐物品。 在信息检索领域，SVD被用于发现文档中的潜在因子；这种潜在语义分析技术也被成为潜在语义索引。 关联规则挖掘（后面单独拿出来详细介绍）关联规则挖掘是一种在大规模交易中识别类似规则关系的技术。 典型案例：啤酒与尿布。 就是通过一系列购物篮分析，分析出购物篮里面的可能同时购买的商品。 推荐系统，也可以根据这个来做一个类似商品的推荐。如捆绑销售。 在实际生活中，如果去一趟超市，可能发现商品并不是按照这种购物篮的方式捆绑，而是可能约有可能一起买的商品，反而相隔了一定的距离，这也是超市为了让人待在超市里更久，增加销量。 基于概率分析的推荐方法还有一种预测用户会对某个物品进行评分的方法是利用概率论已有的形式体系。 用概率方法实现协同过滤，最初简单的实现是将预测问题看作分类问题，通常可以描述为“将一个对象分配给几个事先定义好的类别”的任务。 如邮件的推荐（将其划分为正常邮件和垃圾邮件），里面用到的贝叶斯分类器，可以将经过训练的数据来拟合邮件分类，来判断一封邮件是否是正常邮件。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统》(蒋凡译)读书笔记——概览]]></title>
    <url>%2F2018%2F01%2F12%2Frecommendsys0%2F</url>
    <content type="text"><![CDATA[第一章 引言书中首先以在线书店为例，讲述了书店向特定用户推荐相关书籍的推荐系统，引申出推荐系统的类别——个性化推荐与热门推荐。在线书店是个性化推荐系统。每一个人由于其兴趣爱好不同，在系统中会看到不同的列表。当然也会有畅销书的推荐，而畅销书的推荐，显示的是当前用户群体，大众的关注热点，表示的是一种群体性推荐。 这本书重点讨论的是前面一种：也就是个性化的推荐系统。 提供个性化的推荐系统需要了解到用户的信息。它必须开发并维护一个用户模型或者用户记录来保存用户的偏好。以在线书店为例子，系统需要记住访客的浏览或者买过的书，从而预测他可能感兴趣的书。 尽管用户模型对推荐系统很重要，但是如何获取并利用这个信息，往往是取决于特定的推荐技术的。 用户的偏好可以通过监测用户行为隐式地获取，也可以通过询问访问者，显式地获取。 在获取用户的信息后，系统该如何利用，以及利用什么额外的信息。这些是推荐系统要解决的一部分问题。 目前已经在许多在线书店应用的方法：基于群体或协同的方法。 本书分为两部分：第一部分总结了几年来被广泛接受的推荐系统方面的研究成果，介绍了协同过滤、基于内容的过滤、基于知识的推荐和混合推荐方法的基本理论框架。此外，还包含了解释推荐物品的原因和评估推荐结果质量。 第二部分讨论了推荐系统的最近研究课题。如：如何因对外来攻击和操作推荐系统的做法、支持消费决策和可能的说服策略、社交和语义网背景下的推荐系统和所有领域应用推荐系统。第二部分可以看作是持续研究的参考。 本书的第一部分简介： 协同过滤的推荐这些推荐的思想是：如果用户在过去有相同的偏好，他们在未来也有相同的偏好。 基于协同过滤推荐下，一般需要回答几个问题： 我们如何发现与我们要推荐的用户有着相似的偏好的用户？ 如何衡量相似度？ 如何处理还没有购买经历的用户？ 如果只有很少的评分怎么办？ 除了利用相似的用户之外，还有哪些技术可以预测某个用户是否喜欢其他物品？ 其实，纯粹的协同过滤方法不会用到和要求任何和物品有关本身的知识的。 基于内容的推荐它的核心是：能够得到物品的描述，和这些特征的重要记录。 基于内容的推荐下，需要回答如下的问题： 系统如何获取并持续改进用户记录？ 如何决定哪个物品匹配或至少能接近、符合用户的兴趣？ 什么技术能自动抽取或学习物品的描述，减少人工标注？ 与不涉及内容的方法比较，基于内容的推荐有两大优点： 1.不需要大规模用户就可以达到适度的推荐精准度 2.一旦得到物品的属性就能立即推荐新物品。 基于知识的推荐如果我们把注意力投向其他领域，比如消费电子产品，就会涉及大量单次购买者。这意味着我们可能无法依赖购买记录，而这却是基于内容过滤和协同过滤的前提条件。即使如此，我们还是能够获取到更为精细化和结构化的内容，包括专业化的优质特征。 基于知识的推荐系统需要额外的因果关系知识生成推荐。在这种基于知识的方法中，推荐系统通常会用到有关当前用户和有效商品的额外信息。基于约束的推荐就是此类系统的例子（就类似与taobao，京东此类网站，需要用户设定条件，才能给你过滤，推荐产品） 基于知识的推荐系统要解决的问题如下： 什么机制可根据用户的特点来选择和排名物品？（为什么把某一个放在前面，把某一个放在后面） 哪种领域知识能表示成知识库？ 如何在没有购买记录的领域获取用户信息？ 如何处理用户直接给出的偏好信息？ 哪种交互方式能够用英语交互式推荐系统？ 设计对话时，要考虑哪些个性化因素才能保证获取用户的偏好信息？ 混合推荐由于问题的背景不同，每一种方法都有其优劣势。一种显而易见的方法就是组合不同技术产生更好或更精准的推荐。这种设计尤其适用于克服纯粹协同过滤方法的规模膨胀问题，并可以以来内容分析处理新物品或新用户。 在推荐系统中混合使用不同的方法时必须回答一下几个问题： 哪些方法能被组合，特定组合的前提是什么？ 两个或多个推荐算法是应该顺序计算，还是按照其他方法计算？ 不同的方法如何赋予权重，可以动态赋权吗？ 一般来说，推荐系统的目的有两个：一方面，推荐系统被用于激发用户去做某件事；另一方面，也可以看作是解决信息过载的工具。 因此，推荐系统也深深根植于信息检索和信息过滤的领域。 推荐系统的解释解释是未来让用户更容易理解推荐系统的推理脉络。 如何评估推荐系统？评估的方法有哪些？ 评估方法有：实验、半实验和非实验三种设计方法。需要回答的问题有： 哪些研究适用于评估推荐系统？ 如何利用历史数据实验评估推荐系统？ 用什么衡量来适应不同的评估目标？ 现有的评估技术的局限是什么？尤其是推荐系统的会话性或商业价值方面。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统》读书笔记——基于知识的推荐]]></title>
    <url>%2F2018%2F01%2F12%2Frecommendsys3%2F</url>
    <content type="text"><![CDATA[CF系统需要用户的评分数据作为知识源，向用户推荐商品，而不需要输入并维护其他的附加信息。 基于内容的推荐系统主要应用的知识源包括类别和体裁信息，还有从文档中提取的关键词。这两种方法的优势在于能以相对较小的代价获取并维护这些知识。 但是日常生活中，纯粹的CF系统会由于评分数据很少而效果不好，而且时间因素也在其中占据了影响力。而且，有些产品领域，用户希望能明确定义他们的需求，而这些明确化的需求的处理并不是CF和基于内容的推荐的系统所擅长的。 基于知识的推荐系统可以帮我们解决上面的问题。它不需要评分数据就可以进行推荐，也不存在冷启动的问题。推荐结果不依赖单个用户评分：是依赖用户需求同产品信息之间的相似度形式，或者是根据明确的推荐规则。 基于知识的推荐系统有两种个基本类型：基于约束的推荐和基于实例的推荐。这两种方法在过程上比较相似：用户必须指定需求，系统设计给出方案，如果找不到方案，用户必须更改需求。此外，系统还需给出解释。这两种个方法的不同之处在于如何使用所提供的知识：基于实例系统侧重于根据不同的相似度衡量方法检索出相似的物品，而基于约束的推荐系统则依赖明确定义的推荐规则的集合。 知识表示方法和推理一般来说，基于知识的系统依赖物品特性的详细知识。简单来说，推荐问题，就是从这个目录中挑选能匹配用户需求、偏好的物品。用户的需求可能要表示成物品的需求或阈值范围。 基于约束的推荐是由约束求解器解决约束满足问题或者通过数据库引擎执行并解决的合取查询。基于实例推荐主要是利用相似度衡量标准从目录中检索物品。 经典的约束问题可以用一组（V,D,C）描述，其中，V是一组变量； D是一组这些变量的有限域； C是一组约束条件 识别能匹配用户愿望和需求的一组产品的任务被称为推荐任务。通过在D中选择由V构成的C，寻找可能的结果集合。这就是推荐任务的目标。合取查询是将一组挑选标准按照合取方式链接起来的数据库查询。 实例和相似度基于实例的推荐方法利用相似度检索物品，相似度可以描述为物品属性与给定的用户需求之间的匹配程度。书中讲述了物品与需求之间的相似度计算方法以及局部相似度计算方法。 基于约束推荐系统的交互会话式的推荐系统交互过程： 1.用户指定自己最初的偏好 2.当收集了足够有关用户的需求和偏好的信息，会提供给用户一组匹配的产品，用户可以选择要求系统解释为什么会推荐某个产品。 3.用户可能会修改自己的需求。 尽管这种方法一开始是比较简单，但是实际应用中需要有一些更加精密的交互模式来支持推荐过程中的终端用户。如果目录中没有一个物品能满足用户的所有需求，系统需要能智能地帮助客户解决问题。 默认设置 处理不满意地需求和空结果集 提出为满足需求地修改建议 对基于物品/效用推荐的结果地排序 基于实例的推荐系统的交互早期的基于实例的推荐系统也是纯粹基于查询的方法。用户需要反复指定自己的需求，直至发现目标。这种反复修改十分乏味，而且需要专业的领域知识才能弄懂物品之间的关系属性。这种缺陷让人们提出了基于浏览的方法来检索物品，评价是一种有效的方法。 评价的基本思想是：用户以当前待审核物品未满足的目标来指明他们的修改要求。如：用户觉得价格太高，就会评价期望价格更低廉，如果需要质量更好，可能就要评价质量不够等。基于实例的最新进展是有效整合了基于查询和基于浏览的物品检索。一方面，评价有助于在物品集合内有效地引导用户；另一方面，基于相似度的实例检索有助于识别最相似的物品。基于评价的推荐系统允许用户很方便地表达自己的偏好，而不用强制地指明物品属性的具体值。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础数据结构]]></title>
    <url>%2F2018%2F01%2F12%2Fdatastr%2F</url>
    <content type="text"><![CDATA[链表链表是一种由节点（Node）组成的线性数据集合，每个节点通过指针指向下一个节点。它是一种由节点组成，并能用于表示序列的数据结构。 单链表：每个节点仅指向下一个节点，最后一个节点指向空（null）。 双链表：每个节点有两个指针p,n 。 p指向前一个节点，n指向下一个节点；最后一个节点指向空。 循环链表：每个节点指向下一个节点，最后一个节点指向第一个节点。 时间复杂度： 索引：O(n) 查找：O(n) 插入：O(1) 删除：O(1) 栈栈是一个元素集合，支持两个基本操作：push用于将元素压入栈，pop用于删除栈顶元素。 它是一种后进先出的数据结构（Last In First Out, LIFO）。 时间复杂度 索引：O(n) 查找：O(n) 插入：O(1) 删除：O(1) 队列队列是一个元素集合，支持两种基本操作：enqueue 用于添加一个元素到队列，dequeue 用于删除队列中的一个元素。 是一种先进先出的数据结构（First In First Out, FIFO）。 时间复杂度 索引：O(n) 查找：O(n) 插入：O(1) 删除：O(1) 树树是无向、联通的无环图。 二叉树二叉树是一个树形数据结构，每个节点最多可以有两个子节点，称为左子节点和右子节点。 满二叉树（Full Tree）：二叉树中的每个节点有 0 或者 2 个子节点。 完美二叉树（Perfect Binary）：二叉树中的每个节点有两个子节点，并且所有的叶子节点的深度是一样的。 完全二叉树：二叉树中除最后一层外其他各层的节点数均达到最大值，最后一层的节点都连续集中在最左边。 二叉查找树：二叉查找树（BST）是一种二叉树。其任何节点的值都大于等于左子树中的值，小于等于右子树中的值。 时间复杂度 索引：O(log(n)) 查找：O(log(n)) 插入：O(log(n)) 删除：O(log(n)) 字典树字典树，又称为基数树或前缀树，是一种用于存储键值为字符串的动态集合或关联数组的查找树。树中的节点并不直接存储关联键值，而是该节点在树中的位置决定了其关联键值。一个节点的所有子节点都有相同的前缀，根节点则是空字符串。 树状数组树状数组，又称为二进制索引树（Binary Indexed Tree，BIT），其概念上是树，但以数组实现。数组中的下标代表树中的节点，每个节点的父节点或子节点的下标可以通过位运算获得。数组中的每个元素都包含了预计算的区间值之和，在整个树更新的过程中，这些计算的值也同样会被更新。 时间复杂度 区间求和：O(log(n)) 更新：O(log(n)) 线段树线段树是用于存储区间和线段的树形数据结构。它允许查找一个节点在若干条线段中出现的次数。 时间复杂度 区间查找：O(log(n)) 更新：O(log(n)) 堆堆是一种基于树的满足某些特性的数据结构：整个堆中的所有父子节点的键值都满足相同的排序条件。堆分为最大堆和最小堆。在最大堆中，父节点的键值永远大于等于所有子节点的键值，根节点的键值是最大的。最小堆中，父节点的键值永远小于等于所有子节点的键值，根节点的键值是最小的。 时间复杂度 索引：O(log(n)) 查找：O(log(n)) 插入：O(log(n)) 删除：O(log(n)) 删除最大值/最小值：O(1) hash table哈希用于将任意长度的数据映射到固定长度的数据。哈希函数的返回值被称为哈希值、哈希码或者哈希。如果不同的主键得到相同的哈希值，则发生了冲突。 Hash Map：hash map 是一个存储键值间关系的数据结构。HashMap 通过哈希函数将键转化为桶或者槽中的下标，从而便于指定值的查找。 冲突解决链地址法（Separate Chaining）：在链地址法中，每个桶（bucket）是相互独立的，每一个索引对应一个元素列表。处理HashMap 的时间就是查找桶的时间（常量）与遍历列表元素的时间之和。 开放地址法（Open Addressing）：在开放地址方法中，当插入新值时，会判断该值对应的哈希桶是否存在，如果存在则根据某种算法依次选择下一个可能的位置，直到找到一个未被占用的地址。开放地址即某个元素的位置并不永远由其哈希值决定。 ###图图是G =（V，E）的有序对，其包括顶点或节点的集合 V 以及边或弧的集合E，其中E包括了两个来自V的元素（即边与两个顶点相关联 ，并且该关联为这两个顶点的无序对）。 无向图：图的邻接矩阵是对称的，因此如果存在节点 u 到节点 v 的边，那节点 v 到节点 u 的边也一定存在。 有向图：图的邻接矩阵不是对称的。因此如果存在节点 u 到节点 v 的边并不意味着一定存在节点 v 到节点 u 的边。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础算法--小白专场的时间复杂度与性能比较]]></title>
    <url>%2F2018%2F01%2F12%2Falgofbase%2F</url>
    <content type="text"><![CDATA[排序快速排序（quick sort）稳定：否 时间复杂度 最优：O(nlog(n)) 最差：O(n^2) 平均：O(nlog(n)) 合并排序（merge sort ）合并排序是一种分治算法。这个算法不断地将一个数组分为两部分，分别对左子数组和右子数组排序，然后将两个数组合并为新的有序数组。 稳定：是 时间复杂度： 最优：O(nlog(n)) 最差：O(nlog(n)) 平均：O(nlog(n)) 桶排序（bucket sort ）桶排序是一种将元素分到一定数量的桶中的排序算法。每个桶内部采用其他算法排序，或递归调用桶排序。 时间复杂度 最优：Ω(n + k) 最差: O(n^2) 平均：Θ(n + k) 基数排序（base sort ）基数排序类似于桶排序，将元素分发到一定数目的桶中。不同的是，基数排序在分割元素之后没有让每个桶单独进行排序，而是直接做了合并操作。 时间复杂度 最优：Ω(nk) 最差: O(nk) 平均：Θ(nk) 拓扑排序拓扑排序是有向图节点的线性排序。对于任何一条节点 u 到节点 v 的边，u 的下标先于 v。 时间复杂度：O(|V| + |E|) 图算法深度优先搜索深度优先搜索是一种先遍历子节点而不回溯的图遍历算法。 时间复杂度：O(|V| + |E|) 广度优先搜索广度优先搜索是一种先遍历邻居节点而不是子节点的图遍历算法。 时间复杂度：O(|V| + |E|) Dijkstra算法Dijkstra 算法是一种在有向图中查找单源最短路径的算法。 时间复杂度：O(|V|^2) Bellman-Ford算法Bellman-Ford 是一种在带权图中查找单一源点到其他节点最短路径的算法。虽然时间复杂度大于 Dijkstra 算法，但它可以处理包含了负值边的图。 时间复杂度： 最优：O(|E|) 最差：O(|V||E|) Floyd-Warshall 算法Floyd-Warshall 算法是一种在无环带权图中寻找任意节点间最短路径的算法。该算法执行一次即可找到所有节点间的最短路径（路径权重和）。 时间复杂度： 最优：O(|V|^3) 最差：O(|V|^3) 平均：O(|V|^3) 最小生成树算法最小生成树算法是一种在无向带权图中查找最小生成树的贪心算法。换言之，最小生成树算法能在一个图中找到连接所有节点的边的最小子集。 时间复杂度：O(|V|^2) Kruskal 算法Kruskal 算法也是一个计算最小生成树的贪心算法，但在 Kruskal 算法中，图不一定是连通的。 时间复杂度：O(|E|log|V|) 贪心算法贪心算法总是做出在当前看来最优的选择，并希望最后整体也是最优的。使用贪心算法可以解决的问题必须具有如下两种特性： 最优子结构 问题的最优解包含其子问题的最优解。 贪心选择 每一步的贪心选择可以得到问题的整体最优解。 实例-硬币选择问题给定期望的硬币总和为 V 分，以及 n 种硬币，即类型是 i 的硬币共有 coinValue[i] 分，i的范围是 [0…n – 1]。假设每种类型的硬币都有无限个，求解为使和为 V 分最少需要多少硬币？硬币：便士（1美分），镍（5美分），一角（10美分），四分之一（25美分）。假设总和 V 为41,。我们可以使用贪心算法查找小于或者等于 V 的面值最大的硬币，然后从 V 中减掉该硬币的值，如此重复进行。V = 41 | 使用了0个硬币V = 16 | 使用了1个硬币(41 – 25 = 16)V = 6 | 使用了2个硬币(16 – 10 = 6)V = 1 | 使用了3个硬币(6 – 5 = 1)V = 0 | 使用了4个硬币(1 – 1 = 0) 位运算 位运算即在比特级别进行操作的技术。使用位运算技术可以带来更快的运行速度与更小的内存使用。测试第 k 位：s &amp; (1 &lt;&lt; k);设置第k位：s |= (1 &lt;&lt; k);关闭第k位：s &amp;= ~(1 &lt;&lt; k);切换第k位：s ^= (1 &lt;&lt; k);乘以2n：s &lt;&lt; n;除以2n：s &gt;&gt; n;交集：s &amp; t;并集：s | t;减法：s &amp; ~t;提取最小非0位：s &amp; (-s);提取最小0位：~s &amp; (s + 1);交换值：x ^= y; y ^= x; x ^= y; 运行时分析 大 O 表示大 O 表示用于表示某个算法的上界，用于描述最坏的情况。 小 O 表示小 O 表示用于描述某个算法的渐进上界，二者逐渐趋近。 大 Ω 表示大 Ω 表示用于描述某个算法的渐进下界。 小 ω 表示小 ω 表示用于描述某个算法的渐进下界，二者逐渐趋近。 Theta Θ 表示Theta Θ 表示用于描述某个算法的确界，包括最小上界和最大下界。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web挖掘概述]]></title>
    <url>%2F2018%2F01%2F11%2Fwebmining%2F</url>
    <content type="text"><![CDATA[自Web2.0时代开始，网站提供的功能不在仅仅是页面信息的展示了。更多的是与用户的交互，而在与用户的交互的过程中，又会产生很多的数据。 因此，一些公司就针对Web的数据进行了数据挖掘和分析。 针对Web2.0时代的网站数据分析，主要有：内容挖掘，结构挖掘和日志挖掘三种挖掘方向。 其中，内容挖掘，包括文本挖掘和多媒体挖掘；Web结构挖掘，包括URL挖掘和内外结构挖掘；Web日志挖掘包括一般访问模式追踪和个性访问模式追踪。 Web挖掘的基本步骤主要包括： 数据预处理，包括数据的清洗、用户识别、会话识别和事务识别等过程，对原始Web日志文件中的数据进行提取、分解和合并，转化成合适Web挖掘的的数据格式。 模式识别，模式识别对预处理后的数据进行分析，从中挖掘出潜在的模式。 模式分析，主要任务是从预处理的数据集中过滤掉用户不感兴趣的模式，发现有价值的指示。 可视化技术呈现Web挖掘的结果。 Web内容挖掘主要包括文本挖掘和多媒体挖掘两类，其挖掘对象包括文本、图像、音频、视频和其他各类型的数据。 这里的内容既包括网页，也包括搜索引擎的结果。对非结构化的文本进行Web挖掘，称为文本挖掘，是Web挖掘中比较重要的领域。 Web多媒体数据挖掘可以从多媒体数据中提取隐藏的知识、多媒体数据关联或者说其他没有直接存错在多媒体数据库中的模式，Web多媒体挖掘首先进行多媒体文件的特征选取，然后再用传统的数据挖掘方法进行下一步分析。 Web文本挖掘的方法主要包括文本概括、文本分类和文本聚类等。 文本概括是指从文本集中抽取关键信息，用简洁的形式总结文本集的主题内容。 文本分类是把一些被标记的文本作为训练集，找到文本属性和文本类别之间的关系模式，然后利用这种关系模型来判断新文本的类别。情感分析是文本分类的热点应用领域， 文本聚类是指根据文本的不同特征划分为不同的类，目的是使属于同一类的文本之间的差别尽可能小，而不同类别的文本之间的差异尽可能大。 文本聚类和分类之间的区别是分类学习的样本有类别标记，二聚类的样本没有确定的类别。此外，Web挖掘还包括从大量文档中发现一对词语出现模式的关联分析以及特定数据在未来的情况预测。 关于聚类分析的使用，首先是需要进行分词和数据清洗，去掉停用词而后再进行关联分析。 简要介绍典型的文档聚类的过程：首先利用网络爬虫搜索相关网站，下载数据并将其转换成文本数据，然后搜索其中的关键词，选择一定数量的关键词组成文档向量，利用聚类算法对这些文档进行聚类，对每一类文档也可以抽取其中的高频词汇。当用关键词查询的时候，就能够查询出相关主题的一组文档了。 TF-IDF算法（term frequence -inverse document frequence）TF-IDF算法是文档特征项（关键词）权重计算的一种重要方法，用于计算每个词对文档的描述能力，其基本思想是某个词在文档中出现的频率越高，且在其他文档中很少出现，则该词对该文档的描述能力越强。该算法多用于信息检索、文本挖掘、文本分类等领域。 理解反转：区分是表明差异性的，如果用（ 某词在某文档中出现的次数/总文档数 ）这一指标来衡量，只能发现，该数据越大，说明不能显示该词的特殊性和概括性，因此用起倒数，倒数的值越大，说明该词在所有文档中出现的次数不多，而且集中在该文档中，就更能显示出该词的特殊性了。TF 计算词频tf=某词的频数/文档中出现次数最多的词的词频 IDFidf=log(总文档数/包含某词的文档数)]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>数据分析与挖掘</tag>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘十大算法]]></title>
    <url>%2F2018%2F01%2F11%2Ftop10datamining%2F</url>
    <content type="text"><![CDATA[2006年，在香港举办的IEEE数据挖掘国际会议上，与会专家遴选出十个最具影响力的数据挖掘算法。包括C4.5，K-means，SVM，Apriori，EM，PageRank，AdaBoost，KNN，Navie Bayes和CART算法。 其中，C4.5，SVM， KNN， Navie Bayes，CART 算法是分类算法K-means是聚类分析算法Apriori 是关联分析算法EM 极大似然算法Pagerank Web链接分析法AdaBoost 泛化]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>数据分析与挖掘</tag>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网]]></title>
    <url>%2F2018%2F01%2F11%2Fsearchonscience%2F</url>
    <content type="text"><![CDATA[经过实际使用的体验，国内的搜索引擎–百度和搜狗的搜索体验不佳。因此，尝试使用国外的google搜索，经过试用，体验效果比较良好。 以下内容参考自：http://coderschool.cn/1853.html科学上网-轻松访问Google(持续更新-解决Google打不开)—————————————————————————————————————————————————— 搜索https://gufen.ga/ （无广告，原guso.ml,ggso.ga,guge.ga）https://www.aiguso.ga/ （无广告，体验良好）https://gg.cellmean.com （无广告，体验良好）https://google.gg-g.org/ （无广告，体验良好）https://cc.qqmmgj.com （无广告，体验良好）https://www.xichuan.pub/ （无广告，体验良好）https://freedo.ga/ （流量快耗尽，随时可能关掉）https://b.ggkai.men/ （有广告，体验一般）http://g.suconghou.cn （无广告，体验良好）http://google.suanfazu.com/ （整合搜索，非原版）https://kuaiguge.info （无广告，体验良好）https://g.zmirrordemo.com/ （第一次访问需验证,部分地区被qiang）https://google.speeder.cf/ （第一次访问需验证）https://www.inrealm.xyz （无广告，体验良好） 谷歌学术https://xues.glgoo.com/https://b.ggkai.men/scholarhttp://g.suconghou.cn/scholarhttp://a.a.88dr.com/http://www.tmeishi.com/xs/https://www.scholar.live/https://www.xichuan.pub/scholarhttps://a.ggkai.men/…/scholar.google.com/https://google.speeder.cf/…scholar.google.com/https://xue.glgoo.com 变种网页：http://www.tmeishi.com/ （无广告，搜索体验良好）http://www.ibying.com/ （无广告，搜索体验良好）http://search.twcc.com/ （无广告，体验良好）https://www.tlss.space/ （无广告，体验良好）http://googlebridge.com/ （无广告，体验良好）http://www.yegoogle.com/ （无广告，体验良好）https://www.sov5.com/ （无广告，体验良好）http://www.gycc.com/ （无广告，体验良好）http://www.gfsoso.me/ （有广告，体验一般）http://www.17soso.cn/ （无广告，搜索体验良好）http://www.aifago.com/ （无广告，搜索体验良好）https://www.dcrss.net （无广告，搜索体验良好）https://www.dcrss.com/ （google学术和网页）http://www.daysou.com/ （有广告，体验一般）http://www.lstsrw.org/ （有广告，体验一般） 该网站也有大神总结的优秀资源汇总：http://gufenso.coderschool.cn/ 以下的搜索引擎也可以使用：https://www.googto.org 构图搜索https://www.googto.info 构图搜索]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>搜索</tag>
        <tag>技巧与工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜书网站概览]]></title>
    <url>%2F2018%2F01%2F11%2Fsearchbook%2F</url>
    <content type="text"><![CDATA[按照使用方式和突破限制的方式分类如下： 不需要登录注册http://manybooks.net/ 英文 http://www.allitebooks.com/ IT类型英文书籍，可以下载，可以在线阅读 http://libgen.io/ 可以使用，不用注册 https://www.jiumodiary.com/ 鸠摩搜书，可以使用，不用注册 http://www.inien.com/ 可以使用，提供在线阅读，可以提供搜索、制作下载，不用注册 http://xidong.net/ 可以使用 http://www.gutenberg.org/browse/languages/zh 古登堡计划，可以使用 http://haodoo.net/ 繁体书，可以使用 https://1bookcase.com/ 英文书下载网站 http://www.verycd.com/archives/ 资源较多，较杂的网站 可以使用需要登录注册http://ebook3000.com/ 电子书，貌似有问题的网站，广告比较多，下载需要注册 http://www.hi-pda.com/forum/viewthread.php?tid=569396&amp;extra=page%3D1 资源多，但要登录的网站 http://www.wmwk.org/ 需要注册 http://www.cnepub.com 需要注册，可以搜索，可以阅读，但是无法保证所有搜到的书都可以阅读 http://www.douban.com/group/12509/ 豆瓣登录访问 http://www.douban.com/group/ebex/ 豆瓣登录访问 http://www.douban.com/group/txtbook/ 豆瓣登录访问 http://www.douban.com/group/pdf/ 豆瓣登录访问 http://www.douban.com/group/appleibooks/ 加入豆瓣小组 http://www.epub5.com/ 需要注册,资源不是很多 http://www.11dream.net/ 需要注册，资源不是很多 需要搭梯子使用的网站http://forfrigg.com/ 可能需要搭梯子，类似于鸠摩搜书，复合搜索 http://www.google.com/cse/home?cx=005194810162718251859%3Ao8kqt8bibjo 豆瓣电子书搜索，需搭梯子 http://wpsoso.com/ 网盘soso，貌似需要搭梯子 http://www.lepdf.com/ 搜索，可能需要搭梯子，类似于http://forfrigg.com/ 彩蛋：一些阅读材料kindle 使用技巧及推荐多看阅读 kindle 使用技巧 kindle 使用技巧 多看阅读，小米旗下的电子书网站]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>搜索</tag>
        <tag>技巧与工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》精彩观点提炼]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin4.2%2F</url>
    <content type="text"><![CDATA[用平衡观点看待过去、现在和未来，用郑重的态度过好当下的生活，并连接过去和未来。 明确工作隔阂生活的界限，用未来的视角工作，用享乐主义视角生活。 使用时间之尺，审视时间的长期价值，尽可能删减非必要事件。 让远期未来更具体，为近期未来增加挑战。 把握做事的节奏，区分“求快”的事件和“求慢”事件 提升时间使用的“深度”，减少被动式的休闲比例，至少保持一项以上的业余爱好。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》4.1]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin4.1%2F</url>
    <content type="text"><![CDATA[1.主动构建知识 提问在我们的学习的过程中是至关重要的。如果我们在平时学习的时候不提问，学习的流程应该是这样的： 订立计划 实施学习 回顾和整理，对笔记进行整理，或者画出一张思维导图，将知识结构进行整理。 其实，很多人在做到上面的三点就很了不起了。我们在学习这些知识的过程中，只是扮演着搬运工的角色，只是将书本上面的知识放进脑子了，而并没有用上自己的知识结构和知识构建体系。 2.打开新旧知识之间的通道 罗素在他的《人类的知识》中写道，一个人求知的历程，就像是一个登山者靠近一座被无爱笼罩的高山，刚开始只能看见轮廓，走近的时候，才看清整座山，才感觉到山的高大。 问题就像是向导，引领着我们去接近。我们应该去多想项四个问题： 针对当前的学习资料，我已具备了哪些知识？ 这个问题是对我知道我知道这一问题的回答，自己对自己的知识掌握情况应该是最熟知的； 针对当前的学习材料，我又学到了哪些新的知识？ 这些知识对原有的知识构成了何种补充或挑战？ 这个问题是对自己知道自己不知道的问题的回答； 针对当前的学习材料，还有哪些未知的东西，我可以通过简单的探索就可以获知？ 针对当前的学习材料，还有哪些未知的东西，无法轻易地获得解答，同时值得自己长期去探索的？ 第四个问题是最为特别的，因为它可能变成一项长期任务，值得每一个学习者去深入研究。 这两天看到这样的观点:学习要把握四点： 将基础知识掌握牢固，越牢固越好，最好的效果就类似于卖油翁那样的熟练程度； 大胆假设，小心求证，当自己有何看法或想法的时候，积极做出假设，但是在验证的时候保持严谨的态度，对验证的要求越高越好； 在一二点做好之后，可能在前进的过程中，会遇到自己不能解决的问题，这个时候，就应该主动去和人交流，与人探讨，切忌闭门造车； 博览群书，知识面需要进行扩展，在知识体系交叉的时候，许多新的知识会对自己的研究产生新的思路 也向大家推荐另一本书，《学会提问，批判性思维指南》]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》3.4]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin3.4%2F</url>
    <content type="text"><![CDATA[作者在10年前在一家互联网公司做实习生时，一位导师教了两样东西让他印象最深刻：每住到一家宾馆，首先看门背后的楼层结构图，记住安全出口和逃生通道的位置；每做完一件事情，就要做一下回顾总结。 在从小到大的学习中，我们被告知的所有知识来源于课本，前人已经帮我们探索了大多数的知识，我们只需要认真读书，理解吸收就可以了。也因此，许多人养成了僵化的思维模式，我们得出了这样的结论：实践中遇到的知识可以从理论中推导出来，从理论到时间是一个自然而然的过程。但实际上并不是这样的。 从理论出发不一定能指导实践，只有在实践中通过反思积累的知识才能指导实践。“行动科学”对此有比较深入的阐述：科学理论诞生于“维持其他变量恒定”的理想情景，而实际的问题却则处于一个多因素相互作用、相互依存而又相互冲突的“复杂场域”中，并且具有某种独特性。 事实上，我们在解决问题的时候，更依赖于隐性的知识和理性的推理。行动科学的另一位大师认为：三思而行不一定正确，很多时候甚至是行动先于思考的，因为人们的机智行为是高度技巧及复杂推理形成的，而其中的绝大多数又是隐形的。 故，行动之后的反思，可能反过来发展我们的认知。以个人在实验室配置linux为例，自己以前没有配过，然后此次为了实验的需要，直接硬着头皮上了，自己以前只装过windows的系统，而这两款系统是不一样的，这次就先直接上了，边查资料边配软件，照着资料上面的教程来配。在查资料配软件的过程中，也学会了一些linux的命令，而现在在配置好的系统中的添加文件或修改文件，再也不是一头雾水了，起码看得懂一些代码的意义，明白了语句的作用。 日本管理大师野中郁次郎认为：我们行动中蕴含的知识属于不容易用语言表达出来的隐形知识，往往是经验，是技巧。这就是工匠精神所蕴含的精髓所在。最近也在想工匠精神的意义，第一次接触到工匠精神是在大一时候的，那时的互联网时代这部纪录片中提到了德国的工匠艺术，自己其实一直很钦佩德国人的那种严谨精神的，自己以为的工匠精神应该就是属于那种师傅带几个徒弟，进行精细化指导，手把手教导，自古以来的中国的工匠精神一直就有传承，记得爷爷曾经和我说父亲以前拜师学习木匠的故事，那时出师还要好几年，第一年怎么样，第二年怎么样……当时不太懂，现在有些懂了。正是做一些重复式的内容，才能到后面将事情做好，做好做快。 我自己在平时进行反思的时候，总结还是做了，然后，今天再一看，感觉深度不够，只是停留在表面，不够细致。 作者是这样建议的： 我们需要透过表象看到本质，需要付出更多的思考和观察。在表象之上，是经由归纳抽象出来的经验、假设和模型；在表象之下，是大量更加真实与繁密的细节信息。 我们在做完一件事，应该怎样反思呢？ 信息： 在做这件事利用了哪些信息？哪些信息是最关键的？ 这些信息是从哪些渠道得来的？哪些渠道被证明是很有价值的？ 我可能遗漏了哪些信息？这些信息可以怎么得到？ 预期： 在做事之前，是否对事情的过程与结果形成了正确的预期？ 我为什么会形成这样的预期？是什么造成了预期和事实的偏差？ 我的预期是否促进或阻碍了事情的进展？今后应该如何管理自己的预期？ 结果： 怎样描述这件事的结果？怎样评价它？ 在描述和评价这件事的结果时，自己都用了哪些指标？这些指标是否需要改进？ 结果可以改进吗？需要哪些方面的改进？如何改进？ 进度： 事情的进度合适吗？快了还是慢了？是什么因素造成的？ 当进度出现了问题时使用了哪些手段进行干预？效果怎样？为何效果理想/不理想？ 工具： 在完成这件事的过程中，我使用了哪些工具？ 哪些工具起到了重要的促进作用？哪些工具起到了阻碍的作用？ 如何改进现有的工具使其发挥更好的功效？ 情绪： 在做事的过程中，自己的情绪是怎样的？是否出现了情绪失控的情况？是何原因引发的？ 我是否有意识地调整了自己的情绪？在这期间使用了什么方法？是否需要改进？ 阻碍： 在做事的过程中我遇到了哪些阻碍？其中最大的阻碍是什么？ 我是如何应对这些阻碍的？取得了哪些效果？ 这些阻碍中哪些会长期存在？我需要通过什么持续的努力来减少这些阻碍？ 优势： 在做事的过程中，我发挥了哪些优势？是否有优势没有用上？ 在做事的过程中，我的主要收获有哪些？我的哪些知识和能力得到了提升？ 我可以向做同类事情的其他人学习些什么？他们有哪些优势是自己不具有的？ 缺憾： 在做事的过程中，我的遗憾有哪些？最大的遗憾是什么？是什么原因造成了这个遗憾？ 在做事的过程中，我暴露了哪些缺点？其中哪些缺点是必须且迫切需要改进的？ 关于这件事，别人对我有什么评价和批评？他们的批评有哪些可取之处？ 意义： 这件事对我来说最大的意义是什么？对我的短期生活和长期生活分别有什么影响？ 这件事对身边的人、社会、对世界和对地球的意义是什么？ 我发现了哪些意想不到的意义？]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》3.3]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin3.3%2F</url>
    <content type="text"><![CDATA[很多人认为，同时做多件事情是很有效率的，其实，从认知心理学的观点来看，人的“多线程工作”是不可能的。 因为在任何瞬间，人只能有一个“逐一焦点”，这个注意焦点牵引了人的认知加工资源。有时候我们认为同时关注了两个东西，其实是发生了注意力转移的结果。当然也有例外：就是我们有些熟练的技能，可以自动化、不加注意的进行，那就可以和其他事情一起做。 “核心思考区间”的工作不可中断科学家发现“任务转换”会降低工作效率，主要原因有两个：1.任务留下的惯性思维，这个惯性对切换后的任务有影响；2.需要对转换后的任务进行知识重构，需要熟悉新任务的完成方式。如果我们在非常投入和忘我的思考时打断，我们的损失和懊恼就会非常巨大；相反，如果我们只是在做一些不需要投入大量精力的事情的时候，即使是频繁地中断，对我们的影响也不会太大。 如果，我们能选择合适的中断点，我们的切换损失就会降低。对于提高工作效率而言，着可以研究一下。但是，个人觉得在做一件事情的时候，只需要做好手中的事情就好，不要想着同时做多件事情。 作者给我们的参考意见是：在接受一个新任务之后，第一部，应该是找到其中的核心思考区间，什么是最重要的，自己希望在处理事情的时候，以什么样的方式处理，这是重要的一步。找出了这个区间之后，先聚精会神的完成对该任务的研究和准备，再对外围的内容进行扩充和谋划。 对不同认知类型的工作分层处理我们应该如何按照认知类型来划分任务呢？ 认知类型是指我们头脑加工信息的不同方式。当我们在不同认知类型里面切换任务的时候，我们在调整的过程中需要花费的精力很大。那为什么不先按照一个类型进行分类，我们完成该类型下的所有任务，我们将需要切换其他方式的地方进行标注，待该类型下的任务完成之后，再切换认知模型，如果这样下来，我们的任务是不是感觉会要轻松一些呢？ 自己深有体会：前一段时间，老师让我们先写一篇项目申报书，我们是先将文字写了，然后再将图放上去，但是老师在给我门指导的时候，是这样指导的：这个地方需要补充什么图，描绘的是什么内容，然后继续对我们的文档进行分析，再指出另一个地方需要加上什么图片，描述的是什么内容，待到全部文档修改完，再来具体绘画图片。这应该也是老师比我们的效率高的原因之一吧。 集中处理同质性的工作按照上面分层的方案，对每一层任务进行分析，然后对每一层任务进行各个击破： 以做PPT为例：先确定主题，这套PPT的主题是什么，自己想要传递什么信息，信息的层次是什么，每一层的信息是什么，信息的分层表达形式是什么，如果以文字、excel和图片形式传递，那可以先确定文字的内容，excel内容，图片内容，再来分别完成文字内容、图片的填充。就这样，梳理好之后，就可以进行PPT的制作了。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》3.2]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin3.2%2F</url>
    <content type="text"><![CDATA[目前互联网创业最流行的方法之一是“精益创业”，而其中有个关键的概念“最小化可行产品”。 它体现的是“构建-测量-学习”的循环式的过程。类似在平时我们希望上交一份比较优秀的答卷的时候，我们总是先建立一个最基本的版本，而后再根据市场需求来不断地修改，进行修正。 精益创业的价值在于，在动态演化的市场里，找到一种可操作的适应市场的方式。而在传统的产品生产流程过程中，我们都是在进行周密、系统的规划后，一步一步完成项目的各个部分，最后再构建出精致的成品。然而即使在市场进行了充分的调研后，再到论证、规划、测试，等到产品的发布，所形成的产品也不一定能适应新的市场。 因此，及时推出适应市场产品的关键性就体现出来了。当然，如果市场的演变不大，那也不必去使用精益创业的模式。 一个人的“最小化可行产品”是什么？人生类似于一个不断演化的创业，我们面临着不可知的未来，哪怕我们已经做好了计划和准备，但是还是可能无法适应变化的世界。那为什么不借鉴精益创业的方法呢？推出属于自己的一个最小化的可行性产品呢？ 产品：是一种结果，是对原料的结构性整合，产品能被别人使用和检验，能对世界产生影响，使人受益，同时，它应该也是一种媒介。如果我们按照这种框架来理解，就可以去分析，对于我们每一个人来说，什么才是自己向外宣传的能力或专业化产品了。 我们永远无法准备好“先做好准备再上场”的思想有一个问题：我们无法做好万全准备，总有我们无暇顾及的问题。当我们先做出一定的东西的时候，我们的思路才可能更清晰。 把评判当做一种信息来对待 我们可以将外界的反馈分为三类：正面肯定、否定和批评、提示之前未知的方面。我们总是习惯反对别人的意见。 一方面，我们怯于将自己的产品发布给外界，怯于将自己的产品分享给别人，这是一种自卑的表现；一方面，当自己的观点、产品暴露在大众的目光下，我们虚心接受评判和提示之后，我们的产品才能进步的更快。 对产品不断修正精益创业的最后的关键是，在勇于修正自己的错误。这一步说起来容易也困难：容易是因为我们在前期已经将需要改进的信息获取了，只需要按照我们收集的信息来进行针对性修改；困难是因为我们有没有勇气去走出那一步，这最关键的一步，因为只有自己采取措施，自己的产品才会有所改变。 如果我们希望以精益创业的方式去走过人生，需要做到这三点： 1. 克服”过度准备“的惯性，向前一步，把未完成的事情完成； 2. 克服”自我防卫“的心态，乐于接受反面意见并加以慎重地审核； 3. 克服”沉没成本“的固执，有勇气否定并重新构造自己的产品；]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》3.1]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin3.1%2F</url>
    <content type="text"><![CDATA[一件看上去繁难的事，只要开始做了，就会变得越来越容易。 我们内心的懒惰、恐惧、侥幸附带各种各样的借口，在本该有的行动之前树起了重重无形的阻碍。这些心理的障碍往往超过了这些行动本身的难度。 纵使一件事情真的很难，你就迈出第一步试试，有何不可？ 这个自己深有体会:自己平时在寝室不想收拾，后面每次到检查卫生的时候收拾一次，但是最近感觉这个习惯不太好，于是，就准备每天好好收拾一下。其实发现也是蛮简单，只要每天花个三到五分钟，自己的视觉感受也就感觉好很多，心情愉悦不少，总比以前那种只会在检查卫生的时候打扫一次感觉好很多。 现在自己在想一些事情或看法的时候，如果有什么头绪，自己总会在第一时间去尝试着去做，哪怕只是开个头，开了个头，自己就会时刻惦记着，惦记着，自己也就会在日后的闲暇时间里去做。 如果我们总能想到一件事就去完成一件事，我们同时应付的事情就少了，也就不用多花时间去“管理”这些事情。正像第一章中的，事情总会越管理越多。当我们待完成的事情一多，我们的管理复杂度会直线上升，相应地，记忆负担、情绪负担和人际负担会显著加重。 我们必须要把先做的小事处理掉，是我们保持积极和从容心态的一剂良方。虽说我们在一些参考资料中看到，说当我们先解决大问题，那么后面的情形就会好很多。但是对我而言，个人觉得还是在先解决小问题，再来解决大问题，这种方式更好。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》2.4]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin2.4%2F</url>
    <content type="text"><![CDATA[人生是持续而反复的构造一次选择到底有多重要？我不知道。但是在整个人生中，选择其实是可以被修正、被重塑的。自己对自己现在的情形有些体会，发现有很多选择的情景很相近，很类似。 而当自己的性格没有太大的变动的时候，自己的选择很相似，最后的结果好像也差不多。对此，自己觉得如果需要改变事情的结果，选择的答案来说，最重要的应该是改变下自己的性格。 但是也有那句话，江山易改本性难移，性格的改变是比较困难的，亦或者这就是性格决定命运的解释之一？ 不要因为预设规则，就放弃个人追求书中以珍妮·苏克为例，她的人生变化是非常大的，从芭蕾舞潜力新星到文学博士，再到法学博士，最后到哈佛教授。而在她的自传《我想看到的世界》中，她提到年轻人”应该去发现和追求自己所热爱的东西”，而不是追寻”某种预设期待的轨迹”。正是由于对自己的审视和对梦想的执着追求，她才有勇气去在自己走远的路上调转方向。 当然，这种问题在我们目前也可能会遇到：是选择就业，还是选择继续深造，就业一定要在自己的专业中发挥自己的才能吗？考研，是选择在自己的专业考研，还是跨专业考研？ 作者在书中用决策心理学的“规则遵循理论”来解释这个困境的：人们在做出一个决定的时候，往往是基于自己的身份，并依循自己的身份所应遵守的规则来进行判断，这种做法使得个体的行为与环境情形可以跟好适应。 而在东方文化下，“身份更多的是作为社会和外部的期望加之于个人身上的，而不是个人自由探索的结果。”也是因为它的存在，我们更多的想的是“我应该做什么”，而不是“我想要做什么”，更多的是“我只能做什么”，而不是“我擅长做什么”，更多的是“我现在是谁”，而不是“我未来是谁，我可能是谁”。 重新选择，不等于全部重来按照发展心理学界的观点，我们的人生是由我们自己建设起来的。在建构的过程中，我们会像科学家一样，会做出假设，然后依据假设进行验证，为此投入心血，可能我们的选择，我们的猜想是正确的，但也有可能，我们的选择和猜想假设是错误的，这个时候就需要进行修正了（类似于建模，自己根据问题情况，先进行定性分析，分析事件或事物的构成，条件要素，构成的逻辑，然后再根据逻辑，猜想出一定的规律，然后再用数据进行模拟验证，然后再不断的修正），但是如果逻辑顺序可能有问题了，我们是不是全部要重新来过呢？ 应该也不是必须的，例如，自己在建模的过程中，对数据的关系已经有了一定的了解，哪里的数据会影响另一部分的数据，这些本身就在我们的模拟中已经可以看出来了。相似的，人生的构建，当自己的猜想或假设出现问题，自己也不一定需要推到全部重来，那些留下来的，应该就是我们的人生经验和宝贵的阅历。而这些阅历经验，却对我们的未来的构建产生了新的影响。 书中苏克的故事是两个主题交错的：一个是转折和变化，另一个是继承和坚持，一方面她做出的常人难以想象的人生跳跃，另一方面是她的内心坚持了一些重要的东西。而在职业规划中，那些跳跃式的转变，实际上是”外职业生涯”，而那些坚持应该被视作”内职业生涯”。 “内职业生涯”指的是一个人内在的兴趣、禀赋、动机和能力，而这些因素，往往却是一个人自己的爱好，在人生中一直坚持下来的。而“外职业生涯”往往却会变化，因为人的地位，职业身份等诸多因素的影响而发生变化。 因此，当我们进行选择的时候可能会在外职业生涯中纠结，但其实我们自己的一些属于我们内心的东西却没有改变。 人生是反复的构造与重塑，我们需要明白，我们想在自己的故事中怎样成长、发展、变化和坚持，然后，我们再来做一些属于自己的面相未来的选择。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》2.3]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin2.3%2F</url>
    <content type="text"><![CDATA[社会学家其格蒙特·鲍曼在《流动的时代》中提到，现代社会和过去社会的一个不同点是，过去人们常常因为选择极有限而痛苦；而今，人们可能更苦恼于选择过多（选择困难症来了）。 如何才能选择到最适合自己并确信这个选择是正确的呢？作者提到了“精细化思考”的方法。 “精细化思考”：利用适当的工具，对各个选项从不同角度和方面进行深入、细致的分析，最后得出结果。 但是天性中的选择性策略是有缺陷的：我们可能只是粗略地审查易得的、显然的选项，一旦找到了合格的选项就停止对其他选项的探寻。 最常用的方法是维度分析法：把选择的对象分为不同的维度，然后针对不同的维度进行分析。 作者在书中以找工作为例：假设某人现在拿到了四个offer，仔细分析，每一个offer都拥有各自的优缺点，一时难以抉择的情形。 作者提到的维度分析法是这样做的： 第一步：从源头开始梳理，想想自己为何工作，想在工作中得到什么。这个是在重新定义问题（颇有中国特色的反问，重新定义），想想自己究竟在工作中得到什么或想要达到什么样的目标； 第二步：基于第一步，找出工作中的相关特征（自己觉得在工作中可能比较重要的指标），尽可能全面，常见的有：冒险，权威，竞争，创造性和自我表达，弹性时间，助人，收入，独立，影响他人，智性刺激…… 第三步：根据重要性自己对每一项进行赋予权重（按因素划分，每项-5-5分），并对每一项进行评分（-5-5分），当然，这一评分标准是建立在自己的价值观上面进行评估的； 第四步：针对权重和评分，进行汇总，按照总分=权重*评分之和； 将每一分工作按照这样的四部曲，来评估，当量化的数据出来以后，自己就知道自己真正想要什么了。 这样的方式应该不止是在选择工作中可以用到，在自己有选择困难症的时候，面对选项，列举自己觉得比较重要的因素，并赋予相同的权重，进行评分（前提是自己对选项已经有一定的了解，对此，信息的不对称性是问题的瓶颈，但是这个却无法得到迅速的解决），当评分出来的时候，自己想选择的结果，应该也就出来了。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》2.2]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin2.2%2F</url>
    <content type="text"><![CDATA[我们是如何走入两难困境的？文中提到了三个读者给作者写的信诉说自己的苦恼的案例，很常见的案例： 一在读大学生，想学动画制作，大学专业也想报考，但是无奈，读的国际贸易，而现在大二，在上专业课的同时，还要去蹭课，还要搞社团，忙的一团糟，而且什么都没做好，同学认为自己压力大； 一个刚毕业的大学生，做着一份无关痛痒的工作，而自己想去一展才华，想做一些技术性的工作，学习一些新技能，但是自己没有底气去学习，感觉自己很懦弱，自卑； 一个大学博士生，导师放羊式的教学，无意义的指导及对自己的否定，而自己却还是什么都做不出来，自己灰心了，想只做一些自己想做的事情，不想在自己的专业中继续研究下去了，但是却不知道怎么改变。 而作者针对这三个案例，得出这样的模式： 1. 有唯一的理想目标； 2. 理想目标和现实的差距悬殊，两者构成了严重冲突； 3. 理想和现实的矛盾导致了糟糕的内心状态。 当我们陷入人生的困局的时候，困住我们的不仅有外界的客观现实，还有我们过去的经历、习惯、惯性思维。这些都会在我们思考的时候自动植入“隐含假设”，限制我们的思考角度和范围，结果往往导致我们只是在三两个“可见选项”中做决定，而意识不到更多的“可能选项”。 可见选项：从我们的个人经历和当前的环境自动产生，显而易见，无需主动探索和发现。 可能选项：为被发现的潜在可能性，来自与我们关联的未知世界，或者已知事物的深层信息，需要我们主动探索和发现。 当我们遇到僵局的时候，比如上面的几个案例，可以试试这三步： 1.找出潜意识中的隐含假设（我们自己以为的）； 2.识别隐含假设中的不合理性，进行校正（我们自己以为的，真的是自己以为的，还真的是事实？需要进行判断）； 3.形成新的更灵活的思维框架，思考出“可能选项”，进行尝试（你以为的你以为真的只是你以为之后，想新的办法，越过自己以为的，再次尝试）； 作者提出来了四种普遍的隐含假设： 1. 赛道假设：人生是一场漫长的比赛，这样的想法，就容易陷入对竞争的焦虑和对失败的恐惧中。有这样的假设的人，可能由于害怕失败而不敢尝试，不敢试错，只能在已有的道路上重复着，没有创新的动力。 我个人是这样认为的:年轻人就该多尝试些新事物，多体验，敢于试错，在小问题上面犯错，总比犯大错误更合算一点。而且，因为是年轻人，我们的人生容错率还是比较高的，不是有一句话说，年轻人最大的资本就是年轻嘛？ 2. 低关联假设：人生中的经历是独立的，不存在关联。自己学习的专业知识，只能在工作中用到，生活中遇不到。 3. 僵固性心智：这种思维的人一般会觉得自己的能力有限，而且永远也似乎就那么多的能力，自己没有天赋，智商平平，但是其实，只要自己在持续的进步，能力不也在提升么，这不应该用发展的眼光看吗？ 4. 零和博弈：与人竞争，只想着从竞争对手的手中获取利益，却没有想到，和竞争对手合作，将蛋糕做大，两个人再来分这块大蛋糕。 这四种假设，每个人都应该反思一下，会不会陷入这个困境中。 解决思路： 目标悬置。以往我们在追求多个目标的时候，采用的是串行或并行模式，而目标悬置是将自己的目标暂且停下来，等到时机成熟再去做（这一点好像作者并没有说清楚，什么才是时机成熟），暂且这样理解：为避免人生的每一天都忙忙碌碌，还不如在自己的困境到来的时候，先放下手中的任务，转而做一些自己想做的事情，而过一段时间，再去做，只要保证任务按时完成即可； 能力嫁接。我们往常是将知识结构分开，进行分类，而在我们处理事情的时候，可能会遇到瓶颈，这个时候，可以尝试着用其他的路径的，用其他的知识结构里面的知识来尝试，当然，这种方式的前提是，自己的知识较为渊博，且自己的知识架构比较清晰，而且善于组合和调动，善于类比； 特性改造。其中常见的一个方法，就是将“消费型兴趣”，变为“生产型兴趣”，这种生产，会让自己的热爱，变得持久。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》2.1]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin2.1%2F</url>
    <content type="text"><![CDATA[从终极问题出发：以人生的最高目标为原则。 更高的标准，才会有更好的选择。 决策心理学理论认为：人在面临选择时，通常会采用“满意原则”。“满意原则”，是人从自己最熟悉的待选项开始逐一进行考察，如果看到符合标准的选项，就会采纳。 当我们把自己内心的选择标准提高的时候，自己可能选择到的选项是更好的。 你心中的真正的渴望是什么？ 书中以可汗学院的创始人为例，说明每一个内心应该是有自己的最佳的价值观选项的。我们每一个人应该在自己的内心中定一个高标准的目标：并不是说那些可以衡量且数量力求最大化的目标，而是说从整个人生来看最有意义和最有价值的目标，这个目标，反应了我们心中的真正渴望。 在现有格局上，向上走一步。格局是什么？《一代宗师》中是这样解读的：看自己，看天地，看众生。 作者将格局分为三个维度，四个层次——目标，眼界，信念 零度格局：盲众 目标：无目标或只是追随当下的潮流 眼界：流行文化，亲友的观点 信念：人生苦短，及时行乐的价值观，无稳定的信念，容易被他人诱导和说服 一度格局：逐利者（看自己） 目标：作为精致的或者粗放的利己主义者，就在寻求目标的最大化 眼界：与逐利有关的知识和技能，包括对利益机会的洞察，同时兼具比较完整的常识体系 信念：笃信丛林法则，每个人都是逐利动物，能力越强，获利越大 二度格局：理念人（看天地） 目标：为理念而生的人，毕生追求真理和捍卫真理 眼界：对某个领域有非常深入和系统的研究，并能提出极具创造性的观点。 信念：真理是美德，人生的价值就在于追寻真理之美，且必须保持内心的诚实。 三度格局：至善之人（看众生） 目标：以改良社会、增进人类的福祉为最高目标。 眼界：对他人有强烈的同理心，对人类社会的历史和现状有着深刻的认识，部分人可能同时兼具某个领域的专业知识，兼具理念人的特性。 信念：个人对整个社会负有责任，应该努力改变世界，减少世界的苦难和不公，部分人有坚定的宗教信仰。 同理心是站在当事人的角度和位置上，客观地理解当事人的内心感受，且把这种理解传达给当事人的一种沟通交流方式。同理心就是将心比心，同样时间、地点、事件，而当事人换成自己，也就是设身处地去感受、去体谅他人。同理心是EQ理论的专有名词，在不同场合、不同对象的运用时，又叫共感、同感、移情;又有着特指：它是指正确了解他人的感受和情绪，进而做到相互理解、关怀和情感上的融洽。 不管怎么说，当我们面对人生中的种种选择的时候，如果我们拿出一点勇气，为自己设立更高的目标，发现更多的选项，做出更完美的决定（选择困难症应该也有一部分是因为自己的目标不明确，或者说目标不太高？）]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》1.4]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin1.4%2F</url>
    <content type="text"><![CDATA[我们经常感觉到自己的时间不够用，总是感觉有好多事情要做，永远好像也做不完的感觉。因此，“没有时间”成了我们的口头禅，有时候是实情，但也有可能是借口。当自己不想去做某一件事情的时候，我们就会以这个理由去拒绝；但是也有这样的理论：工作会自动膨胀，直至占满所有可用的时间，说明时间是自己花完的，就像钱一样，自己花着花着就没了。 时间管理，只是会让我们越来越快时间管理，确实是可以让我们的工作效率提高，但是这只是一种表象的提高，是技术层面的解决问题，但是还是没有解决我们的根本性问题：为什么自己总感觉没有时间。因为这个只是在安排时间，将时间进行分割，细化来适应我们的工作和生活。 工作要快，但是生活要慢生活本来是有快有慢的节奏，而不是一快到底。李欧梵教授提到这样的观点：我们应该每天留一些时间来自己去思考“面壁”，在私人空间里，静静得去倾听自己的内心的声音，让自己心中的小人打架。这样做的目的是让自己不随波逐流；而对于工作上面的事物，需要快捷、干练的手段去处理。 提高时间的使用深度社会学家曾发现一个”时间悖论”：半个多世纪以来，人们可自由支配的闲暇时间在总体上是呈现增加的趋势，但人们在主观上却觉得自己的闲暇时间在减少。The more you have of it, the less you see. 心流指人全情投入于意见事中的状态，当它发生时，人心无旁骛、全神贯注，甚至忘记了自己的存在，自然会获得满足感，此时，时间的使用深度就得到高度的提升。 如何获得高质量的休闲呢？作者提出这样的观点：找到并保持至少一项长期的业余爱好，对待自己的爱好，要保持郑重的态度，并坚持自己的爱好。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》1.3]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin1.3%2F</url>
    <content type="text"><![CDATA[如何评判一件事情是否值得做？在分析一件事值不值得做、花多少精力去做的时候，可以从两个角度来评估：a. 这件事情在当下给自己带来的收益的大小（包含心智、情感、身体物质等多方面），作者称之为“收益值”；b. 这项收益随时间衰减的速度，即这项收益的持续时间，能否产生长期的效果。 按这两个角度，可以将平时处理的事情，分成 1. 高收益值、长半衰期事件，如找到真爱，学会一种有效的思维技巧； 2. 高收益值、短半衰期事件，如买一件当季流行的衣服，玩一下午手机； 3. 低收益值、长半衰期事件，如练一小时书法，读懂著作的一个章节； 4. 低收益值、短半衰期事件，如挑起与人的一次争吵掐架，漫无目的刷微博。 当然，这些收益值和半衰期的评判标准，只是作者的观点，而每个人的标准不同，可能自己在刷微博的时候，发现一个有意思的观点论证，让自己耳目一新，这样也是可以看做长半衰期高收益事件。 少做短半衰期事件在现代社会中，生活的快节奏、碎片化和功利性等特点，让现代人陷入两个无能之中“选择无能”和“执行无能”。 * 选择无能：我们很难判断这两个事情哪个更重要，就像自己在两颗苹果中选一个，很难选择，但或许，正是这种迟疑与犹豫，让我们自己更纠结，还不如自己先选了一个苹果吃了，说不定还能选择另一个苹果（也有另一种看法：当自己的想法不确定的时候，掷硬币吧，那时，你就会想到自己在内心中的真正想法） * 执行无能：自己明知这件事情很重要，但是自己还是不去做，对于拖延症患者来说，这是常有的事情。针对这两条无能问题，作者提出了简单的法则：尽量少做“短半衰期”的事情。当然，在使用这条法则之前，每个人还是应该在自己的内心中有一个评判标准，什么是短半衰期，什么是长半衰期的事情。 这条法则的意义有两层： 1. 收益值的高低无关紧要，一些不重要不紧急的事情，只要对自己有长期的益处，就应该去做（也有人将事情按照紧急程度和重要程度划分为四个区域） 2. 不要只盯着那些高大上的事情。一些不重要不紧急的事情仍可以做。（这一点应该对拖延症患者不适用吧，拖延症患者如果只是想着平时只做长半衰期的事情，那平时的任务，估计应该也完成不太好） 作者提供了一些建议：长半衰期时间指南： 积累可信的知识，训练实践技能，构建新的思维模式，提升审美品味，反思和总结个人经历，保持和促进健康，建立和维持相互信任的关系，寻找和获得稀缺性资源，探索、提出独创性的构思和发明…… 我们为什么要多读经典？经典的定义在这里指长半衰期的作品。经典的价值，在于你总会在书中发现新的东西，经典作品，在不同人去解读，得到的看法不同，同一个人，在不同时期解读，得到的看法也不相同，这也应该是作品的多像吧。 “林迪效应”：对于会消亡的事物，生命每增加一天，其预期寿命就会缩短一些；对于不会消亡的事物，生命每增加一天，则意味着更长的预期剩余寿命。 辨别生活中的噪音碎片化，快节奏和功利性的生活方式和习惯，让大部分人保持着浮躁而激动的心，每天我们获知的消息，时刻侵袭着我们的大脑，而我们处理信息的能力是有限的，如何在繁杂的信息中分辨出有用的信息是每一个人在当今社会中必备技能之一。 作者在书中提到了一个立竿见影的方法来辨别噪音：调整信息价值得到时间尺度。 法国历史学家布罗代尔曾提出历史记叙的三种时间尺度的观点：时间最长的是关注一个地区的地理和气候环境（很好奇有木有），次之的是社会文化层面的因素，最短的时间尺度才是传统的历史学所关注的具体历史事件（觉得有道理，并且对历史研究感兴趣的同学可以参考一下这个观点，很新奇的感觉）。 布罗代尔认为：具体的历史事件的出现，具有随机性，在表象的背后，是更深刻和稳定的导致该事件发生的中、长期时间尺度的因素，这些因素，才是历史学家应该关注的。 如果我们用这种观点去评判我们每一天的信息，将即时兴奋切换成长久收益的信息模式，我们就应该很容易辨别出噪音了。这应该也是与某些学者提出的大学生应该与经典为友，与大师为友的观点不谋而合了。 作者在书中这样写道：当我们用“时间之尺”丈量历史中的自己，与那些杰出人士相遇、交谈，你就会发现，与同时代的人相比，并没有那么重要，更没有必要在比较中自我怀疑、自怨自艾。人生那么短，路又那么长，你好好走就是了。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》1.2]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin1.2%2F</url>
    <content type="text"><![CDATA[对于人生而言，“五年意味着什么”？五年，发生的事情有很多，通常会从人生的一个阶段跨越到另一个阶段。 如果我们只想随波逐流，做一个平凡的普通的人，那我们就不必提前五年去思考，思考自己未来五年的规划；但是，如果我们想在自己的领域中走出一番成就或走出不一样的人生的时候，就必须要做一下规划，并持之以恒的坚持下去。 两种未来视角的思维差异有心理专家将未来分为“近期未来”和“远期未来”，五年，就是一个典型的远期未来的时间长度。 远期未来的视角下，人们倾向于用抽象、概括的方式去思考。 但是也正是由于这种抽象，概括性，我们的想法常常是缺乏细节的，缺乏具体的操作流程。我们更多关注的是这件事情对我们的意义和价值。 常见的情形：我们在想着自己要在某个时间内完成某项听起来很高大上的任务（比如在没有多少经验的情况下进行科研立项，完成自己以前就有的想法：我最近就在研究编写一个爬虫程序（虽说这种程序现在很多），爬虫听起来很高大上的感觉，自己想着写出来这种程序，针对一些关键性网站能爬取一些有用的信息，这个就是我觉得写这个程序的意义所在，但是在执行的过程中，会发现一些问题：我是参照着网络上的一个视频在学习的，但是发现很多代码看不懂。这个一方面应该是自己的基础语法结构还没牢固，还没有学会走，就想开始跑了，另一方面，应该是对程序的流程与思路还不太明确吧，只想着做出来之后的效果与作用，没考虑到其中的实施进展情况） 在近期视角下，我们更容易到具体情境中去考虑，想得更多的是“怎么做”，而不是“要不要做”，有人说过，停止焦虑的最好方法就是just do it，先尝试一下，既避免了自己还在纠结要不要做，也在实际地操作中锻炼了自己的思维方式，还是以我自己写那个程序来看吧：在寒假时就想好，要好好学一下爬虫技术，但是那个时候发现，已经有了很多优秀的爬虫库（就是自己只要将相应的爬虫库导入到自己的程序中，就可以实现相应的爬取功能），我就在纠结，要不要继续学习，就这样，纠结了好久。一直纠结到前一段时间在网上继续学习基础知识，边学习，边尝试利用库爬取文件，就这样，还在一步一步学习，就想着利用这些库爬一些有用的数据，还在尝试，上次简单地爬取了一个网页，还是很兴奋的。 面对问题时，采取不同视角各有利弊：做规划时，我们会主要考虑目标的价值和意义，能给我们带来什么好处，但是这种视角，缺乏相应的执行的可行性；对于我们眼前的事情而言，我们会更多地考虑事情的可行性和步骤，让我们一步一步地去执行我们的目标，却限制了我们的自我挑战性，我们更多关注的是可行性较高、更容易操作和执行的事情。 如何解决生活中的两种未来的冲突？一方面，从“远期未来”角度，我们应该“重战术、轻战略” ，减少对价值和意义的强调，也不要一个劲儿地担忧目标没有完成怎么办，而应该将更多的注意力投入到思考目标的实现途径，将注意力放到“怎么去做”上；另一方面，从“近期未来”的角度来看，应该提高逃避的成本，强迫自己去做手中的事情。逃避，永远不是解决问题的最佳路径，逃避，只是在延后你处理问题的时间。 在书中，作者提了两个策略，让我们参考处理远期未来和近期未来的矛盾： 使远期未来的目标更具体化，情景化，可实施；自己想要在未来的一段时间内要做什么（在什么地方做，做的是什么职位，期望的职位是什么，能力达到的水平是什么，薪资怎样，身边的人是怎样的……这样的描述越详细越好，描述好之后，就想着自己未来要想达到这个水平，自己从现在起，应该在那些地方做出努力，现在的付出程度应该是怎样的，就这样，给自己定下来一个个简单而清晰的目标，越简单越清晰越好） 降低近期未来的“非期望行为”的便利性，主动增加挑战的难度：当自己在处理难度比较大的问题的时候，而此时又有更简单的事情可以让我们处理的时候，我们就常常想放弃难度大的事情，转而处理简单易行的事情，如果我们将放弃的成本逐渐加大的时候，我们就不会想着去放弃，当自己的作业还没做完，而难度比较大，一时无法处理，现在还不急，自己想着还有美剧没刷，很容易就让自己缴械投降，去看美剧，而不是继续看手中的题目了，如果我们将自己的放弃成本加大的时候，比如，这次放弃了，下周整个一周就不能看美剧了，这样的条件，会不会让自己继续坚持下去，坚持去做手中的作业和任务呢？]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《精进》1.1]]></title>
    <url>%2F2018%2F01%2F11%2Fjingjin1.1%2F</url>
    <content type="text"><![CDATA[1. 向孩子学习“郑重的态度”对待时间的态度，民国时期的梁漱溟先生认为对待时间的态度应该是“郑重”：不敷衍、不迟疑、不摇摆，认真聚集于当下的事情，自觉而专注的投入。 当我们处于孩提时期，对事物保持好奇心和热情的时候，总是保持着严肃的态度，我有时候在跑步的时候，看见操场上的小孩子在沙坑里玩沙子的时候总是保持着聚精会神的表情，仿佛在他们手中，不再是沙子，应该是他们自己在构建自己手中的美好世界。小时候的我们对自己的要求就那么低，玩得开心就好。反倒长大了，欲望，诱惑太大，我们对生活不再保持足够的耐心与热情。我们想要的太多，想得到的总是太多，对生活的要求总是太高，但是却没有投入相应的时间和精力。可能正是我们这种态度，我们的生活不再保持美好。 2.不同的场合，不同的时间视角书中按视角将人划分为五类： 1. 积极的过去视角，这类人对待过去的态度是积极的，常常怀念自己的过去的美好; 2. 消极的过去视角，这类人常常对生活保持着消极的态度，常常回忆自己的悲惨过去; 3. 享乐主义视角，对生活保持享乐的态度，永远不会想着未来的发展，对过去的情状也无感，及时行乐的态度; 4. 宿命论的视角，认为生活本就是一场游戏，生活的主宰就是在上帝或者其他诸神手中，一切都是命中注定，自己只能接受和忍受外界的安排; 5. 未来视角，对未来充满激情，有着自己的清晰的目标和想法，为了未来的目标，可以放弃自己的当下的享乐，充分利用生命中的时间。 以画类比，看画的角度，时间不同，得到的感受也就不一样，在不同的时间使用不同的视角看人生，对生活的态度应该也是不一样的。 因此，作者建议我们需要在合适的时间切换视角：在工作中以未来的视角（因为这种视角需要效率和规划性），在休息时间，需要保持着享乐主义的态度，这个时间应该要好好享受一下当下的时间，在过年等聚会时间，需要保持一颗怀旧的视角，怀念过去的美好与欢乐。 3.由当下向未来和过去延伸其实我很疑惑为什么会向过去延伸。 不疏离过去，也不漠视未来的当下感，这也应该是为什么会向过去延伸，向未来的展望，对过去的总结。 瑞典心理学家林德沃经过研究后提出，具有平衡式时间视角的人，在内心具有一种延伸的“当下感” 。给我们提供了可行十条建议： 1.活在当下——不瞻前顾后，不左顾右盼，不患得患失。我们经常因为自己的想法太遥远，无法实现，但是内心的悸动，又无法抚慰我们躁动的心灵的时候，这句话就是我们经常勉励自己的一句鸡汤。 2.严肃地对待时间——审慎、郑重地思考时间对我们的价值并利用好它。我觉得可以这样:在某一天可以记录一下自己的时间用途，花了多少时间做了什么事情，比如：自己在图书馆学习，看了多长时间书，然后一不小心玩手机玩了多久，就可以计算自己的时间利用率。 3.留意自己拥有的空间并享受它——找到自己的独享时刻，不要疲于奔命。对生活保持张弛的节奏，留出只属于自己的时间，在这个时间里什么都不要做，只发呆，幻想，放松自己就好。 4.反思别人和自己的时间视角——认识到自己和他人的时间的异同，换位思考。有时候换不同的视角看生活，想想也是挺好，前一段时间就想起来以前和同学出去旅游的事情，挺开心，暑假还想约着出去浪一浪。 5.从现在出发连接过去——过去并没有过去，它对今天仍有意义。过去的每一次选择，每一次投入，都是有意义的，自己在前不久发现，自己的生活中的一些事情，好像是以同样的方式发生的，同样的情景再现，我就在想，或许是自己的性格决定的？这个时候，就有问题发生了。 6.并不完全沉浸过去——比过去更重要的是现在。 7.制定实现目标的计划——未来视角让我们的行动更加有序。应该每一个人心中都有自己的想法和目标，或者清晰，或者，模糊。 8.平衡计划和非计划的时间——由于随机性和不可预见因素的影响，我们的生活并不能完全被计划，平衡计划和非计划就是在未来视角和现在视角找到平衡点。 9.视未来存在于当下——未来并非遥不可及，它就出现在即将来临的每一分每一秒。 10.对未来保持积极的态度——既然未来难以预测，那么积极的心态对他能让我们在当下更有行动力。保持乐观，但不是盲目乐观，要积极，不要激进。]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>精进</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐阅读]]></title>
    <url>%2F2018%2F01%2F11%2Findex%2F</url>
    <content type="text"><![CDATA[搜书网站概览科学上网参考]]></content>
  </entry>
</search>
